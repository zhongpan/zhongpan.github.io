<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[年终总结-前端易用性规范]]></title>
    <url>%2F2020%2F12%2F30%2F042-front-end-usability-specfication%2F</url>
    <content type="text"><![CDATA[从今年7月份开始，因为公司需要，亟需解决产品易用性痛点问题，我开始前端易用性改进工作，我总结了易用性的6个方面，梳理出24个改进点，在随后的版本中逐步落地，目前大部分改进点都得到落实，产品的易用性得到极大的改观，几个月的辛勤付出终有成果，心中甚是欣慰。与此同时，我也负责前端的设计开发以及前端负责人工作，还好自己的基本功扎实😊，虽然一直从事后端研发，极短的时间内，我已经可以娴熟的进行前端开发和解决各种疑难问题，而团队管理更是我擅长的，团队有成员对我说，你要是早点来就好了，我就可以学到更多东西，这真是对我最大的褒奖。短短几个月的时间，我已经把前端的代码翻了一遍，说实在，以前代码的规范性非常差，组件复用性更差，完全没有复用的思想，整个团队要提升的地方实在太多了。在开发过程中，我发现很多问题经常重复出现，例如残留数据的问题，一方面是状态管理使用不当，全部都是用的全局状态，另一方面是没有重置状态的意识。因此我将近几个月的经验总结成一份规范，不仅能够提升易用性而且可以避免很多前端Bug，也算是一份年终总结，希望对大家有所启发和帮助。 一.前言 一个易用的界面会给人带来舒适的体验，同时也能够吸引用户，拉近用户与产品之间的距离，从而创造市场价值。易用性包含如下6个方面： 可操作性：用户在这个界面中有很强控制力； 错误处理：用户很少会造成错误，他们从错误中走出很快； 效率：用户完成一个任务很快； 吸引力：这个界面很吸引人，让用户享受使用的过程； 可理解性：用户很容易理解他所看到的，对于一个新用户来说，学习如何在界面中操作很简单； 规范性：界面依从规范、标准。 本规范围绕上述目标，从两个方面规范和指导设计开发过程，从而保证产品的易用性，第一方面是组件规范，主要解决吸引力和规范性问题，使得产品具有美观和一致的界面风格；第二方面是设计规范，主要解决可操作性、错误处理、效率、可理解性和规范性方面的问题，总结了最佳的交互设计实践，提升易用性的同时也能够避免很多前端Bug。 本规范主要面向UCD设计师和前端开发工程师，在进行原型设计和前端设计开发时需要严格遵循本规范。 二.组件规范 每一个产品有自己的界面风格，这种风格主要体现在组件的视觉表现上，是一种产品显著区别于另一个产品的主要因素。当一种风格确定下来之后，需要通过组件规范形成统一的约束，使得产品风格能够保持一致。 组件规范以平面设计稿的方式呈现，详见[SVN](https://cloudsvn.fiberhome.com/develop_team/04 Portal组/08\ 质量提升/前端设计规范/组件规范/FitOS_新规范.zip)，主要涵盖如下几个方面： 1.布局和导航 页面结构 菜单 主菜单最多二级，位置侧边栏或顶部，一级菜单有图标 面包屑，菜单帮助 页面 三级页签导航（可选） 列表页或概览页 详情页（可选） 全局header 2.图标 格式：svg，线框风格，单色（随主题色变化） 大小： 交互效果：hover，选中 3.颜色 主题色 辅助色 中性色 功能色 特殊色 4.字体字号 字体 字号 加粗 行高 5.组件 表格 按钮 页签 对话框 向导 表单 三.设计规范 设计规范主要从交互设计和开发实现的角度，总结最佳设计实践，提升用户的使用体验。设计规范分强制和建议级别，强制级别必须遵守，包括已有功能和新功能，建议级别可自行决定是否采用，但是一旦采用则必须所有功能保持一致。 1.路由 【强制】二级菜单，三级页签，详情页均有独立的URL，URL不能携带query参数。 【强制】详情页通过URL直接访问，不能从列表获取任何数据。 【强制】详情页的URL一般格式是：{列表页URL}/{id}，存在一些特殊情况，如有一些没有存库的资源，底层openstack详情接口需要同时传id和name，这时详情页的URL格式为：{列表页URL}/{id}/{name}，如遇其他特殊情况需要更新规范，列举在此。 【强制】凡是有详情页的，操作日志中需要能够支持跳转，删除操作除外，其他页面是否做跳转的原则： 跳转确实对用户有帮助，能够通过跳转后的详情页查看有用的信息。 如果某种资源做过跳转，那么所有页面保持一致，例如私网的VPC做了跳转，那么其他页面的VPC也都做跳转。 跳转通过名字跳转，没有名字则使用id。 【强制】点击一级菜单，展开二级菜单；点击二级菜单，显示对应列表页或者自动切换到三级页签的第一个页签；三级页签切换页签，展示相应列表页，URL也随之切换。 【强制】进入二级菜单后，二级菜单保持高亮。 【强制】列表页通过名字列链接跳转到详情页，没有名字则通过id列，如果列表已经包含了详情的所有信息，则可以不实现详情页。 【建议】详情页可以通过面包屑进行切换，不需要返回列表页再切换。 2.操控 【建议】列表页和详情页都可以发起操作。 【建议】表格提供批量操作入口。 【强制】点击等事件不要进行传播，例如点击tooltip，不要传播到tooltip后面的组件。 【强制】操作流程中不能出现跳转到其他页面而中断当前流程，应该采用弹出对话框方式，保证当前操作流程能够继续。 【强制】全局header固定在顶部，滚动时总是固定在顶部。 【强制】需要控制tooltip显示的位置，不要遮盖按钮等用户交互元素。 【建议】当更多操作超过10个时，需对操作进行分组并横向排列，每列最多显示10个操作。 【建议】列表行操作，当操作小于等于4个时，全部固定显示，超过4个时，固定编辑，删除，其他放入更多。 【强制】不支持的操作不显示，对于固定显示的操作则置灰。 【建议】能够一页完成的就不要使用向导分多步完成。 【建议】新建父对象后直接跳转到详情页新建子对象。 【强制】下拉框当页面滚动时不能和选择器分离，而是虽则选择器滚动而滚动。 【建议】umi的loading是接口粒度，列表某行的操作未返回会阻塞其他行操作，此时需要自己实现行粒度的loading。 【强制】对话框完成时，操作失败则对话框不关闭，成功则关闭。 【强制】列表支持点击行任意位置选中，此时单元格事件优先级更高，如果响应单元格事件，则不再选中行，符合规则3。 【强制】列表不可选择的行置灰。 3.输入 【建议】对于不太重要的字段提供默认值，如子网名称，作用不太大，可使用默认值。 【强制】对于可枚举数据，不要使用输入框，选项小于等于3项时，使用平铺的单选控件，大于3项时使用下拉选择控件。 【建议】IP段的输入根据掩码简化地址的输入。 【建议】IP地址池的输入要将复杂的语法转换为用户的交互输入，语法不直接暴露给用户。 【强制】下拉选择内容多于10项时提供搜索。 【强制】搜索输入框提供清除按钮。 【建议】在适当的地方提供拖拽功能：例如上传文件选择。 【建议】有明确范围的数值输入使用数字输入组件，输入超过最大值时自动变为最大值，输入小于最小值时自动变为最小值。 【强制】下拉选择内容显示不完整时需要tooltip。 4.提示与反馈 【强制】所有用户输入，前端需要对长度、范围、格式进行本地校验，校验通过才能下发后台。 【强制】操作置灰提示方式是右侧放置问号图标，问号图标上tooltip给出提示。 【强制】按钮置灰提示方式是直接tooltip给出提示。 【强制】输入控件的提示放在控件下方，不要通过在label旁的问号图标给出提示。 【强制】输入错误提示通过冒泡方式实时提示。 【强制】用户触发页面加载时，页面和按钮需要loading效果，定时刷新时不要loading。 【强制】按钮不能操作时置灰不可用。 【建议】编辑操作，用户没有改变数据时，不可下发后台。 【强制】后端请求返回提示使用通知组件，浏览器右侧弹出，成功则停留5秒自动关闭，失败不自动关闭，用户可手动关闭。详情默认收起，如果失败，则自动展开详情。 【建议】异步操作请求提示不自动关闭，当操作真正完成时后端推送消息给前端，此时在原来的消息提示中给出最终结果，后续同上。 【强制】全局消息提示使用消息组件，要么嵌入页面中，用户可关闭。要么浏览器上方弹出，停留4秒自动关闭，用户不可手动关闭。最多显示1个全局弹出消息。 【强制】危险操作给出确认提示，用户确认之后才下发后台。 【强制】必填字段在label的左侧必须有红色的*。 【建议】文本输入框要有灰色占位符，提示用户需要输入什么内容。 【强制】没有数据时，需要有空状态文字和图标提示。 5.数据管理 【强制】所有页面需要提供手工刷新，有状态变化的页面需要支持自动刷新。 【强制】使用定时器时需要非常小心，避免出现僵尸定时器，这通常是由于打开定时器和清除定时器没有配对，重复打开了定时器，这样前一个定时器就永远关闭不了，好的习惯是在打开定时器之前总是先清除一下。 【强制】对话框关闭或完成后，需要清除上一次数据，避免再次打开对话框出现数据残留，特别的antd form的数据需要resetFields或setFieldsValue清除。清除数据前先隐藏对话框，避免对话框出现闪变。 【强制】离开页面（包括通过浏览器回退），需要清除页面状态，避免再次回到页面出现残留，如对话框自动出现。 【强制】只有状态需要在多个组件共享时才放到redux中，否则状态保存在组件中。这样可以避免切换页面后会先显示上次的数据问题，也可以避免一些数据残留问题。 【强制】向导页面中已填数据，在上下步切换时需要保留。 【建议】必须进行数据选择的选择列表，只有一项数据时，默认选中。 【强制】字段为null时显示–，空字符串还是显示实际值。 【强制】避免字段出现重叠或超出边框，要让字段自动换行或省略显示。 【强制】字段过长时省略显示，备注最长32，其他最长16，超过长度则截取最大长度后追加…显示，中文一个字长度按1算。 【强制】列表数据过多时需要使用分页，支持分页、跳页、设定每页条数。 【强制】表格支持宽度随屏幕自适应，不出现横向滚动条。但是缩放宽度过小，无法显示所有列时，需要横向滚动条。 【建议】对话框支持拖动和最大化。 【建议】不常用选填内容放入[高级选项]，默认收起。 【强制】列表中枚举字段需要支持过滤（依赖openstack底层过滤需要视底层是否支持而定，如底层不支持则无法实现过滤）。 【强制】列表中名称，时间字段需要支持排序，其他字段视需求而定（同样依赖底层的视底层能力而定）。 【强制】所有资源都需要创建和更新时间字段。 【强制】列表字段支持自定义显示的列，某些列可固定显示，不允许隐藏。 【强制】列表支持字段模糊检索（同样依赖底层的视底层能力而定）。 【建议】复杂的列表需要支持组合条件检索，并能够保存查询条件，以便重复使用。 【建议】列表支持字段为空的检索。 6.一致性 【强制】中文环境，标点统一用中文标点。 【强制】中文环境，除了常用的英文缩写，如CPU，英文都应该翻译为中文。 【强制】错误提示统一不加感叹号。 【强制】专业术语在不同地方保持一致性。 【强制】所有页面符合组件规范。 【强制】所有页面需要适配各种主题，在各主题下显示正常，包括亮色，暗色和暗黑主题。 7.兼容性 【强制】浏览器至少支持chrome 85及以上和firefox 79及以上。 【强制】最小分辨率1366*768，保证此分辨率及以上的显示正常。]]></content>
      <categories>
        <category>design</category>
        <category>frontend</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>frontend</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js定时器的陷阱]]></title>
    <url>%2F2020%2F11%2F12%2F041-the-trap-of-js-timer%2F</url>
    <content type="text"><![CDATA[项目遇到一个奇怪的问题，分页查询的页面会自动在两页之间跳变，F5刷新之后问题消失，第一次测试报这个问题的时候，让测试按照最近的操作重试一下，又重现了一次，想着应该是必现问题，后面再看，结果后面再看的时候，怎么都无法重现了。过了一段时间，问题又再次出现，这次一定不能再放过了，F12查看网络调用，发现确实会发送两次调用，两次都是定时刷新触发的，自此心里基本有数应该是定时器导致的。接下来通过分析代码，终于找到问题的根源，是因为出现僵尸定时器，在背后还在一直运行，它里面的状态是不会变的，始终是某一页，这样切换到新的页之后，就会在两页之间自动切换。这个问题还挺隐晦的，特此记录下。 问题分析 页面如下： 列表代码如下，页面第一次创建的时候设置定时器timerList，当点击新建的时候会clear掉timerList，然后新建的完成或取消的时候再调用openTimer恢复定时器。 12345678910111213141516171819202122232425262728componentDidMount() &#123; const &#123; dispatch &#125; = this.props; dispatch(&#123; type: 'image/getImageListByTid', &#125;); this.timerList = setInterval(this.timer, 5000);&#125;componentWillUnmount() &#123; clearInterval(this.timerList); clearInterval(this.timerModal);&#125;timer = () =&gt; &#123; const &#123; page &#125; = this.state; const &#123; dispatch &#125; = this.props; dispatch(&#123; type: 'image/getImageListByTid', payload: &#123; checkschedual: true, ...page, &#125;, &#125;);&#125;;openTimer = () =&gt; &#123; this.timerModal = setInterval(this.timer, 5000);&#125;; 新建镜像完成的逻辑，回调函数callback中调用openTimer。 123456789101112131415161718192021222324okHandle = () =&gt; &#123; const &#123; dispatch, form, refresh, openTimer &#125; = this.props; const &#123; switcheValue &#125; = this.state; const callback = () =&gt; &#123; refresh(); openTimer(); &#125;; form.validateFields((err, fieldsValue) =&gt; &#123; if (err) return err; // console.log('file: ', fieldsValue); const data = &#123; ...fieldsValue, enable_integrity_check: switcheValue, // min_ram:fieldsValue.min_ram*1024, // 后台单位MB &#125;; dispatch(&#123; type: 'image/createImage', payload: data, callback, &#125;); &#125;);&#125;; 新建镜像取消的逻辑，回调会调用openTimer。 123456789101112cancelHandle = () =&gt; &#123; const &#123; dispatch, openTimer &#125; = this.props; this.setState(&#123; step: 0, type: 'imagefile', &#125;); dispatch(&#123; type: 'image/addModal', payload: false, callback: openTimer, &#125;);&#125;; model代码，成功的时候关闭对话框，失败的时候不关闭对话框。不管成功还是失败都会调用回调函数。 123456789101112131415161718192021222324252627282930// 通过镜像地址创建镜像会调用该方法 *createImage(&#123; payload, callback &#125;, &#123; call, put &#125;) &#123; const formData = new FormData(); formData.append('name', payload.name); formData.append('disk_format', payload.disk_format); formData.append('fileurl', payload.fileurl); formData.append('hypervisor_type', payload.hypervisor_type); formData.append('min_disk', payload.min_disk); formData.append('min_ram', payload.min_ram); formData.append('system', payload.system); formData.append('version', payload.version); formData.append('description', payload.description); formData.append('visibility', payload.visibility); formData.append('type', payload.type); formData.append('back', payload.back); formData.append('enable_integrity_check', payload.enable_integrity_check); const response = yield call(createImage, formData); if (isResponseSuccess(response)) &#123; notification.success(&#123; message: '操作成功！', &#125;); yield put(&#123; type: 'addModal', payload: false, &#125;); &#125; else &#123; errorMessage(response); &#125; if (callback) callback(); &#125;, 所以如果新建镜像失败，会调用一次回调函数，设置了定时器。此时对话框还没关闭，不管是取消，还是再次完成，都会再次调用openTimer，而openTimer中没有请除上次的定时器，造成了僵尸定时器。 问题修改 要么model中新建只有成功才调用回调函数，要么openTimer中总是先请除上次的定时器。 问题启示 setInterval和clearInterval必须成对出现，clearInterval的参数是setInterval的返回值。用成员变量记录setInterval的返回值时，如果重复设置setInterval，一定要注意请除上次的定时器，否则就会出现僵尸定时器。 123456openTimer = () =&gt; &#123; if (this.timerModal) &#123; clearInterval(this.timerModal); &#125; this.timerModal = setInterval(this.timer, 5000);&#125;; componentDidMount中设置的定时器，注意在componentWillUnmount中要请除，如果用useEffect会更好，设置定时器和清除定时器可以写在一起，不用分开，可维护性更好。]]></content>
      <categories>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[antd dialog的一个问题分析]]></title>
    <url>%2F2020%2F10%2F27%2F037-analysis-of-a-problem-in-antd-dialog%2F</url>
    <content type="text"><![CDATA[最近项目中碰到一个奇怪问题，项目是react+antd+umi实现，当页面中点击对话框，页面会自动滚动到最上面。最终定位跟antd dialog的实现有关，在某种条件下会触发此问题，下面分享一下问题定位过程，希望对遇到类似问题的同学有所帮助。 问题现象 列表页滚动到下方，点击操作弹出对话框，列表自动滚动到最上面。此问题在上个版本还不存在。 问题定位 一定是触发了滚动事件才会滚动到列表最上方，怎么定位哪里触发的滚动事件呢，从代码入手毫无头绪。好在可以通过浏览器devtool打断点，跟踪一下哪里触发。首先尝试了scroll事件，确实能够进到断点，但是堆栈只有一贞，没法找到源头。接着尝试了focus事件。 进到focus事件时已经滚动到上方了，此时有堆栈信息，从堆栈定位到rc-dialog，其中有一句this.switchScrollingEffect()，高度怀疑这里触发了问题，在这里打上断点，证实了确实如此。 进一步调试到this.switchScrollingEffect里面，问题触发点在setStyle这里，设置了如下样式后，就会触发滚动到最上面。 switchScrollingEffect如下： setStyle如下： 结合最近修改代码反复尝试，发现导致上面问题在于如下样式： 1234html &#123; overflow-x: hidden; overflow-y: auto;&#125; 也就是当html有如上样式(body没有，如果body也有也不会触发问题），再设置body样式{overflow: hidden, overflow-x: hidden, overflow-y: hidden}，就会出现自动滚动到最上面。 找到问题原因后我写了一小段代码最小化重现了这个问题。 123456789101112131415&lt;html&gt; &lt;body id="content"&gt; &lt;button onclick="restore()"&gt; restore &lt;/button&gt;&lt;br&gt; sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt;s&lt;br&gt; &lt;button onclick="setStyle()"&gt; setstyle &lt;/button&gt; &lt;button onclick="showsize()"&gt; showsize &lt;/button&gt; &lt;/body&gt;&lt;/html&gt; 12345678910html &#123; border: 1px solid red; height: 100%; overflow-y: auto;&#125;body &#123; border: 1px solid blue; height: 100%;&#125; 1234567891011121314151617function setStyle() &#123; var s = document.getElementById("content"); s.style['overflow'] = 'hidden'; s.style['overflow-x'] = 'hidden'; s.style['overflow-y'] = 'hidden';&#125;function showsize() &#123; var s = document.getElementById("content"); alert("document.body.offsetWidth="+document.body.offsetWidth+"\nwindow.innerWidth="+window.innerWidth+"\ndocument.body.scrollHeight="+document.body.scrollHeight+"\nwindow.innerHeight="+window.innerHeight);&#125;function restore() &#123; var s = document.getElementById("content"); s.style['overflow'] = ''; s.style['overflow-x'] = ''; s.style['overflow-y'] = '';&#125; 问题分析 因为body没有设置overflow-y，默认是visible，所以内容显示到框外。 html设置了overflow-y: auto，所以出现纵向滚动条。 当点击setstyle后，body高度变小，html就滚动到最上面。 如果html没有设置overflow-y: auto，则默认是visible，不会滚动，仍然保持在底部。 如果body也设置了overflow-y:auto，则body会出现纵向滚动条，而html的滚动条只会覆盖body的clientheight区域。点击setstyle，只是将body的滚动条去掉，不会触发html滚动。 所以rc-dialog.switchScrollingEffect()的实现是存在一些缺陷的。 其他问题 上面调试过程中，isBodyOverflowing的实现引起了我的兴趣。 1234567function isBodyOverflowing() &#123; return ( document.body.scrollHeight &gt; (window.innerHeight || document.documentElement.clientHeight) &amp;&amp; window.innerWidth &gt; document.body.offsetWidth );&#125; window.innerWidth &gt; document.body.offsetWidth这个条件，表示body的width+padding没有超过浏览器窗口内宽。document.body.scrollHeight &gt; ​ (window.innerHeight || document.documentElement.clientHeight)表示有纵向滚动。所以这个条件的意图有点让人不太理解，前一个条件很容易不满足，例如设置body的width为100vw。 （图片来自https://www.pianshen.com/article/6486132360/）]]></content>
      <categories>
        <category>antd</category>
      </categories>
      <tags>
        <tag>debug</tag>
        <tag>antd</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[antd input能在DOM中看到密码问题]]></title>
    <url>%2F2020%2F10%2F20%2F038-the-password-show-in-dom-with-antd-input%2F</url>
    <content type="text"><![CDATA[又是一个antd组件问题，密码组件在dom中可以看到密码，这算是一个低级的问题，为什么还会存在这种问题，翻看antd源代码，发现其实专门解决过这个问题，但是并没有解决彻底，在某些场景下仍然存在。 问题现象 F12在页面DOM中可以看到密码保存在value属性中，获得焦点时value会消失，但是又会自动出现。 问题定位 通过在input元素上打断点调试，发现antd input组件曾经解决过此问题，见https://github.com/ant-design/ant-design/issues/20541，但是其解决方法是在componentDidMount，onFocus，onBlur，onChange四个时机删除value属性，这就是为什么获得焦点时value会消失，但是为什么又会自动出现呢，因为有一些场景这种解法是无效的。 场景一： 如上页面，对话框使用了dva上的状态，背后的列表页面5s定时刷新导致dva上状态变化，导致对话框update，这时value又会出现。 场景二： 包裹在Form.Item(V4)或getFieldDecorator(V4之前）里面的Input不是第一层节点，此时Input失去焦点时，value也会又出现。 例1： 1234567891011121314151617181920212223&lt;FormItem labelCol=&#123;&#123; span: 6 &#125;&#125; wrapperCol=&#123;&#123; span: 14 &#125;&#125; label="新密码"&gt; &lt;PasswordTooltip name=&#123;form.getFieldValue('name')&#125; password=&#123;form.getFieldValue('password')&#125; &gt; &#123;form.getFieldDecorator('password', &#123; // initialValue:text.password, rules: [ &#123; required: true, message: '请输入密码！' &#125;, &#123; pattern: /\S+/, message: '不允许出现空口令!', &#125;, &#123; validator: passwordValidator(form.getFieldValue('name') || '') &#125;, ], &#125;)( &lt;Input type=&#123;passwordType ? 'password' : 'text'&#125; // suffix=&#123;&lt;Icon component=&#123;passwordType ? Close : Eye&#125; onClick=&#123;this.changeType&#125; /&gt;&#125; /&gt;, )&#125; &lt;/PasswordTooltip&gt; &lt;/FormItem&gt; 例2： 12345678910111213141516171819202122232425262728293031323334&lt;FormItem shouldUpdate&gt; &#123;() =&gt; ( &lt;FormItem labelCol=&#123;&#123; span: 6 &#125;&#125; wrapperCol=&#123;&#123; span: 14 &#125;&#125; label="新密码" name='newPassword' rules=&#123;[ &#123; required: switchState, message: '请输入新密码' &#125;, &#123; pattern: /\S+/, message: '不允许出现空口令', &#125;, this.validatePassword, this.validateToOldPassword, this.validateToConfirm, this.validateCloudname, ]&#125; &gt; &lt;PasswordTooltip name=&#123;this.formRef.current &amp;&amp; this.formRef.current.getFieldValue('username')&#125; password=&#123;this.formRef.current &amp;&amp; this.formRef.current.getFieldValue('newPassword')&#125; &gt; &lt;Input disabled=&#123;!switchState&#125; type=&#123;passwordType ? 'password' : 'text'&#125; // suffix=&#123;&lt;Icon component=&#123;passwordType ? Close : Eye&#125; onClick=&#123;this.changeType&#125; /&gt;&#125; onChange=&#123;(e) =&gt; &#123; this.formRef.current.setFieldsValue(&#123; 'newPassword': e.target.value &#125;); this.formRef.current.validateFields(['newPassword']); &#125;&#125; /&gt; &lt;/PasswordTooltip&gt; &lt;/FormItem&gt; )&#125; &lt;/FormItem&gt; 场景三： 包裹在Form.Item(V4)或getFieldDecorator(V4之前）里面的Input，onchange的触发会失效，这样输入一个字符后，value又会出现。 解决方案 将Input包装了一层，在其componentDidUpdate中删除value属性。不解为什么antd input不在componentDidUpdate中删除value属性。 123456789101112131415161718192021import React, &#123; PureComponent &#125; from 'react';import &#123; Input &#125; from 'antd';export default class ClearValueInput extends PureComponent &#123; componentDidUpdate() &#123; if (this.input &amp;&amp; this.input.input) &#123; if (this.input.input.hasAttribute('value')) &#123; setTimeout(() =&gt; this.input.input.removeAttribute('value')); &#125; &#125; &#125; saveInput = (input) =&gt; &#123; this.input = input; &#125; render() &#123; return &lt;Input &#123;...this.props&#125; ref=&#123;this.saveInput&#125;/&gt;; &#125;&#125;]]></content>
      <categories>
        <category>antd</category>
      </categories>
      <tags>
        <tag>debug</tag>
        <tag>antd</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[css伪类使用技巧一例]]></title>
    <url>%2F2020%2F09%2F29%2F039-an-example-of-using-css-pseudo-class%2F</url>
    <content type="text"><![CDATA[最近测试同学提了一个问题，觉得antd的时间选择组件，没有提示时分秒，不太友好。这种问题都能够提出来，是不是感觉咱们的前端水平已经被激发到极致了，估计antd是不太会接受这种问题的。这种问题怎么解呢，你还别说，真让我想到一个好办法，那就是用css伪类。 问题如下： 增加如下样式： 123.ant-picker-time-panel::before &#123; content: '时\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0分\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0\00A0秒';&#125; 最终效果，测试同学应该满意了：]]></content>
      <categories>
        <category>css</category>
      </categories>
      <tags>
        <tag>antd</tag>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[antd下拉菜单横向布局]]></title>
    <url>%2F2020%2F09%2F27%2F040-antd-dropdown-horizontal-layout%2F</url>
    <content type="text"><![CDATA[项目中的下拉菜单项越来越多，最近又加了分组，长长的一条非常难看，能不能把它改成横向布局呢。项目是用的antd，研究了下antd的下拉菜单是用ul,li实现的，通过css的flex布局和grid布局很容易实现横向布局，当然也可以只用grid布局实现，grid对于一维二维都可以胜任，更加强大。 项目原来的样子： 改成横向布局的样子： 下面通过一个简化的例子演示一下： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;div style="display:block;min-width:98px;position: absolute;"&gt; &lt;ul class="menu"&gt; &lt;li class="group"&gt; &lt;div&gt;菜单1&lt;/div&gt; &lt;ul class="group-list"&gt; &lt;li class="item"&gt;item1&lt;/li&gt; &lt;li class="item"&gt;item2&lt;/li&gt; &lt;li class="item"&gt;item3&lt;/li&gt; &lt;li class="item"&gt;item4&lt;/li&gt; &lt;li class="item"&gt;item5&lt;/li&gt; &lt;li class="item"&gt;item6&lt;/li&gt; &lt;li class="item"&gt;item6&lt;/li&gt; &lt;li class="item"&gt;item8&lt;/li&gt; &lt;li class="item"&gt;item9&lt;/li&gt; &lt;li class="item"&gt;item10&lt;/li&gt; &lt;li class="item"&gt;item11&lt;/li&gt; &lt;li class="item"&gt;item12&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class="group"&gt; &lt;div&gt;菜单2&lt;/div&gt; &lt;ul class="group-list"&gt; &lt;li class="item"&gt;item1&lt;/li&gt; &lt;li class="item"&gt;item2&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class="group"&gt; &lt;div&gt;菜单3&lt;/div&gt; &lt;ul class="group-list"&gt; &lt;li class="item"&gt;item1&lt;/li&gt; &lt;li class="item"&gt;item2&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li class="group"&gt; &lt;div&gt;菜单4&lt;/div&gt; &lt;ul class="group-list"&gt; &lt;li class="item"&gt;item1&lt;/li&gt; &lt;li class="item"&gt;item2&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; 默认效果是这样的，两级菜单，第一级分组，第二级菜单项。 增加如下样式，首先让第一级横向排列，用到了flex布局，方向是row，不换行。然后第二级为了实现最多8行，用到了grid布局，行数固定为8，先按列排，多于8行时列自动扩展。 1234567891011121314151617181920212223ul,li &#123; list-style: none; padding: 0; margin: 0;&#125;.menu &#123; display: flex; flex-direction: row; flex-wrap: nowrap;&#125;.group &#123; flex: auto; &#125;.group-list &#123; display: grid; grid-template-columns: repeat(auto-fill, auto); grid-template-rows: repeat(8,auto); grid-auto-flow: column;&#125;.item &#123; margin: 0 8px;&#125; 最终效果如下：]]></content>
      <categories>
        <category>antd</category>
      </categories>
      <tags>
        <tag>antd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[antd pro动态切换主题]]></title>
    <url>%2F2020%2F09%2F24%2F036-dynamically-switches-theme-in-antd-pro%2F</url>
    <content type="text"><![CDATA[antd pro官方文档(https://pro.ant.design/docs/dynamic-theme-cn)介绍了动态切换主题的方法。但是并没有讲的特别清楚，而且编译主题的时间比较长，非常影响开发效率。下面介绍下我的解决办法。 动态主题切换原理 主题切换主要是通过unm-plugin-antd-theme插件实现的，unm-plugin-antd-theme插件通过antd-pro-merge-less将less文件编译为一个主题css文件。 所以unm-plugin-antd-theme的配置文件就是定义的antd-pro-merge-less(https://github.com/chenshuai2144/antd-pro-merge-less)的参数，如下所示，要求是json文件，文件名theme.config.json： 12345678910111213141516171819202122232425262728293031323334353637383940&#123; "theme": [ &#123; "key": "dark", "theme": "dark", "fileName": "dark.css", "modifyVars": &#123; "@font-size-base": "12px", "@table-padding-vertical": "10px", "@link-color": "#0070cc" &#125; &#125;, &#123; "key": "volcano", "fileName": "volcano.css", "modifyVars": &#123; "@primary-color": "#FA541C", "@font-size-base": "12px", "@table-padding-vertical": "10px", "@link-color": "#0070cc" &#125; &#125;, &#123; "key": "volcano", "theme": "dark", "fileName": "dark-volcano.css", "modifyVars": &#123; "@primary-color": "#FA541C", "@font-size-base": "12px", "@table-padding-vertical": "10px", "@link-color": "#0070cc" &#125; &#125; ], "min": true, "isModule": true, "ignoreAntd": false, "ignoreProLayout": false, "cache": true &#125; 每种主题一个不同的文件，fileName定义了文件名，theme为dark表示暗黑主题（对应antdpro主题设置里面的realDark），不写就是亮色主题（对应antdpro主题设置里面的light或dark）。modifyVars就是对antd默认变量的定制。 大家会发现上面没有默认的亮色主题，因为这时不用单独的主题css文件，直接用默认的就好。 每次编译的时候unm-plugin-antd-theme插件会遍历上面定义的每个主题，编译为一个css文件，开发时位于node_modules/.plugin-theme/theme目录下，发布时位于theme目录下，每个主题文件编译都需要几秒钟，所以主题多了编译时间会比较慢，后面会讲怎么解决这个问题。 关于插件的使用，umi3对于umi-plugin开头的插件是自动加载的，不需定义在config文件的plugins中，如果看到一些老的教程不要感到困惑。 好了，有了主题文件，接下来要解决怎么应用主题，其实很简单，就是重新加载新的css文件。antd pro官方给了参考例子，也可以参考SettingDrawer源代码(https://github.com/ant-design/ant-design-pro-layout/blob/master/src/SettingDrawer/index.tsx)。 解决主题编译慢问题 开发期每次都编译主题是难以忍受的，所以我的办法是开发器能够配置是否编译主题，而发布时总是编译主题。可惜unm-plugin-antd-theme插件本身并没有提供这种配置能力。好在umi的扩展能力非常强，我写了一个插件来解决这个问题，其实非常简单，代码只有几行，插件名umi-plugin-config（https://github.com/zhongpan/umi-plugin-config）。 当然你也可以使用umi禁用插件的方法，unm-plugin-antd-theme插件的key为antdTheme，修改config文件如下： 123456import &#123; defineConfig &#125; from 'umi';const &#123; REACT_APP_ENV &#125; = process.env;export default defineConfig(&#123; antdTheme: REACT_APP_ENV !== 'dev', &#125;); 主题配置文件优化 unm-plugin-antd-theme插件的配置文件为json格式，很不方便，一来不好加入注释，二来不好共享一些变量，毕竟多个主题有一些一样的成分。为了解决这个问题，还是用到umi插件的扩展能力，在插件中将js定义转换为json定义，详见上述umi-plugin-config插件。 通过例子说明这样做的好处。你是否还记得默认的亮色主题没有编译为单独的主题css，那么所有主题统一修改的变量如果做呢。 默认的主题还是可以通过umi的配置文件的theme配置： theme.js 12345export default &#123; "@font-size-base": "12px", "@table-padding-vertical": "10px", "@link-color": "#0070cc",&#125;; config.js 123456import &#123; defineConfig &#125; from 'umi';import theme from './theme';const &#123; REACT_APP_ENV &#125; = process.env;export default defineConfig(&#123; theme,&#125;); 然后其他主题就需要在unm-plugin-antd-theme插件的json配置文件中，每个主题的modifyVars中都要加入，本文一开始已经给出了例子。 显而易见上面的统一变量修改需要烦人的重复，将json改成js后就能完美解决这个问题，theme.config.json会变成如下，需要使用CommonJS规范定义模块： 123456789101112131415161718192021222324252627282930313233343536const theme = require('./theme').default;module.exports = &#123; "theme": [ &#123; "key": "dark", "theme": "dark", "fileName": "dark.css", "modifyVars": &#123; ...theme &#125; &#125;, &#123; "key": "volcano", "fileName": "volcano.css", "modifyVars": &#123; "@primary-color": "#FA541C", ...theme &#125; &#125;, &#123; "key": "volcano", "theme": "dark", "fileName": "dark-volcano.css", "modifyVars": &#123; "@primary-color": "#FA541C", ...theme &#125; &#125; ], "min": true, "isModule": true, "ignoreAntd": false, "ignoreProLayout": false, "cache": true &#125;; 这样就可以将统一的变量修改复用起来。umi-plugin-config插件检测到theme.config.js文件就会自动转换为theme.config.json文件。 自定义主题注意事项 要支持动态切换主题，需要严格规范样式的定义方式，下面是我总结的一些建议，供大家参考。 不要使用固定的颜色，尽量使用antd已有变量，例如@text-color，表示文字颜色，在亮色和暗黑模式下值是不一样的，antd-pro-merge-less已经为我们自动处理了，我们只需要使用@text-color 所有主题统一修改的变量定义在上述theme.js中，所有主题复用 主题不一样的变量修改在各自的配置中定义 所有主题统一修改的样式定义在global.less文件中，需要设置最高优先级 所有主题用的同一个变量但是值不一样（亮色和暗黑不一样），定义在组件less中，需要使用:global 如果无法通过less定义样式，可以通过settings获取主题后动态设置行内样式]]></content>
      <categories>
        <category>antd</category>
      </categories>
      <tags>
        <tag>antd</tag>
        <tag>theme</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CriteriaQuery使用的一个陷阱]]></title>
    <url>%2F2020%2F07%2F20%2F035-a-trap-for-using-criteriaquery%2F</url>
    <content type="text"><![CDATA[使用Spring Data Jpa的CriteriaQuery进行动态条件查询时，可能会遇到一个陷阱，当条件为空时，查询不到任何结果，并不是期望的返回所有结果。这是为什么呢？ 例如下述代码，当predicates为空时，返回结果总是为空。 123456789101112public Page&lt;VmhostWithRelationPO&gt; listVmhostSpecWithRelationByPage(String name) &#123; Specification&lt;VmhostWithRelationPO&gt; spec = (root, cq, cb) -&gt; &#123; root.join("user", JoinType.LEFT); root.join("tenant", JoinType.LEFT); List&lt;javax.persistence.criteria.Predicate&gt; predicates = new ArrayList&lt;&gt;(); ...... return cb.or(predicates.toArray(new javax.persistence.criteria.Predicate[0])); &#125;; PageRequest pagable = PageRequest.of(0, 5); Page&lt;VmhostWithRelationPO&gt; page = vmhostSpecWithRelationDao.findAll(spec, pagable); return page;&#125; 看下or的注释就明白了，因为空条件总是为false，而and的空条件总是为true。所以，如果最后是and就没有问题，只有or的时候有问题。 12345678910111213141516171819public interface CriteriaBuilder &#123; /** * Create a conjunction of the given restriction predicates. * A conjunction of zero predicates is true. * @param restrictions zero or more restriction predicates * @return and predicate */ Predicate and(Predicate... restrictions); /** * Create a disjunction of the given restriction predicates. * A disjunction of zero predicates is false. * @param restrictions zero or more restriction predicates * @return or predicate */ Predicate or(Predicate... restrictions);&#125; 所以正确的写法应该这样： 123456789101112public Page&lt;VmhostWithRelationPO&gt; listVmhostSpecWithRelationByPage(String name) &#123; Specification&lt;VmhostWithRelationPO&gt; spec = (root, cq, cb) -&gt; &#123; root.join("user", JoinType.LEFT); root.join("tenant", JoinType.LEFT); List&lt;javax.persistence.criteria.Predicate&gt; predicates = new ArrayList&lt;&gt;(); ...... return predicates.isEmpty() ? cb.conjunction() : cb.or(predicates.toArray(new javax.persistence.criteria.Predicate[0])); &#125;; PageRequest pagable = PageRequest.of(0, 5); Page&lt;VmhostWithRelationPO&gt; page = vmhostSpecWithRelationDao.findAll(spec, pagable); return page; &#125; 如果条件为空则返回一个空conjunction，也就是空的and，总是为true。 公司项目的代码中常见这种写法： 1234567891011121314151617public Page&lt;VmhostWithRelationPO&gt; listVmhostSpecWithRelationByPage(String name) &#123; Specification&lt;VmhostWithRelationPO&gt; spec = (root, cq, cb) -&gt; &#123; root.join("user", JoinType.LEFT); root.join("tenant", JoinType.LEFT); List&lt;javax.persistence.criteria.Predicate&gt; predicates = new ArrayList&lt;&gt;(); ...... if (predicates.isEmpty()) &#123; cq.where(); &#125; else &#123; cq.where(cb.or(predicates.toArray(new javax.persistence.criteria.Predicate[0]))); &#125; return cq.getRestriction(); &#125;; PageRequest pagable = PageRequest.of(0, 5); Page&lt;VmhostWithRelationPO&gt; page = vmhostSpecWithRelationDao.findAll(spec, pagable); return page;&#125; 也能正常工作，但是其实没有必要在toPredicate方法中调用where，toPredicate只需要返回条件，外层会调用where。 123456789101112131415public interface Specification&lt;T&gt; extends Serializable &#123; /** * Creates a WHERE clause for a query of the referenced entity in form of a &#123;@link Predicate&#125; for the given * &#123;@link Root&#125; and &#123;@link CriteriaQuery&#125;. * * @param root must not be &#123;@literal null&#125;. * @param query must not be &#123;@literal null&#125;. * @param criteriaBuilder must not be &#123;@literal null&#125;. * @return a &#123;@link Predicate&#125;, may be &#123;@literal null&#125;. */ @Nullable Predicate toPredicate(Root&lt;T&gt; root, CriteriaQuery&lt;?&gt; query, CriteriaBuilder criteriaBuilder);&#125;]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data JPA中多表联合查询最佳实践]]></title>
    <url>%2F2020%2F07%2F20%2F034-best-practice-of-multi-table-joint-query-in-spring-data-jpa%2F</url>
    <content type="text"><![CDATA[Spring Data JPA对于单表操作非常方便，采用定义接口的方式，不用写任何实现代码就可以获得常用的数据库操作。但是对于多表联合查询，则不那么方便了，目前公司项目是采用数据库视图的方法，将多表联合查询全部变成了单表查询。数据库视图有众多好处，不失为一种解决方案，但是也存在一些弊端： 当数据库表结构变化需要同步修改视图，维护繁琐； 业务需求变化可能导致频繁修改视图暴露的字段； 有些场景可能只需要2表联合，有些场景需要更多表联合，要么建立一个大视图，要么需要建立多个类似视图，都不太好； 视图的SQL会变得越来越庞大，难以维护； 定义了实体类，JPA自动建表会把视图建成表； SQL SERVER会将视图的查询转换为对基本表的查询，性能不高。 总之将一部分业务逻辑放到数据库层维护，并不是一个特别好的方式。那么Spring Data JPA对多表查询还有哪些方法呢？有没有更好的选择呢？ 答案是肯定的，直接上结论。 方案 说明 自定义接收对象 SQL 分页 多表联合 问题 方案一 @Query JPQL：DTO或投影原生SQL：Object[]，map(2个字段时) JPQL或原生SQL JpaRepository 实体上配不配关系都可以 1.查询条件要嵌入SQL语句内，一些复杂的情形不好处理，例如某个字段模糊检索，字段是动态的；2.分页查询countQuery把查询语句重复了一遍 方案二 Specification 不支持，只能返回对应PO 无SQL 结合JpaRepository 需要在实体上配置关系，如@OneToOne，否则无法实现左连接，只能联合查询 1.实体需要配置连接关系2.每一个关联对象都是单独的数据库查询 方案三 EntityManager 不支持投影，其他同@Query JPQL或原生SQL 自己封装 实体上配不配关系都可以 相比于@Query好处是，JPQL字符串可以动态拼接，可以处理一些复杂的查询情形。但是分页需要自己封装。 方案四 CriteriaQuery DTO 无SQL 自己封装 需要在实体上配置关系，如@OneToOne否则无法实现左连接，只能联合查询 同Specification，且分页需要自己封装 终极方案 QueryDSL DTOTuple 无SQL 支持 实体上配不配关系都可以 解决以上所有问题 选择一个好的解决方案，需要考虑如下几个方面： 能够自定义对象接收查询结果集； 能够支持复杂的、动态的查询条件； 既然使用JPA，当然最好能够用类型安全的查询方式，并且使用起来比较自然； 能够原生支持分页查询； 能够支持left join，并且对实体定义没有约束。 上表就是从这几个方面进行分析，最后QueryDSL堪称完美，下面详细介绍几种方案。 示例代码：https://github.com/zhongpan/jpa-demo.git Spring Data JPA 先了解下JPA、Hibernate、Spring Data JPA三者的关系是什么？ JPA是一个接口规范，随着Java EE 5发布，也是EJB3.0的一部分。Hibernate是先于JPA出现的一种历史悠久的ORM框架，它实现了JPA，也是目前用的最多的实现。而Sprint Data JPA是Spring中提供的开箱即用的基于JPA的数据库访问框架，其采用的实现正是Hibernate。Spring Data JPA提供的数据库访问能力如下： 从上述接口的名字就可以看出： CrudRepository：最基本的增删改查操作 PagingAndSortingRepository：分页查询 QueryByExampleExecutor：基于样本查询，避免了传一堆参数，还要判断是否null JpaSpecificationExecutor：基于Specification查询，就是对CriteriaQuery的封装，类型安全的查询方式 QuerydslPredicateExecutor：基于QueryDSL的查询，也是类型安全的 上述接口的实现在SimpleJpaRepository和QuerydslJpaPredicateExecutor中，其中就是基于JPA的EntiryManager接口进行封装。如果我们要重写实现，也是通过EntiryManager来完成。 方案一 首先想到的方法自然是使用@Query注解，直接使用JPQL进行联合查询，自定义接收对象，left join都不在话下。主要的问题是对于一些复杂的、动态的查询条件不好处理，另外分页的countQuery不得不把主查询重写一遍，有点烦人。 1234567891011121314151617181920@Repositorypublic interface VmhostDao extends JpaRepository&lt;VmhostPO, String&gt; &#123; // 方法一：使用@Query注解，可以自定义接收对象 // 问题：查询条件要嵌入SQL语句内，一些复杂的情形不好处理，例如某个字段模糊检索，字段是动态的；分页查询countQuery把查询语句重复了一遍 @Query("select new com.example.demo.entity.VmhostDTO(v, u, t) from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id where v.name like %?1%") List&lt;VmhostDTO&gt; findVmhost(String name); @Query("select new com.example.demo.entity.VmhostInfoDTO(v.id, v.name, u.username, t.name) from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id where v.name like %:name%") List&lt;VmhostInfoDTO&gt; findVmhostInfo(String name); @Query("select v.id as id, v.name as name, u.username as userName, t.name as tname from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id") List&lt;VmhostInfoByProjection&gt; findVmhostInfoByProjection(); @Query(value = "select new com.example.demo.entity.VmhostInfoDTO(v.id, v.name, u.username, t.name) from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id where v.name like %:name%", countQuery = "select count(*) from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id where v.name like %:name%") Page&lt;VmhostInfoDTO&gt; findVmhostInfoByPage(String name, Pageable pageable);&#125; 方案二 那么SQL的组织能否动态编程控制呢，自然会想到Specification查询，查询条件可以通过CriteriaQuery动态拼装。这也是Spring Data JPA中用的最广泛的查询方式。但是这种方式存在一些限制，首先不能灵活自定义接收对象，只能返回PO，其次要想实现left join，必须在实体上定义关系，最后关联对象不是一次查询回来的，而是单独的查询。 1234567891011121314151617181920212223public interface VmhostSpecWithRelationDao extends JpaRepository&lt;VmhostWithRelationPO, String&gt;, JpaSpecificationExecutor&lt;VmhostWithRelationPO&gt; &#123; // 方案二：使用Specification查询 // 问题：实体必须配置关系，否则无法左连接；每个关联对象是单独数据库查询&#125;package com.example.demo.service;@Servicepublic class VmhostService &#123; public List&lt;VmhostWithRelationPO&gt; listVmhostSpecWithRelation(String name) &#123; Specification&lt;VmhostWithRelationPO&gt; spec = (root, cq, cb) -&gt; &#123; root.join("user", JoinType.LEFT); root.join("tenant", JoinType.LEFT); return cb.like(root.get("name"), "%" + name + "%"); &#125;; List&lt;VmhostWithRelationPO&gt; list = vmhostSpecWithRelationDao.findAll(spec); return list; &#125;&#125; 可能大家有两点疑问： @Query和Specification能否混用，@Query定义select的结果，Specification定义查询条件 答案：不行，总是@Query有效，你定义的Specification参数压根就不会理会 1234567891011121314// JpaSpecificationExecutor的参数和JpaRepository不一样，没啥用，SimpleJpaRepository总是用的JpaRepository的参数public interface VmhostSpecDao extends JpaRepository&lt;VmhostPO, String&gt;, JpaSpecificationExecutor&lt;VmhostInfoDTO&gt; &#123; // 方案二：@Query和Specification是不能混用的，也无法改变接收结果集对象 // 无法混用，总是query有效，spec参数压根就不会理会 @Query("from VmhostPO") List&lt;VmhostPO&gt; findVmhost(Specification&lt;VmhostPO&gt; spec); // 覆盖JpaSpecificationExecutor的方法可以吗？一样的，根本不会走到findAll的默认实现 @Override @Query("select new com.example.demo.entity.VmhostInfoDTO(v.id, v.name, u.username, t.name) from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id") List&lt;VmhostInfoDTO&gt; findAll(Specification&lt;VmhostInfoDTO&gt; spec);&#125; Specification控制接收结果集对象 答案：对不起，Specification的toPredicate中执行select是无效的，里面只能返回查询条件。 123456789101112131415 // 这样写没有用，生成如下sql // select vmhostpo0_.id as id1_2_, vmhostpo0_.addresses as addresse2_2_, vmhostpo0_.availablezone as availabl3_2_, vmhostpo0_.baremetal as baremeta4_2_, vmhostpo0_.cpucore as cpucore5_2_, vmhostpo0_.createtime as createti6_2_, vmhostpo0_.disksize as disksize7_2_, vmhostpo0_.floatip as floatip8_2_, vmhostpo0_.hostname as hostname9_2_, vmhostpo0_.locked as locked10_2_, vmhostpo0_.metadata as metadat11_2_, vmhostpo0_.name as name12_2_, vmhostpo0_.privatenetworkid as private13_2_, vmhostpo0_.ramsize as ramsize14_2_, vmhostpo0_.tenantid as tenanti15_2_, vmhostpo0_.tenantname as tenantn16_2_, vmhostpo0_.type as type17_2_, vmhostpo0_.userid as userid18_2_, vmhostpo0_.username as usernam19_2_, vmhostpo0_.vmstatus as vmstatu20_2_ from vmhost vmhostpo0_ cross join auth_user authuserpo1_ cross join auth_tenant authtenant2_ where vmhostpo0_.userid=authuserpo1_.id and vmhostpo0_.tenantid=authtenant2_.id and (vmhostpo0_.name like ?)public Optional&lt;VmhostInfoDTO&gt; listVmhostSpec(String name) &#123; Specification&lt;VmhostInfoDTO&gt; spec = (root, cq, cb) -&gt; &#123; // 只能cross join，要left join需要在实体上建立关系 Root&lt;AuthUserPO&gt; user = cq.from(AuthUserPO.class); Root&lt;AuthTenantPO&gt; tenant = cq.from(AuthTenantPO.class); // 这里执行select没有用，这个函数只能返回查询条件，外层会覆盖select cq.multiselect(root.get("id"), root.get("name"), user.get("username"), tenant.get("name")); return cb.and(cb.equal(root.get("userid"), user.get("id")), cb.equal(root.get("tenantid"), tenant.get("id")), cb.like(root.get("name"), "%" + name + "%")); &#125;; return vmhostSpecDao.findOne(spec);&#125; 因为SimpleJpaRepository的实现已经固定了select，跟JpaRepository的类型参数相关，跟JpaSpecificationExecutor的类型参数无关 1234567891011121314protected &lt;S extends T&gt; TypedQuery&lt;S&gt; getQuery(@Nullable Specification&lt;S&gt; spec, Class&lt;S&gt; domainClass, Sort sort) &#123; CriteriaBuilder builder = em.getCriteriaBuilder(); CriteriaQuery&lt;S&gt; query = builder.createQuery(domainClass); Root&lt;S&gt; root = applySpecificationToCriteria(spec, domainClass, query); query.select(root); if (sort.isSorted()) &#123; query.orderBy(toOrders(sort, root, builder)); &#125; return applyRepositoryMethodMetadata(em.createQuery(query));&#125; 方案三 上面两种使用方法是Spring Data JPA用的最多的，接下来只能从底层入手，直接使用EntiryManager。这种方法完全靠自己，所有接口都需要自己实现，丧失了Spring Data JPA的便利性。 123456789101112131415161718192021222324252627282930@Repositorypublic class VmhostEMDao &#123; @Autowired @PersistenceContext private EntityManager entityManager; // 方案三：使用原生的entityManager，解决@Query的SQL无法动态拼接问题 // 此时分页就需要自己封装了，也没有了JPA自动实现的接口 // 注意这里like后面要引号 @SuppressWarnings("unchecked") public List&lt;VmhostDTO&gt; findVmhost(String name) &#123; List&lt;VmhostDTO&gt; list = entityManager.createQuery( "select new com.example.demo.entity.VmhostDTO(v, u, t) from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id where v.name like '%" + name + "%'") .getResultList(); return list; &#125; @SuppressWarnings("unchecked") public List&lt;VmhostInfoByProjection&gt; findVmhostInfoByProjection() &#123; // 此时总是Object[]，不支持投影 List&lt;VmhostInfoByProjection&gt; list = entityManager.createQuery( "select v.id as id, v.name as name, u.username as userName, t.name as tname from VmhostPO v left join AuthUserPO u on v.userid = u.id left join AuthTenantPO t on v.tenantid = t.id") .getResultList(); return list; &#125;&#125; 方案四 类似于方案二之于方案一，我们也可以使用类型安全的查询方式CriteraQuery。 123456789101112131415161718192021222324@Repositorypublic class VmhostCQDao &#123; @Autowired @PersistenceContext private EntityManager entityManager; // 方案四：相对于方案三，使用了类型安全的CriteriaQuery，其实Specification也是用的CriteriaQuery，所以存在和Specification一样的限制，但是可以控制select了，比Specification灵活一点 public List&lt;VmhostDTO&gt; findVmhost(String name) &#123; CriteriaBuilder builder = entityManager.getCriteriaBuilder(); CriteriaQuery&lt;VmhostDTO&gt; query = builder.createQuery(VmhostDTO.class); // 实体上没有配置关系，无法使用left join，只能联合查询(inner join) Root&lt;VmhostPO&gt; root = query.from(VmhostPO.class); Root&lt;AuthUserPO&gt; rootUser = query.from(AuthUserPO.class); Root&lt;AuthTenantPO&gt; rootTenant = query.from(AuthTenantPO.class); query.multiselect(root, rootUser, rootTenant).where(builder.equal(root.get("userid"), rootUser.get("id")), builder.equal(root.get("tenantid"), rootTenant.get("id")), builder.like(root.get("name"), "%" + name + "%")); List&lt;VmhostDTO&gt; list = entityManager.createQuery(query).getResultList(); return list; &#125;&#125; 终极方案 到了终极方案了，Spring Data JPA集成了对QueryDSL的支持，官方参考见：http://www.querydsl.com/static/querydsl/latest/reference/html_single。 是不是有点像方案二+方案四，单表的时候直接使用JpaRepository和QuerydslPredicateExecutor提供的默认实现。 1234567891011121314package com.example.demo.dao;import com.example.demo.entity.VmhostPO;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.querydsl.QuerydslPredicateExecutor;public interface VmhostQDSLDao extends JpaRepository&lt;VmhostPO, String&gt;, QuerydslPredicateExecutor&lt;VmhostPO&gt;, VmhostRepository &#123; // 方案五：VmhostRepository使用原生的entityManager配合QueryDSL，完美解决所有问题 // 对于单表也可以使用QuerydslPredicateExecutor，自动拥有默认实现&#125; 多表的时候就基于EntityManager扩展，但是querydsl已经帮我们做了很多工作，不是从头开始。querydsl的书写方式相对于CriteriaQuery也更加自然，易于理解。 1234567@NoRepositoryBeanpublic class BaseRepository &#123; @PersistenceContext protected EntityManager em;&#125; 1234567public interface VmhostRepository &#123; public List&lt;VmhostDTO&gt; findVmhost(Predicate predicate); public QueryResults&lt;VmhostDTO&gt; findVmhostByPage(Predicate predicate, Pageable pageable);&#125; 1234567891011121314151617181920212223242526272829303132333435@Repositorypublic class VmhostRepositoryImpl extends BaseRepository implements VmhostRepository &#123; // 多表左连接 @Override public List&lt;VmhostDTO&gt; findVmhost(Predicate predicate) &#123; JPAQueryFactory queryFactory = new JPAQueryFactory(em); JPAQuery&lt;VmhostDTO&gt; jpaQuery = queryFactory .select(Projections.constructor(VmhostDTO.class, QVmhostPO.vmhostPO, QAuthUserPO.authUserPO, QAuthTenantPO.authTenantPO)) .from(QVmhostPO.vmhostPO).leftJoin(QAuthUserPO.authUserPO) .on(QVmhostPO.vmhostPO.userid.stringValue().eq(QAuthUserPO.authUserPO.id.stringValue())) .leftJoin(QAuthTenantPO.authTenantPO) .on(QVmhostPO.vmhostPO.tenantid.stringValue().eq(QAuthTenantPO.authTenantPO.id.stringValue())); jpaQuery.where(predicate); return jpaQuery.fetch(); &#125; @Override public QueryResults&lt;VmhostDTO&gt; findVmhostByPage(Predicate predicate, Pageable pageable) &#123; JPAQueryFactory queryFactory = new JPAQueryFactory(em); JPAQuery&lt;VmhostDTO&gt; jpaQuery = queryFactory .select(Projections.constructor(VmhostDTO.class, QVmhostPO.vmhostPO, QAuthUserPO.authUserPO, QAuthTenantPO.authTenantPO)) .from(QVmhostPO.vmhostPO).leftJoin(QAuthUserPO.authUserPO) .on(QVmhostPO.vmhostPO.userid.stringValue().eq(QAuthUserPO.authUserPO.id.stringValue())) .leftJoin(QAuthTenantPO.authTenantPO) .on(QVmhostPO.vmhostPO.tenantid.stringValue().eq(QAuthTenantPO.authTenantPO.id.stringValue())) .offset(pageable.getOffset()).limit(pageable.getPageSize()); jpaQuery.where(predicate); return jpaQuery.fetchResults(); &#125;&#125; 总结 以上方法都还是在Spring Data JPA框架之内，如果你愿意，你也可以去重写SimpleJpaRepository，重写了注意通过如下注解启用。 1@EnableJpaRepositories(repositoryBaseClass = XXXXXX.class) 其实QueryDSL已经做了很好的封装，完全没有必要重复造轮子，Spring Data JPA也提供了很多扩展点，在保留其便利性的基础上，根据需要去扩展，不需要全部推倒重来。]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解Spring Boot中日期时间格式处理]]></title>
    <url>%2F2020%2F06%2F04%2F033-time-format-processing-in-springboot%2F</url>
    <content type="text"><![CDATA[在springboot中开发RESTful接口，经常会遇到日期时间转换相关的问题，例如我们明明输入看起来很正常的日期时间字符串，但是系统却报错无法解析： JSON parse error: Cannot deserialize value of type java.time.OffsetDateTime from String “2020-06-06 14:26:31” 或者接口返回的日期时间字符串是一个很奇怪的字符串： 2020-06-04 14:41:54.767135400+08:00 如何正确的处理日期时间，本文将一探究竟。 日期时间格式标准 有两个标准组织对日期时间格式进行规范，一个是IETF，一个是ISO。虽然IETF的定义更早，但是它存在一些问题，ISO的定义使用更普遍。但是不管哪种定义，我们常常使用的yyyy-MM-dd HH:mm:ss这种格式都不是标准的，你是否非常惊讶呢。 IETF RFC822-&gt;RFC2822-&gt;RFC5322 日期时间的本文表示最早是在电子邮件消息中被讨论和定义，可以追溯到Internet刚诞生之时，ARPANET使用的文本信息格式中所定义，也就是RFC822，发布于1982年。此后经过若干次修订，定型是RFC2822，最新版是RFC5322。 通过几个例子来了解下这种格式长什么样子。 最常见的样子如下，通过linux命令date可以打印： date --rfc-email Thu, 04 Jun 2020 13:54:52 +0800 有些格式已经不建议使用，RFC2822定义为过时的格式，如： 年份使用4位以下数字 时区使用时区名，如UT，GMT RFC1123 RFC1123并不定义日期时间格式，而是描述应用程序之间通信协议的需求，包括各种应用层协议，如TELNET，FTP，SMTP等，涉及到日期时间格式的正是SMTP，它引用了RFC822，并说明了年份修改为2到4个数字，建议时区总是使用数字。 RFC1036 同样RFC1306也不定义日期时间格式，而是描述USENET中对日期时间的要求，同样引用了RFC822。 综上IETF的时间格式主要为电子邮件定义，但是只要以可读文本方式表示时间都可以使用。IETF的定义带有明显的时代和地区特征，并不具有国际通用性，也不便于阅读和解析，因此又出现了ISO的日期时间格式。 ISO8601,RFC3339 ISO的日期时间格式有助于避免由许多不同的国家符号引起的国际通信混乱，并提高了计算机用户界面的可移植性。第一版发布于1988年。 RFC3339是ISO8601的概要版本。 先通过例子了解一下他们长什么样子。 date --iso-8601=ns 2020-06-04T14:41:54,767135400+08:00 date --rfc-3339=ns 2020-06-04 14:41:54.767135400+08:00 以上是最常见的样子，ISO8601相对于RFC5322有几个主要变化： 多了秒的小数部分，用.或,连接 精度上可以从年到秒的小数部分都可以，例如2020、2020-06、2020-06-04都是合法的 日期和时间之间增加了连接字符T 可以表示一年的第几周的星期几，例如2020-W01-1表示2020年第一周的星期一 UTC时区可以简写为Z 年月日或时分秒之间的连接符可省略 RFC3339和ISO8601的区别： RFC3339允许将日期和时间之间的连接符T换为空格 秒的小数部分通常使用.连接 未使用一年的第几周的星期几的表示 Java日期时间编程接口 Java的发展过程中出现过几个不同的日期时间编程接口。java8之前的日期时间接口存在众所周知的问题，这时只能寻求第三方库库来解决，这就是joda，java8大量借鉴了joda，推出了新的日期时间库。自此，java8日期时间接口成为首选。 java8之前 java8 joda 本地时间 java.util.Date java.time.LocalDatejava.time.LocalTimejava.time.LocalDateTime org.joda.time.LocalDateorg.joda.time.LocalTimeorg.joda.time.LocalDateTime 带时区时间 java.time.OffsetTimejava.time.OffsetDateTimejava.time.ZonedDateTime org.joda.time.DateTime 格式化和解析 java.text.DateFormat java.time.format.DateTimeFormatter org.joda.time.format.DateTimeFormatter 举例 Date date = new Date();SimpleDateFormat fmt = new SimpleDateFormat(“yyyy-MM-dd HH:mm:ss”);String str = fmt.format(date);date = fmt.parse(“2020-06-06 15:13:25”); LocalDateTime date = LocalDateTime.now();DateTimeFormatter fmt = DateTimeFormatter.ofPattern(“yyyy-MM-dd HH:mm:ss”);String str = fmt.format(date);TemporalAccessor acc = fmt.parse(“2020-06-06 15:13:25”);date = LocalDateTime.from(acc); LocalDateTime date = LocalDateTime.now();DateTimeFormatter fmt = DateTimeFormat.forPattern(“pattern”);String str = fmt.print(date);date = fmt.parseLocalDate(“2020-06-06 15:13:25”); 以上各种日期时间编程接口都提供了格式化和解析接口，实现字符串和日期时间对象之间的互相转换，我们可以定制日期格式，例如常用的格式yyyy-MM-dd HH:mm:ss，那么格式化和解析都会按照这个格式，解析时如果不符合格式就会异常。 sprintboot中如何处理日期时间 确切的说是如何处理json和java日期时间对象之间的转换。 springboot极大的简化了springmvc的开发，对于开发RESTful接口也是一样，开箱即用。这是通过autoconfigure和starter实现的。 首先引入spring-boot-starter-web依赖。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; spring-boot-starter-web会引入spring-boot-starter-json，spring-boot-starter-json又会引入jackson-databind，jackson-datatype-jdk8和jackson-datatype-jsr310。可见json的实现默认是使用的jackson。其中jackson-datatype-jsr310就包含了java8日期时间的序列化、反序列化方法。 其次springboot应用，也就是使用了@SpringBootApplication注解，通过autoconfigure对jackson进行了自动配置。实现代码在sprint-boot-autoconfigure的JacksonAutoConfiguration.java文件中。 其中有三个点对jackson进行配置：Jackson2ObjectMapperBuilder，Jackson2ObjectMapperBuilderCustomizer和ObjectMapper，以上所有配置最终都是影响ObjectMapper。 Jackson2ObjectMapperBuilder是ObjectMapper的工厂，只有一个，所以这里使用了@ConditionalOnMissingBean 1234567891011121314@Configuration(proxyBeanMethods = false) @ConditionalOnClass(Jackson2ObjectMapperBuilder.class) static class JacksonObjectMapperBuilderConfiguration &#123; @Bean @Scope("prototype") @ConditionalOnMissingBean Jackson2ObjectMapperBuilder jacksonObjectMapperBuilder(ApplicationContext applicationContext, List&lt;Jackson2ObjectMapperBuilderCustomizer&gt; customizers) &#123; Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder(); builder.applicationContext(applicationContext); customize(builder, customizers); return builder; &#125; Jackson2ObjectMapperBuilder会调用Jackson2ObjectMapperBuilderCustomizer对builder进行定制，即上述customize方法，Jackson2ObjectMapperBuilderCustomizer可以有多个 12345678910@Configuration(proxyBeanMethods = false) @ConditionalOnClass(Jackson2ObjectMapperBuilder.class) @EnableConfigurationProperties(JacksonProperties.class) static class Jackson2ObjectMapperBuilderCustomizerConfiguration &#123; @Bean StandardJackson2ObjectMapperBuilderCustomizer standardJacksonObjectMapperBuilderCustomizer( ApplicationContext applicationContext, JacksonProperties jacksonProperties) &#123; return new StandardJackson2ObjectMapperBuilderCustomizer(applicationContext, jacksonProperties); &#125; 最后你可以直接配置ObjectMapper，只能有一个，所以你需要指定@Primary，默认是通过builder创建 12345678910111213 @Configuration(proxyBeanMethods = false)@ConditionalOnClass(Jackson2ObjectMapperBuilder.class)static class JacksonObjectMapperConfiguration &#123; @Bean @Primary @ConditionalOnMissingBean ObjectMapper jacksonObjectMapper(Jackson2ObjectMapperBuilder builder) &#123; return builder.createXmlMapper(false).build(); &#125; &#125; 那么对于日期时间的处理，springboot的默认行为是怎么样的呢，默认的代码配置在上述StandardJackson2ObjectMapperBuilderCustomizer中。 1234567891011121314151617181920212223242526static final class StandardJackson2ObjectMapperBuilderCustomizer implements Jackson2ObjectMapperBuilderCustomizer, Ordered &#123; ...... private void configureDateFormat(Jackson2ObjectMapperBuilder builder) &#123; // We support a fully qualified class name extending DateFormat or a date // pattern string value String dateFormat = this.jacksonProperties.getDateFormat(); if (dateFormat != null) &#123; try &#123; Class&lt;?&gt; dateFormatClass = ClassUtils.forName(dateFormat, null); builder.dateFormat((DateFormat) BeanUtils.instantiateClass(dateFormatClass)); &#125; catch (ClassNotFoundException ex) &#123; SimpleDateFormat simpleDateFormat = new SimpleDateFormat(dateFormat); // Since Jackson 2.6.3 we always need to set a TimeZone (see // gh-4170). If none in our properties fallback to the Jackson's // default TimeZone timeZone = this.jacksonProperties.getTimeZone(); if (timeZone == null) &#123; timeZone = new ObjectMapper().getSerializationConfig().getTimeZone(); &#125; simpleDateFormat.setTimeZone(timeZone); builder.dateFormat(simpleDateFormat); &#125; &#125; &#125; 其逻辑是首先读取spring.jackson.date-format属性，如果不为空就会设置builder.dateFormat，如果是一个类（当然是从java.text.DateFormat派生），那么初始化为这个类的实例，否则认为配置的yyyy-MM-dd HH:mm:ss这种格式化字符串，然后创建SimpleDateFormat实例。 另外springmvc本身还有一个MappingJackson2HttpMessageConverter，其实也是配置Jackson2ObjectMapperBuilder。]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>jackson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决vscode调试python遇到connection refused问题]]></title>
    <url>%2F2020%2F04%2F22%2F032-vscode-debug-connection-resused%2F</url>
    <content type="text"><![CDATA[最近用vscode远程开发功能调试一台centos虚拟机上的python的时候遇到connection refused问题，而之前在windows本地环境一直是好的，非常奇怪。研究了2天的时间，才把问题找到，通过定位问题，把vscode的调试器的架构也有了更深入的了解，下面记录下问题定位过程。 环境 VS Code version: 1.44.2 Python Extension version: 2020.4.74986 OS and version: centos7 Python version: 2.7.5 vscode调试器架构 官方参考： vscode Debugger Extension：https://code.visualstudio.com/api/extension-guides/debugger-extension Debug Adapter Protocol：https://microsoft.github.io/debug-adapter-protocol/overview 为了解耦vscode和具体语言的调试器，中间增加了一个Debug Adapter，它是在独立进程运行的，vscode和Debug Adapter之间通过Debug Adapter Protocol通信。 Python的Debug Adapter就是在Microsoft Python Extension中实现的。 深入调试交互过程 通过一个Windows正常环境来探索调式器交互过程。 12345678910111213&#123; "version": "0.2.0", "configurations": [ &#123; "name": "Python: 当前文件", "type": "python", "request": "launch", "program": "$&#123;file&#125;", "console": "integratedTerminal", "port": 50624 &#125; ]&#125; 上面的launch.json指定了具体端口，便于我们分析进程之间的关系，打上断点，启动一个调试，然后通过端口分析进程之间的关系。 上述9088进程就是Debug Adapter，运行代码如下： 14272进程如下： 正是vscode终端打印的： ptvsd_launcher.py是调用的ptvsd，上述命令就是ptvsd --client --host localhost --port 50624 D:/test/pytest/main.py。 然后再结合阅读python-extension和ptvsd的源代码，终于理清上述进程关系，下面是launch的过程，attach的过程有所区别。 vscode Debugger UI启动一个调试，首先会创建Debug Adapter进程，就是上述9088进程，它们之间是通过stdin和stdout通信的。Python Debug Adapter的实现是通过派生node debug adapter实现的。 Debug Adapter进程会根据launch.json的host和port进行TCP监听，如果不设置host就是localhost，如果不设置port，会自动随机选择一个。 Debug Adapter启动调试程序，实际是通过ptvsd完成的，运行在单独的进程，就是上述14272进程，launch的情况下ptvsd以client mode运行，也就是ptvsd主动连接Debug Adapter，因为这时Debug Adapter先于调试程序运行，调试程序可以第一时间知道Debug Adapter的端口。如果是attach的情况，就需要IDE连接调试程序，告诉调试程序Debug Adapter的地址，然后调试程序再连接Debug Adapter。 调式程序连接到Debug Adapter，然后按照DAP进行调试交互。 问题定位 弄清楚调式的交互过程后，就容易定位问题了，connection refused问题就出在ptvsd连接Debug Adapter的时候，一开始以为是不是Debug Adapter退出了，但是ps查看进程是正常的。通过阅读源代码发现ptvsd连接Debug Adapter的超时时间可以在launch.json中设置，默认是20秒(20000)，我将timeout改大了一点，便于分析问题。 1234567891011121314&#123; "version": "0.2.0", "configurations": [ &#123; "name": "Python: 当前文件", "type": "python", "request": "launch", "program": "$&#123;file&#125;", "console": "integratedTerminal", "port": 50624, "timeout": 200000 &#125; ]&#125; 然后通过netstat查看，发现Debug Adapter并不是绑定在127.0.0.1。 进一步查看/etc/hosts文件，发现localhost正是10.0.2.15，是eth0的ip。 而Debug Adapter启动调试程序时的–host参数总是为localhost，不管launch.json中如何设置。 自此问题基本定位清楚： Debug Adapter中监听是通过node net模块实现，net模块对于localhost会通过getaddrinfo获取ip，而getaddrinfo会通过/etc/hosts查询ip； ptvsd中是通过Python socket模块实现tcp通信，socke模块connect时，如果地址是localhost，它总是会使用127.0.0.1，bind也是一样。下述代码除了返回/etc/hosts中的ip，总是会加上127.0.0.1。 1socket.getaddrinfo("localhost", None, socket.AF_INET, socket.SOCK_STREAM) 问题解决 方案1：launch.json中指定host为127.0.0.1且必须为127.0.0.1 方案2：Debug Adapter启动调试程序时的–host参数从launch.json中获取，同时launch.json中指定host为任意合法ip Debug Adapter总是绑定到0.0.0.0，也就是任意地址 ptvsd连接Debug Adapter时也根据getaddrinfo将localhost转换为ip，并选择非127.0.0.1的ip 第一种方案是最简单的，其他方案需要修改python-extension或ptvsd的源代码。 以上都是仍然使用ptvsd的解决方法，还有一个方法是切换到最新的debugpy也可以解决问题，参照microsoft/ptvsd#2104。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>vscode</tag>
        <tag>python</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch学习笔记(一)-搭建环境]]></title>
    <url>%2F2020%2F04%2F14%2F031-elasticsearch-learning-notes-01%2F</url>
    <content type="text"><![CDATA[Elasticsearch目前的发展可谓如日中天，DBEngine上在搜索引擎领域位列第一，作为后来者热度远高于排第二、三的Splunk和Solr。Elasticsearch的创造者叫Shay Banon，最初是因为Lucence的接口使用困难，开发了一个库Compass，让Lucence更好用，2010年Shay Banon对Compass进行重写，把它变成一个高性能的、分布式的服务端软件，取名叫Elasticsearch，并在github开源。这背后还有一个温情的故事，据说起因是为了给新婚老婆写一个菜谱搜索应用，但是最后菜谱应用没有写出来，倒是造就了一个伟大的软件。 Elasticsearch发展非常快，2010年2月发布第一个版本，2012年成立Elastic公司，2014年开始商业化，2018年纽交所上市，在全球股市低迷的背景下上市当天仍大涨94%，创造了开源软件的传奇。而Elastic公司非常的厚道，坚持开源路线，从6.3版本开始将其收费部分x-pack也&quot;开源&quot;，x-pack的基础功能终身免费，不需要注册，其中就包括安全方面必须的HTTPS，权限控制等。 今天开始我想通过一系列文章来探索和学习一下ES，第一篇介绍一下ES的基本概念，搭建单节点的ES，通过图形客户端去探索ES的数据结构和操作，通过监控了解ES运行状态，建立对ES总体上的一个认识。 ES简介 ES是基于Apache Lucence的一个开源的，高性能的分布式搜索引擎，Java语言开发，对外提供RESTful接口，使用方便。 ES从6.3版本开始内置支持SQL，当然只支持查询操作，从而降低了学习其特有query DSL的成本，更是让上层基于SQL的应用可以无缝和ES对接。虽然ES越来越像一个数据库，但是它们之间还是有区别的，最显著的一个区别是，ES添加数据不是立刻刷新索引的，有一个刷新间隔，默认1s，所以不会立刻查询到，这就是所谓的近实时，ES强在检索和分析。所以ES和关系数据库的区别像是OLAP和OLTP的区别，适用于不同场景。 ES主要的应用场景，一个是作为应用的前端检索缓存，提高海量数据的检索效率和做一些聚合检索，先从ES检索，然后再从后端存储检索。第二个是日志和监控，以ES为中心，Elastic公司发展起一个生态圈，之前叫ELK，也就是Elasticsearch、Logstash和Kibana，后来加入Beats，名字改成了Elastic Stack。以Kibana为入口，发展到上层的各种分析和可视化应用，包括热门的机器学习。 先理解ES的一些基本概念，可以和关系数据库做一个类比： index：类似关系数据库的表，但是index不需要预先定义schema。 document：类似关系数据库的记录，json格式。 mapping：也就是类似关系数据的schema了，可以显式定义，也可以添加document时ES自动识别创建。可以指定mapping为严格模式，这样添加数据库就必须严格符合mapping中定义的字段。 type：从7.0开始删除了，其实就是mapping的名字，一个index下只有一个type，有点鸡肋。虽然删除了，但是其实内部默认还是有一个type，名字固定是_doc。 index template：创建index时自动对index做一些设置，例如index的主分片个数，复制分片个数，mapping字段的类型等等。 ilm：也就是index lifecycle management，可以对数据分为Hot，Warm，Cold，Delete四个阶段，可以相应的定义不同的策略。 ES安装 最简单的方式就是用docker来运行。 官方参考文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html 我使用的最新的7.6.2版本，单节点运行时需要指定discovery.type=single-node，以前老的版本是不需要的。 1docker run -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.6.2 为了后续配置方面可以把配置文件放到host机器上，然后mount到容器。 1docker run -d --name es -p 9200:9200 -p 9300:9300 -v $(pwd)/elasticsearch.docker.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /etc/localtime:/etc/localtime elasticsearch:7.6.2 这里加入-v /etc/localtime:/etc/localtime是为了将容器的时间和host保持一致，后面再讲为什么。 配置文件elasticsearch.docker.yml如下。 123cluster.name: "es_cluster"network.host: 0.0.0.0discovery.type: single-node 从上面命令看到ES暴露2个端口，9200用于提供RESTful接口，9300端口提供TCP的内部协议通信，9300对外的访问在7.0标识为Deprecated，只有集群内部之间仍通过9300通信。 通过curl就可以很方便的操作ES。 1curl http://127.0.0.1:9200 返回如下内容表示ES启动正常。 1234567891011121314151617&#123; "name" : "903d9b3ecb97", "cluster_name" : "es_cluster", "cluster_uuid" : "zV0tEFQ4TgqtyzsabtzMMg", "version" : &#123; "number" : "7.6.2", "build_flavor" : "default", "build_type" : "docker", "build_hash" : "ef48eb35cf30adf4db14086e8aabd07ef6fb113f", "build_date" : "2020-03-26T06:34:37.794943Z", "build_snapshot" : false, "lucene_version" : "8.4.0", "minimum_wire_compatibility_version" : "6.8.0", "minimum_index_compatibility_version" : "6.0.0-beta1" &#125;, "tagline" : "You Know, for Search"&#125; 图形客户端 推荐一个图形客户端dejavu，使用reactjs开发，相比常用的elasticsearch-head插件更加现代化一些，功能也丰富一些，对于探索ES非常有帮助。 为了解决跨站访问问题，需要修改ES的配置如下，其中http.cors.allow-origin也可以指定为具体的地址，也就是dejavu的地址，例如http://localhost:1358。 1234567cluster.name: "es_cluster"network.host: 0.0.0.0discovery.type: single-nodehttp.cors.enabled: truehttp.cors.allow-origin: "*"http.cors.allow-headers: X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorizationhttp.cors.allow-credentials: true 然后重启一下ES。 dejavu也支持docker运行。 1docker run -d --name=es-ui -p 1358:1358 -d appbaseio/dejavu 访问http://localhost:1358即可使用。 输入ES地址和index名(*表示所有)就可以开始读取和操作document和mapping。目前不支持index tempalte，ILM策略等。 需要注意如果ES是空的是连接不上的，必须至少有一个index。 监控 ES最常用的场景是用于监控和分析，对ES自身的监控自然也不在话下。ES本生提供了丰富的监控指标，结合Kibana可以将他们可视化。通过监控可以让我们更了解ES内部的细节。 Kibana安装 官方文档：https://www.elastic.co/guide/en/kibana/7.6/docker.html 同样使用docker运行： 1docker run -d --link es:elasticsearch -p 5601:5601 -v $(pwd)/kibana.docker.yml:/usr/share/kibana/config/kibana.yml -v /etc/localtime:/etc/localtime --name kibana kibana:7.6.2 –link可以在kibana容器中通过http://elasticsearch:9200访问ES。 配置文件kibana.docker.yml如下。 1234server.name: kibanaserver.host: "0"elasticsearch.hosts: [ "http://elasticsearch:9200" ]xpack.monitoring.ui.container.elasticsearch.enabled: true kibana中对ES的监控是在Stack Monitoring功能中，其中也包含对Kibana，Logstash及Beats的监控。 数据采集有两种方式，通过Metricbeat采集或内部采集，官方推荐前者： 时区问题 先说遇到的一个问题，从Kibana查询最近一个小时的数据会发现查询不到，但是采集是正常的，也可以看到监控的index已经有数据了。 问题原因是访问kibana的机器是用的本地时间，而ES、Kibana运行的机器是UTC时间。需要把ES运行的机器改成本地时间。ES是运行在容器中的，问题就变成了怎么修改容器的时区。答案就是先改Host的，然后让容器和Host保持一致。我是用的centos，以下是centos的修改方法。 123sudo timedatectl set-timezone Asia/Shanghaisudo hwclock --localtime -wsudo timedatectl set-ntp yes ES的容器基础镜像也是centos，所以运行容器的时候加上-v /etc/localtime:/etc/localtime即可。 通过内部采集 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/7.6/monitoring-production.html 先来看第一种方式，非常简单，通过kibana图形界面就可以操作。 点击Turn on monitoring就可以了，Kibana会帮我们将ES的xpack.monitoring.collection.enabled设为true，你也可以通过Kibana的Dev Tools中的Console自己修改，是一样的效果： 123456PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;xpack.monitoring.collection.enabled&quot;: true &#125;&#125; 另外有几个参数也很重要，它们默认是true的，如果它们任意一个为false，那么也无法监控。 xpack.monitoring.enabled：决定是否启用监控 xpack.monitoring.elasticsearch.collection.enabled：决定是否采集elasticsearch 内部采集是Kibana默认通过local exporter保存在被监控ES集群中，index名为.monitoring-*，例如对kibana的采集数据的index形如.monitorning-kibana-7-2020.04.13，每天一个。 通过Metricbeat采集 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/7.6/configuring-metricbeat.html 这是官方推荐的方式，两种方式二选一。 首先同样需要ES的xpack.monitoring.collection.enabled设置为true，为了禁用内部采集将xpack.monitoring.elasticsearch.collection.enabled设置为false。 然后安装Metricbeat，官方文档：https://www.elastic.co/guide/en/beats/metricbeat/7.6/running-on-docker.html 同样用Docker运行，分两步，第一步是setup过程，主要是初始化一些数据。 1234docker run --rm --link kibana:kibana --link es:elasticsearch \docker.elastic.co/beats/metricbeat:7.6.2 \setup -E setup.kibana.host=kibana:5601 \-E output.elasticsearch.hosts=[&quot;elasticsearch:9200&quot;] 第二步启动Metricbeat。 123456789docker run -d --link es:elasticsearch \ --name=metricbeat \ --user=root \ --volume=&quot;$(pwd)/metricbeat.docker.yml:/usr/share/metricbeat/metricbeat.yml:ro&quot; \ --volume=&quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot; \ --volume=&quot;/sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro&quot; \ --volume=&quot;/proc:/hostfs/proc:ro&quot; \ --volume=&quot;/:/hostfs:ro&quot; \ docker.elastic.co/beats/metricbeat:7.6.2 metricbeat 这里可能会遇到一个问题。 1Exiting: error loading config file: config file (&quot;metricbeat.yml&quot;) must be owned by the user identifier (uid=0) or root 或者： 1Exiting: error loading config file: config file (&quot;metricbeat.yml&quot;) can only be writable by the owner but the permissions are &quot;-rw-rw-r--&quot; (to fix the permissions use: &apos;chmod go-w /usr/share/metricbeat/metricbeat.yml&apos;) 原因参考：https://www.elastic.co/guide/en/beats/libbeat/5.3/config-file-permissions.html#config-file-permissions 因为在host中我是用的non-root用户uid1000，到容器里面的uid1000是metricbeat用户，但是beats要求配置文件必须是root拥有并且权限为0644。解决办法是把metricbeat.docker.yml的owner改成root。 12sudo chown root metricbeat.docker.ymlsudo chmod 0644 metricbeat.docker.yml 配置文件metricbeat.docker.yml如下。 123456789101112131415161718192021222324252627282930313233343536metricbeat.config: modules: path: $&#123;path.config&#125;/modules.d/*.yml # Reload module configs as they change: reload.enabled: falsemetricbeat.autodiscover: providers: - type: docker hints.enabled: truemetricbeat.modules:- module: elasticsearch metricsets: - ccr - enrich - cluster_stats - index - index_recovery - index_summary - ml_job - node_stats - shard period: 10s hosts: ["http://elasticsearch:9200"] #username: "user" #password: "secret" xpack.enabled: trueprocessors: - add_cloud_metadata: ~output.elasticsearch: hosts: '$&#123;ELASTICSEARCH_HOSTS:elasticsearch:9200&#125;' username: '$&#123;ELASTICSEARCH_USERNAME:&#125;' password: '$&#123;ELASTICSEARCH_PASSWORD:&#125;' 用dejavu可以查看到metricbeat写入的index形如.monitoring-es-7-mb-2020.04.13，也是每天一个。 回到Kibana的Stack Monitoring可以查看到ES和Kibana的监控了，其中ES是通过Metricbeat采集的数据，而Kibana是通过第一种方式内部采集的。]]></content>
      <categories>
        <category>elastic stack</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>kibana</tag>
        <tag>metricbeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode调试springmvc应用]]></title>
    <url>%2F2020%2F03%2F27%2F030-debug-springmvc-in-vscode%2F</url>
    <content type="text"><![CDATA[Spring Boot已经成为开发web应用的标配，通过Spring Initializr创建Spring应用时，Spring Boot已经是必选项。Spring官方提供两个重要开发工具： Spring Tools 4：包括eclipse、vscode和Theia三个版本，其中针对eclipse的就是之前的STS(Spring Tools Suit)，是一个基于eclipse的定制版本，而针对vscode的其实就是Spring Boot Extension Pack，是一系列vscode插件。其中Spring Boot Dashboard插件提供对spring boot应用的调试支持。 spring initializr：用于创建一个spring模板项目，目前Spring Boot是必选项。 如果你使用的Spring Boot，那么vscode调试不是问题，已经有很好的支持。但是你可能出于学习的目的，创建的单纯Spring mvc项目，那么调试就需要一番折腾了。 前提 安装Java Extension Pack，配置好Java开发环境。使用maven管理项目。 运行 首先要将Spring应用运行起来，最简单的方法是通过maven插件实现，例如jetty插件，当然tomcat也是一样的。在pom.xml中加入jetty-maven-plugin，详细配置可参考： 123456789101112131415161718192021222324252627&lt;project&gt; &lt;build&gt; &lt;finalName&gt;springmvc-study&lt;/finalName&gt; &lt;pluginManagement&gt;&lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-maven-plugin&lt;/artifactId&gt; &lt;version&gt;9.4.27.v20200227&lt;/version&gt; &lt;configuration&gt; &lt;webApp&gt; &lt;contextPath&gt;/$&#123;project.build.finalName&#125;&lt;/contextPath&gt; &lt;/webApp&gt; &lt;stopKey&gt;CTRL+C&lt;/stopKey&gt; &lt;stopPort&gt;8999&lt;/stopPort&gt; &lt;scanIntervalSeconds&gt;10&lt;/scanIntervalSeconds&gt; &lt;scanTargets&gt; &lt;scanTarget&gt;src/main/webapp/WEB-INF/web.xml&lt;/scanTarget&gt; &lt;/scanTargets&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt;&lt;/project&gt; 然后通过mvn jetty:run就可以运行了，而且还支持热部署，当你修改源文件后，会自动重新编译加载。 调试 调试是通过java远程调试实现的，设置maven_opts环境变量后运行mvn jetty:run或者通过vscode的task功能运行，配置如下： 1234567891011121314151617181920&#123; // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format "version": "2.0.0", "tasks": [ &#123; "label": "Jetty debug", "type": "shell", "command": "mvn jetty:run", "group": "build", "isBackground": false, "problemMatcher": [], "options": &#123; "env": &#123; "maven_opts": "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000" &#125; &#125; &#125; ]&#125; 然后在debug视图中创建launch.json： 123456789101112131415&#123; // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 "version": "0.2.0", "configurations": [ &#123; "type": "java", "name": "Debug (Attach) - Remote", "request": "attach", "hostName": "localhost", "port": 8000 &#125; ]&#125; 接下来就可以愉快的调试了✌。]]></content>
      <categories>
        <category>spring mvc</category>
      </categories>
      <tags>
        <tag>vscode</tag>
        <tag>debug</tag>
        <tag>spring mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解feign、ribbon和hystrix三者的关系及超时配置]]></title>
    <url>%2F2020%2F03%2F23%2F029-hystrix-ribbon-feign-relationship%2F</url>
    <content type="text"><![CDATA[spring cloud中有几个重要的组件，深入理解它们之间的关系才能更好的使用它们： ribbon：实现服务定位和客户端负载均衡； hystrix：实现服务熔断、服务降级、资源隔离等； feign：声明式的http客户端，用于服务之间的http调用。相比于resttemplate，feign与ribbon和hystrix集成更友好，是spring cloud的顶层组件。 上述ribbon和hystrix都是netflix贡献组件，目前它们都处于维护模式，不再增加新特性，将逐渐被spring cloud官方组件取代，例如从Hoxton.M2开始整合spring-cloud-loadbalancer用于替换ribbon，但目前还不成熟，还是老老实实用ribbon，而断路器方面spring cloud抽象了Spring Cloud Circuit Breaker，hystrix只是其中一个实现，还有其他实现可选，例如阿里贡献的sentinel。 三者关系 hystrix和ribbon并没有直接关系。 feign底层默认是通过ribbon进行服务定位和负载均衡，使用feign时你感知不到ribbon的存在，也可以不使用ribbon。如果你使用resttemplate，则需要通过@LoadBalanced使用ribbon。 hystrix有两种使用情况，一种是在controller的handler方法上增加@HystrixCommand注解，作用的是整个handler；另一种情况是调用其他app服务时，也就是@feignclient注解的http客户端，此时调用这个http客户端的handler可不需要hystrix。 总结一下：服务之间的http调用可通过feign实现，feign底层是通过ribbon实现服务发现和负载均衡，不管是否使用feign，ribbon都是必不可少的；feign客户端可选的可启用hystrix支持，hystrix也可以用在服务端整个handler上。 如上所示，app1.controll1.handler1调用app2.controller2.handler2的过程是： 如果feign.hystrix.enabled=true（默认为false），则feign通过jdk动态代理，将调用封装为HystrixCommand，在hystrix thread pool中执行，否则进入3和4； hystrix thread pool中执行http调用，还是回调feign接口； feign扩展了ribbon客户端，使用ribbon的服务定位和负载均衡获得可用服务； feign扩展的ribbon客户端发起对app2.controller2.handler2的http请求，ribbon可以开启重试，如果请求超时则自动重试。 了解上述关系对于如何设置超时时间至关重要。如果hystrix的超时时间到达，则1就返回fallback了，不会等到4执行完。一般hystrix的超时时间要大于feign的超时时间。 另外上述服务端设置了断路器，实际上客户端可以不用设置。 超时相关配置 feign 官方参考 有两种配置方式： 属性配置，包括全局和实例； 代码配置，也包括全局和实例。 优先级：实例&gt;全局，属性&gt;代码 属性总是优先的，可以设置feign.client.default-to-properties为false，使得代码配置优先。 全局属性配置名默认是default，可以设置feign.client.default-config为其他名字。 属性配置 12345678910111213feign: hystrix: enabled: true client: config: # 全局配置 default: connectTimeout: 5000 readTimeout: 5000 # 实例配置，feignName即@feignclient中的value，也就是服务名 feignName: connectTimeout: 5000 readTimeout: 5000 代码配置 全局代码配置 12345678910111213141516171819202122package com.example.microservice2;import org.springframework.cloud.openfeign.support.SpringMvcContract;import org.springframework.context.annotation.Bean;import feign.Request;import feign.Retryer;public class FeignClientConfiguration &#123; @Bean public Request.Options feignRequestOptions() &#123; // 默认连接超时10秒，读取超时60秒 return new Request.Options(); &#125; // 默认重试是关闭的 @Bean public Retryer feignRetry() &#123; // 默认重试5次，首次间隔100毫秒，最大间隔1秒 return new Retryer.Default(); &#125;&#125; 1234567891011121314151617package com.example.microservice2;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication@EnableDiscoveryClient //开启注册服务@EnableFeignClients(defaultConfiguration = FeignClientConfiguration.class) //开启feign消费服务public class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; 实例代码配置 123456789101112package com.example.microservice2;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;@FeignClient(value = "microservice1", fallback = HelloServiceHystric.class, configuration = FeignClientConfiguration.class) // 访问微服务1，指定断路器类public interface HelloService &#123; @RequestMapping(value = "/hello", method = RequestMethod.GET) String hello(@RequestParam(value = "name") String name);&#125; hystrix 官方参考 hystrix有四种配置方式： 全局代码默认属性； 全局属性配置； 实例代码配置； 实例属性配置。 优先级：1&lt;2&lt;3&lt;4 和feign一样，也是属性优先代码，实例优先全局。 属性配置 123456789101112131415161718192021222324hystrix: command: #全局默认配置 default: #线程隔离相关 execution: timeout: #是否给方法执行设置超时时间，默认为true。一般我们不要改。 enabled: true isolation: #配置请求隔离的方式，这里是默认的线程池方式。还有一种信号量的方式semaphore。 strategy: THREAD thread: #方式执行的超时时间，默认为1000毫秒，在实际场景中需要根据情况设置 timeoutInMilliseconds: 10000 # 实例配置 HystrixCommandKey: execution: timeout: enabled: true isolation: strategy: THREAD thread: timeoutInMilliseconds: 10000 代码配置 123456789101112131415161718192021222324@RefreshScope // 开启配置更新@RestController@Slf4jpublic class HelloController &#123; @Value("$&#123;serviceName&#125;") // 读配置文件的这个属性 private String serviceName; @RequestMapping("/hello") @HystrixCommand(fallbackMethod = "helloError", commandProperties = &#123; @HystrixProperty(name = "execution.isolation.thread.timeoutInMilliseconds", value = "4000") &#125;) // 指定断路器方法，断路器监控用 public String hello(HttpServletRequest request, @RequestParam(value = "name", defaultValue = "Hugo") String name) throws InterruptedException &#123; log.info("ServerName:&#123;&#125; time:&#123;&#125;", request.getServerName(), DateFormatUtils.format(new Date(), "yyyyMMdd HH:mm:ss")); TimeUnit.SECONDS.sleep(4); return "hello " + name + ", my name is " + serviceName; &#125; public String helloError(HttpServletRequest request, String name) &#123; return "microservice1 hystrix," + name + "!"; &#125;&#125; feign中的hystrix怎么配置 只能通过属性设置，那么commandkey是什么呢？ 1234567package com.example.microservice2;@FeignClient(value = "microservice1", fallback = HelloServiceHystric.class, configuration = FeignClientConfiguration.class) // 访问微服务1，指定断路器类public interface HelloService &#123; @RequestMapping(value = "/hello", method = RequestMethod.GET) String hello(@RequestParam(value = "name") String name);&#125; 上例中默认行为如下，groupkey为&quot;microservice1&quot;，commandkey为&quot;HelloService#hello(String)&quot;，threadpoolkey为null。 1234567891011121314151617181920212223public interface SetterFactory &#123; /** * Returns a hystrix setter appropriate for the given target and method */ HystrixCommand.Setter create(Target&lt;?&gt; target, Method method); /** * Default behavior is to derive the group key from &#123;@link Target#name()&#125; and the command key from * &#123;@link Feign#configKey(Class, Method)&#125;. */ final class Default implements SetterFactory &#123; @Override public HystrixCommand.Setter create(Target&lt;?&gt; target, Method method) &#123; String groupKey = target.name(); String commandKey = Feign.configKey(target.type(), method); return HystrixCommand.Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey(groupKey)) .andCommandKey(HystrixCommandKey.Factory.asKey(commandKey)); &#125; &#125;&#125; 1234567891011121314151617public abstract class Feign &#123; public static String configKey(Class targetType, Method method) &#123; StringBuilder builder = new StringBuilder(); builder.append(targetType.getSimpleName()); builder.append('#').append(method.getName()).append('('); for (Type param : method.getGenericParameterTypes()) &#123; param = Types.resolve(targetType, targetType, param); builder.append(Types.getRawType(param).getSimpleName()).append(','); &#125; if (method.getParameterTypes().length &gt; 0) &#123; builder.deleteCharAt(builder.length() - 1); &#125; return builder.append(')').toString(); &#125; &#125; 所以这样设置： 12345678910hystrix: command: HelloService#hello(String): execution: timeout: enabled: true isolation: strategy: THREAD thread: timeoutInMilliseconds: 10000 也可以改变上述commandkey的默认行为： 12345678910111213141516171819202122232425262728293031package com.example.microservice2;import java.lang.reflect.Method;import com.netflix.hystrix.HystrixCommand;import com.netflix.hystrix.HystrixCommandGroupKey;import com.netflix.hystrix.HystrixCommandKey;import org.springframework.context.annotation.Bean;import feign.Target;import feign.hystrix.SetterFactory;public class FeignClientConfiguration &#123; @Bean public SetterFactory feignHystrixSetter() &#123; return new MySetterFactory(); &#125;&#125;class MySetterFactory implements SetterFactory &#123; @Override public HystrixCommand.Setter create(Target&lt;?&gt; target, Method method) &#123; String groupKey = target.name(); String commandKey = target.type().getName(); return HystrixCommand.Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey(groupKey)) .andCommandKey(HystrixCommandKey.Factory.asKey(commandKey)); &#125; &#125; 这样commandkey就会变成&quot;com.example.microservice2.HelloService&quot;。 ribbon 官方参考 ribbon只有属性配置，同样存在全局和实例配置，格式如下： 1&lt;clientName&gt;.&lt;nameSpace&gt;.&lt;propertyName&gt;=&lt;value&gt; nameSpace是可配置的，默认为ribbon。clientName可为远端服务名，即@feignclient的value，空表示全局配置。 1234567891011121314151617181920# 全局配置ribbon: # 服务最大重试次数,不包含第一次请求，默认0 MaxAutoRetries: 5 # 负载均衡切换次数,如果服务注册列表小于 nextServer count 那么会循环请求 A &gt; B &gt; A，默认1 MaxAutoRetriesNextServer: 3 #是否所有操作都进行重试 OkToRetryOnAllOperations: false #连接超时时间，单位为毫秒，默认2秒 ConnectTimeout: 3000 #读取的超时时间，单位为毫秒，默认5秒 ReadTimeout: 3000# 实例配置clientName: ribbon: MaxAutoRetries: 5 MaxAutoRetriesNextServer: 3 OkToRetryOnAllOperations: false ConnectTimeout: 3000 ReadTimeout: 3000 超时时间关系 feign超时 feign可以设置自身超时，也可以设置ribbon超时，那么它们的关系是怎么样的？看feign代码： 12345678910111213141516public class LoadBalancerFeignClient implements Client &#123; static final Request.Options DEFAULT_OPTIONS = new Request.Options(); IClientConfig getClientConfig(Request.Options options, String clientName) &#123; IClientConfig requestConfig; if (options == DEFAULT_OPTIONS) &#123; requestConfig = this.clientFactory.getClientConfig(clientName); &#125; else &#123; requestConfig = new FeignOptionsClientConfig(options); &#125; return requestConfig; &#125;&#125; 如果没有设置过feign超时，也就是等于默认值的时候，就会读取ribbon的配置，使用ribbon的超时时间和重试设置。否则使用feign自身的设置。两者是二选一的，且feign优先。 如果设置的feign的超时，则超时时间大概是Retryer.Default.maxAttempts*(ConnectTimeout+ReadTimeout) 如果仅设置了ribbon，则超时时间大概是(ConnectTimeout+ReadTimeout)*(MaxAutoRetries+1)*(MaxAutoRetriesNextServer+1)； 建议使用ribbon超时设置。 feign重试和ribbon重试 feign自身重试目前只有一个简单的实现Retryer.Default，包含三个属性： maxAttempts：重试次数，包含第一次 period：重试初始间隔时间，单位毫秒 maxPeriod：重试最大间隔时间，单位毫秒 重试间隔算法如下： 123456789101112131415161718public interface Retryer extends Cloneable &#123; class Default implements Retryer &#123; /** * Calculates the time interval to a retry attempt. &lt;br&gt; * The interval increases exponentially with each attempt, at a rate of nextInterval *= 1.5 * (where 1.5 is the backoff factor), to the maximum interval. * * @return time in nanoseconds from now until the next attempt. */ long nextMaxInterval() &#123; long interval = (long) (period * Math.pow(1.5, attempt - 1)); return interval &gt; maxPeriod ? maxPeriod : interval; &#125; &#125;&#125; 第一次重试间隔period，第二次period*1.5，第三次period*1.5*1.5，…，最大值不超过maxPeriod。 和ribbon的重试相比： 重试次数包含了首次； 不能设置多实例服务切换； 重试有一个延迟时间。 feign超时和hystrix超时 hystrix的超时时间要大于feign的，否则没有等到feign超时，hystrix就fallback了，特别是重试机制会无法起作用。]]></content>
      <categories>
        <category>java</category>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>hystrix</tag>
        <tag>ribbon</tag>
        <tag>feign</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决github pages无法被baidu抓取页面问题]]></title>
    <url>%2F2020%2F03%2F09%2F028-fix-baidu-seo-of-github-pages%2F</url>
    <content type="text"><![CDATA[因为github屏蔽了baidu的页面抓取，所以baidu一直无法索引自建博客。而在国内毕竟baidu的使用量还是更大，为了让自己的博客有更多人访问，还是需要打通这条通道。查找了一些网上资料，解决办法可以通过使用国内的类似服务coding pages作为镜像。 coding在国内云端开发者服务里有良好的口碑，2019年被鹅厂收购，当时广大码农担心又一个好技术被葬送，现在来看有鹅厂的加持，coding发展还是不错的，也为广大开发者提供更多实惠。所以如果你正在选择一个建站方式，直接使用coding pages也是个不错的选择。下面说下已经使用github pages时如何解决baidu无法抓取页面问题。 coding pages 最近coding进行了一系列产品整合升级操作，所以网上有些教程已经过时，大概梳理下时间线： coding成立于2014年，提供一站式云端开发体验，有个人版和企业版两个产品； 2018 年4月与腾讯云达成战略合作，推出了轻量级代码托管产品 「腾讯云开发者平台」； 2019年4月推出全新coding，在将原企业版提权为主打产品，并加入敏捷项目协同、持续集成、制品库、单项目多仓库等新功能及数百项特性改进； 2019年8月腾讯完成对coding的全资收购； 2019年12月28日起，腾讯云开发者平台停止注册； 将coding个人版与腾讯云开发者平台升级至全新coding，2019.12.25~2020.1.20将原有账户全部升级完。 自此coding只有一个入口coding.net，免费版本支持5人以内团队、20个项目和50G空间。 coding pages是coding提供的类似github pages功能，有些教程说要升级腾讯云开发者账户才能使用，现在不需要了，直接coding.net注册即可使用。网上对于coding pages的评价可能还是停留在产品整合升级之前，对其评价并不是太好。目前最新的coding pages，初步使用还算不错，至少比github pages要快很多。 coding pages有一个不太好的地方是，一开始没有找到入口😂。原来默认没有启用入口，新版coding中把pages归到构建与部署功能中，要到项目设置的功能开关中开启。 这样项目导航菜单中才会出现pages入口。 之所以归类到构建与部署功能，也是有道理的，coding的pages功能类似一个特殊的部署，它的主要特性是选择从哪个仓库在什么时候手工还是自动部署成网站，然后给你一个域名，这个域名不像github可以定制为形如”用户名.github.io“这样的，而是自动分配的形如&quot;00kj2k.coding-pages.com&quot;这种的域名。coding pages也没有提供建站模板等功能。 熟悉以上这些后，你应该可以很容易建立自己的coding pages了。注意coding pages并不认仓库中的CNAME文件，需要在界面中配置域名。 镜像方法 接下来就是解决如何将coding pages作为github pages的镜像了，有两种方案： 方案一：本地hexo d时同时部署到coding pages 方案二：通过github持续集成自动同步到coding pages。而持续集成也有很多选择，可以选择github自家的github actions，也可以选择第三方的，如travis-ci。 推荐方案二，并且推荐使用github actions。这里顺便把hexo d也做成自动的，我之前都是每次自己hexo g然后hexo d，现在用上github actions，实在是省心了不少。 github actions github actions是githut自家的持续集成，它的最大的特色是可以复用别人写好的action，这些action也在github的仓库里，github还专门弄了一个市场，供大家选用。 毫不意外的，它里面已经有hexo的部署action，以及仓库的同步action，直接拿来就可以了。 仓库访问凭证准备 我是这样规划仓库的：hexo源文件一个github仓库，github pages一个仓库，coding pages一个仓库 github actions是建在hexo源文件仓库上，执行时先进行hexo部署，推送到github pages仓库（也就是相当于执行hexo d），然后将github pages仓库同步到coding pages仓库。 github pages deploykey deploykey和全局设置里的ssh key类似，区别是针对单个仓库的。 生成密钥对，如下命令生成myssh和myssh.pub文件： 1ssh-keygen -t rsa -b 4096 -C &quot;zhongpan2000@gmail.com&quot; -f D:\myssh 将myssh.pub的内容设置到github pages仓库的deploykey中。 将myssh的内容设置到hexo源文件仓库的secrets中，名为github_pages_deploykey，后续在action使用。 coding pages token coding没有提供仓库级别的ssh key，只能使用token。在项目设置的开发者选项中创建并配置权限，创建后得到用户名和密码两个字符串，通过https://user:password@your.repo.url就可以操作仓库。 将上述&quot;user:password&quot;也存储到hexo源文件仓库的secrets中，名为coding_blog_user_token，后续在action使用。 完整workflow 最后给出完整的git actions脚本。 12345678910111213141516171819202122232425262728name: Hexo Build and Deployon: [push]jobs: hexo-build-deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Cache node modules uses: actions/cache@v1 with: path: node_modules key: $&#123;&#123;runner.OS&#125;&#125;-$&#123;&#123;hashFiles('**/package-lock.json')&#125;&#125; - uses: yrpang/github-actions-hexo@master with: deploykey: $&#123;&#123;secrets.github_pages_deploykey&#125;&#125; username: zhongpan email: zhongpan2000@gmail.com - name: repo-sync uses: wei/git-sync@v1 env: SOURCE_REPO: "https://github.com/zhongpan/zhongpan.github.io.git" SOURCE_BRANCH: "master" DESTINATION_REPO: "https://$&#123;&#123; secrets.coding_blog_user_token &#125;&#125;@e.coding.net/zhongpan/blog.git" DESTINATION_BRANCH: "master" with: args: $SOURCE_REPO $SOURCE_BRANCH $DESTINATION_REPO $DESTINATION_BRANCH DNS解析 最后就是DNS的问题的，自建网站有自己的域名，为了解决baidu抓取页面问题，国内需要解析到coding pages上，具体也可以有两种方式： 方案一：境内访问coding pages，境外访问github pages 方案二：仅baidu访问coding pages，其他访问github pages 推荐方案一，coding pages在境内访问速度非常快，境外还是访问github pages，这样相当于自建了一个CDN。我使用的阿里云域名服务，设置域名解析记录如下： 如果你需要启用https，会遇到一个问题，在coding pages中申请证书失败，是因为证书是通过Let’s Encrypt申请的，这是国外的一个服务，它验证站点的时候会经过上述DNS解析到github pages上去，解决办法是先暂定境外线路解析，申请成功之后再开启。申请证书有效期是3个月，到期后需要重新申请。github pages也是通过Let’s Encrypt申请证书，也是3个月有效期，但是到期会自动帮你申请。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>github actions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradle多模块Spring Boot项目问题总结]]></title>
    <url>%2F2020%2F03%2F04%2F027-create-multi-module-spring-boot-gradle-project%2F</url>
    <content type="text"><![CDATA[模块化是设计软件的一个基本手段，将软件模块化使得模块可以被复用，可以独立维护。Spring boot应用通常使用Spring initializr创建，它是不支持多模块创建的。多模块构建的能力是构建工具所提供，也就是Maven或Gradle。一些高级的IDE，如JIDE，可以支持多模块创建，就是帮你修改好Maven或Gradle的脚本。你可能使用的vscode这类编辑工具，但是也想体验一下多模块，那么就只能手工修改构建脚本。 Gradle是基于Groovy语言构建出来的一个特定领域语言(DSL)，用来进行项目的构建。我觉得它和C++领域的CMake比较像，但是CMake是完全特化的脚本语言，而Gradle是基于JVM上的脚本语言Groovy构建，其编程和扩展能力远远超过CMake，Gradle也支持C++语言的构建。一个Gradle脚本是后缀为gradle的文件，其中通常为DSL语句，具有特定的格式，这能降低脚本的编写难度，同时你也完全可以参杂进Groovy语句，实现更复杂的功能。同时借助插件，还可以很容易扩展这个DSL，实际上Gradle大部分功能都是通过插件实现的。 从网络上可以搜索到很多关于如何构建Gradle多模块项目的资料，但是它们都有共同的问题，所使用的Gradle版本较低，按照其方法都有一些问题。本文基于目前最新的版本，总结一下创建多模块Gradle项目的问题。 版本 Spring boot 2.2.5 Spring boot 2.2.5使用的Gradle版本是6.0.1 官方文档 最权威的关于如何创建多模块项目的文档莫过于官方文档，地址如下： Spring官方：https://spring.io/guides/gs/multi-module/ Gradle官方：https://guides.gradle.org/creating-multi-project-builds/ 所以当你遇到问题的时候，多去找原文、找源头。 Gradle插件引入方式变化 参照网络上的文章（例如此文VS Code开发Spring Boot + Cloud示例（四）Spring Boot + Gradle多项目框架）出现问题的原因，是你使用了新版本的Gradle，其DSL语法有变化，主要的变化是插件引入的方式变了。 以前的方式： 1apply plugin: 'java' 新的方式： 123plubins &#123; id 'java'&#125; 具体从哪个版本默认采用新的方式，我没有去考证，两者的区别见这里。plugins存在一些限制，不能放在subprojects或allprojects里面，且必须放在一开始。问题就出在这里，如果你按文章里面的将根项目build.gradle的内容全部移到subprojects里面，就会报如下错误： Could not find method plugins() for arguments … 如果想在根项目引入插件，在子模块复用，可以这样做，注意核心插件不能使用version和apply： 12345678910111213plugins &#123; id 'org.springframework.boot' version '2.2.5.RELEASE' apply false id 'io.spring.dependency-management' version '1.0.9.RELEASE' apply false id 'java'&#125;subprojects &#123; apply plugin: 'org.springframework.boot' apply plugin: 'io.spring.dependency-management' apply plugin: 'java'&#125; 常用插件 开发Spring boot应用常用如下插件： 123456plugins &#123; id 'org.springframework.boot' version '2.2.5.RELEASE' id 'io.spring.dependency-management' version '1.0.9.RELEASE' id 'java' id 'java-library'&#125; 插件文档： dependency management插件（社区插件）: org.springframework.boot Spring boot插件（社区插件）: io.spring.dependency-management java语言插件（核心插件）：java java库开发（核心插件）：java-library 社区插件可以在门户https://plugins.gradle.org上查询(不过没有统一的插件使用说明文档，不太方便)，核心插件随Gradle发行，使用说明包含在Gradle文档中。 dependency management插件 这个插件提供了类似maven的依赖管理功能，通过如下block定义，其中定义的依赖，在Gradle dependencies中使用可以省略版本号。 123dependencyManagement &#123;&#125; 常用的方式是通过maven bom导入依赖： 123456789dependencyManagement &#123; imports &#123; mavenBom &apos;io.spring.platform:platform-bom:1.0.1.RELEASE&apos; &#125;&#125;dependencies &#123; implementation &apos;org.springframework.integration:spring-integration-core&apos;&#125; Spring boot插件 这个插件为Spring boot应用开发提供了一些便利。例如和dependency-management插件一起使用可以自动导入Spring boot的maven bom。 123456789plugins &#123; id 'org.springframework.boot' version '2.2.5.RELEASE' id 'io.spring.dependency-management' version '1.0.9.RELEASE' id 'java'&#125;dependencies &#123; implementation 'org.springframework.boot:spring-boot-starter-web'&#125; 等价于： 1234567891011121314plugins &#123; id 'io.spring.dependency-management' version '1.0.9.RELEASE' id 'java'&#125;dependencyManagement &#123; imports &#123; mavenBom("org.springframework.boot:spring-boot-dependencies:2.2.5.RELEASE") &#125;&#125;dependencies &#123; implementation 'org.springframework.boot:spring-boot-starter-web'&#125; 还提供了bootRun任务启动应用： 1gradle bootRun java插件的依赖配置变化 有一个不一样的地方可能会困扰你，网上的很多文章里面会出现： 123dependencies &#123; compile('org.springframework.boot:spring-boot-starter-web')&#125; 但是最新的是这样的： 123dependencies &#123; implementation 'org.springframework.boot:spring-boot-starter-web'&#125; 这是因为在最新的java插件中依赖配置有变化，有一些deprecated了，如下灰色字体，例如compile，详见。 java-library提供如下依赖配置： spring-boot-devtools不生效 spring-boot-devtools主要提供Automatic restart和LiveReload功能，极大的提升了开发调试的效率，在Spring Initializr中创建项目时选择DevTools，会生成如下构建脚本： 1234567891011121314configurations &#123; developmentOnly runtimeClasspath &#123; extendsFrom developmentOnly &#125; compileOnly &#123; extendsFrom annotationProcessor &#125;&#125;dependencies &#123; developmentOnly 'org.springframework.boot:spring-boot-devtools'&#125; 在IDE中通过调式运行应用时，devtools功能一切正常，但是如果你通过如下命令启动应用，会发现自动重启并没有生效。 1gradle bootRun 虽然bootRun会自动识别devtools，但是还是需要先编译，因为devtools默认监控的classpath下文件改变，另外重新编译后才能看到修改的效果。为了自动编译，可以通过gradle的连续构建实现，再执行如下命令： 1gradle build --continuous 保持上述两个命令都开着，这时修改文件就会先触发build，然后再触发bootRun重启。 参考： Continuous Auto-restart With Spring Boot DevTools and Gradle SpringBoot配置devtools实现热部署 理解依赖配置 启用devtools工具后，依赖配置configurations中会出现自定义配置developmentOnly，这是什么目的呢？ configurations里面的内容，每一项称为一个dependency configuration，其实是对dependency的一个分组，在dependencies往这个分组里面加入dependency，gradle对每个configuration解析出一个依赖树，供task使用。 每一个dependency configuration代表一定角色，决定了： 构建生命周期的哪些步骤会使用分组中的依赖 分组中的依赖会传递吗，是传递到消费模块的编译期还是运行期，还是都有 分组中的依赖会打包进构建输出吗 前文的java及java library插件预定义了一些dependency configuration，如implementation、compileOnly等，它们的角色由插件预定义好。 implementation compileOnly runtimeOnly api 构建阶段 compile time runtime test compile time test runtime compile time runtime compile time runtime test compile time test runtime 传递到消费模块编译期 no no no yes 传递到消费模块运行期 yes no yes yes 打包到输出 yes no yes yes 对于devtools，它的实现原理是启用特殊的类加载器，仅用在开发调式阶段，所以它不会应用到构建阶段，也不需要传递到消费模块，也不需要打包到输出，所以这里就自定义了一个依赖配置developmentOnly，目的就是不参与构建阶段、不传递、不打包，这个依赖配置也可以用别的名字。 那么devtools又是怎么生效的呢： 在IDE中进行调试运行时，检测到devtools时就会使用其中的&quot;restart&quot;类加载器加载打开的工程，其他jar使用&quot;base&quot;类加载器加载； 使用gradle bootRun运行时，检测到devtools时同上 参考： Faster Development with Spring Boot DevTools Spring boot devtools tutorial Maven Scopes and Gradle Configurations Explained]]></content>
      <categories>
        <category>java</category>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>gradle</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用在线表格airtable进行软件开发管理]]></title>
    <url>%2F2020%2F02%2F22%2F026-development-management-with-airtable%2F</url>
    <content type="text"><![CDATA[2019年的冬天爆发新冠病毒疫情，而我正处于这次疫情的中心武汉，除夕我带着大儿女回到武汉郊区的农村老家，而老婆带着刚满一岁的小儿女留在武汉城区，那个时候已经公布人传人，武汉也刚封城，我虽然意识到严重，但还没有惊慌，执意回了老家。直到后来确诊人数暴增，大量疑似病人无法入院治疗，网络上各种无助的求救，才开始担心起来。现在回想起来决定回老家非常冒险，万一感染亲人将不堪设想。 春节过后，因为疫情，工作中多了一些相关的统计工作，例如统计在家远程办公时长、统计复工情况、统计外地返汉信息等等，经常需要把团队的反馈合并到一起，非常的繁琐。于是我开始寻找一款能够协同编辑的在线表格，让我找到了Airtable，瞬间解决了困扰我的问题，真是如获至宝。 随着复工时间一再推迟，在家远程办公将成为一种常态，怎么做好远程办公是需要好好琢磨的一个事情。怎么样让大家分散各地，但是能够行动一致、高效的工作，是非常有挑战的。这周经过一番折腾，利用Airtable把团队的任务管理起来，我觉得是一个很好的尝试，也让我更加喜欢上Airtable。 Airtable解决了一个很重要的问题，就是度量和可视化。所谓没有度量就没有改进，管理中需要量化各种数据，这些数据会成为一把戒尺，有意无意中规范影响大家的行为。选择什么的度量指标，是因时因事而异的。再好的项目管理软件也不可能面面俱到，满足各种需求。而Airtable的纯粹，就是为了处理数据、显示数据，可随心所欲定制，成为了它的过人之处。 下面我就详细说说，我是怎么几小时内就用Airtable打造一个软件开发任务管理系统，我对它非常满意，而如果要开发一个类似的项目管理系统绝不是一时半会能办到的。 Airtable简介 Airtable是一个国外的在线服务，注册后即可使用，分免费和付费版本。Airtable中按照workspace-base-table三级来组织数据，一个workspace中可以有多个base，一个base下有多个table。base下的table之间可以关联，但是跨base的表是不能关联的。 Airtable免费版本不限制base数目，只限制每个base最多1200条记录，2G附件空间，具备团队协作功能，这就完全可以满足一个小团队的使用，大不了数据满了再开一个base，非常厚道。收费版本是按照每workspace以及workspace中的用户数来计算费用，记录数和空间更大，具备高级功能，首次注册可试用15天pro版。 与excel区别 Airtable和excel虽然有相似功能，但设计理念完全不同于excel。Airtable的base和数据库很像，其中每个table，每一列有固定类型和含义，而excel就是二维的单元格，可随心所欲编辑。正因为如此，Airtable和excel在公式的使用上存在很大不同，例如： Airtable的公式是定义在列上的，也就是应用在列的所有单元格，而excel的每个单元格可以定义不同的公式； Airtable的公式不能跨table，需要借助关联表实现跨table，并且公式总是访问本行数据，而excel的公式可以访问其他sheet数据，也可以访问本sheet任意单元格数据。 实际上Airtable不是一个在线excel，它更像一个开发平台，凡是基于关系数据的信息处理系统，都可以用它实现。 主要功能 视图 用于展现和操作表的界面，有Grid、Form、Calendar、Gallerv、Kanban几种，Grid是最常用的，新建table时默认建立一个Grid视图。一个table可以建多个视图。Grid视图中有过滤、分组、排序、着色等功能。过滤不像excel那样在列上操作，感觉没有excel方便，每个视图可以设置不同的过滤条件。 表建模和数据操作 就像设计关系数据库一样，使用Airtable提供的字段类型，将表及关联设计出来。字段类型大概有这么几类： 输入字段，例如文本、数字、选择、checkbox等； 公式字段，是基于其他字段计算出来的； 表和表之间的关联是通过特殊的字段类型Link to another record建立，基于关联字段可以做一些查找和聚合计算，包括rollup、lookup和count几个字段类型，可以实现丰富的报表统计； 另外还有自增、创建时间、最后更新时间等有用的字段类型。 blocks 可以理解成基于上述表的小应用。主要有可视化、数据操作、报表等应用。感觉下一步会做成开放平台，第三方也可以开发blocks，拭目以待。 协作 Owner可以邀请其他人一起协同工作，可基于workspace级别或base级别，不能基于表级别。协同者有Creator、Editor、Commenter、Read only几种角色，权限由高到低。协同编辑时可以实时看到其他人编辑过程，很酷的体验。 模板 也就是预定义好的表模型，创建base时选择，开箱即用。 理解表的关联 表的关联是实现复杂的报表统计的关键。 主键 每个表的第一列总是主键，不能移动或删除。主键倒不是说内容必须唯一，而是在关联表字段选择时呈现的就是主键的内容，最好能够区分不同的行，但不是强制的。实际上Airtable内部有每一行的唯一标识，可以通过公式RECORD_ID()获取。 Link to another record字段类型 建立A表到B表的关联，是在A表中建立一个Link to another record类型字段，并选择B表。那么在编辑此字段会弹出选择界面列出B表的所有记录。此字段可以设置是否允许多选，如果否，则只能选择一条记录，那么A和B就是多对一的关系。如果允许多选，那么A和B就是多对对的关系。A中建立关联字段后，B表中也会自动建立一个关联字段和A关联，这个字段默认是允许多选的。两边是联动的，都可以进行编辑，一般我们会将一边隐藏掉，总是从另一边进行编辑。 rollup、lookup、count字段类型 这几个字段类型必须先建立Link to another record字段，通过Link to another record字段的内容进行查询或计算。 lookup和rollup是针对关联的表中相关记录的某个字段，选择Link to another record字段字段后还需要选择关联表的某个字段，区别是rollup还可以择应用一些聚合函数，例如SUM、MAX等； lookup和rollup只能对关联表的一个字段进行统计，如果希望作用于多个字段，可以在关联表中建立一个公式字段先把多个字段组合在一起； count只需选择哪个Link to another record字段，统计选择的个数，其实用rollup也可以实现，相对于是对rollup的特化。 值类型 上述字段的值类型，在官方文档中没有找到明确说明，通过测试验证结果如下： Link to another record 公式字段访问时为字符串，如果是多选，则是&quot;, &quot;分隔的字符串。如果要统计其个数，一种方法是建立count字段，另一个方法是通过公式LEN({field})-LEN(SUBSTITUTE({field给},&quot;,&quot;,&quot;&quot;))+1 rollup字段聚合函数访问时为数组 lookup：同上 rollup：同聚合函数的返回类型，其中有几个特别注意是返回的数组类型，包括ARRAYCOMPACT、ARRAYFLATTEN、ARRAYUNIQUE，它们的返回值可以应用COUNTA公式得到数组长度，如果需要当成字符串使用则要使用ARRAYJOIN或CONCATENATE先转成字符串。针对数组类型，我遇到一个需求，想把两个数组连接起来，并去掉重复值，Airtable提供了去除数组重复值的公式，但是没有提供连接数组的公式，没有办法实现这个需求，有所遗憾。 count：数字 实现目标 我的出发点很简单，希望大家有一个统一的视图，都可以看到有哪些任务，任务的进展状态。同时能够量化显示完成率，完成得分等数据并进行排序，形成彼此之间的对比，从而让大家清楚做什么并努力达成目标。 组织结构 我管理的团队共26人，我希望划分成若干个小组，通过小组长帮助我管理好任务。小组长要负责任务的计划、跟踪和状态更新。 管人方面 我希望看到每个人的工作量及排序，以此确定任务安排是否饱满和均衡； 我希望看到每个人的任务完成情况得分及排序，这个得分体现了是否按时完成，完成质量如何，以此牵引大家按时保质完成任务； 我希望团队组长每周汇报团队相关任务的进展，以便我及时发现问题，做出调整； 管事方面 任务有两种，一是开发任务，一种是Bug处理任务，这两种是存在区别的，我希望需要区别对待； 我希望任务是分级的，类似scrum的Epic-Theme-User Story，我只关注顶层的任务，团队组长关注下级任务，我觉得2级就够了，至于下级任务按什么方式定，是按scrum的固定周期迭代，还是长任务，不是我关心的； 我希望从顶层任务的视角能够看到完成率及排序，从而清楚总体的任务完成情况如何； 我希望从顶层任务的视角能够看到工作量及排序，从而清楚工作投入的分布； 我希望具体任务视图能够有一些明显提示任务快到期或已经超时。 创建表 戳这里，进入我分享的base，详细查看字段类型和公式，还可以实际体验一下。 根据以上需求，首先建立表结构，如下图所示。 （1）上述聚合关系表示关联表，n:1是单选，m:n是多选； （2）团队和成员表定义了团队分组； （3）Backlog是一级任务，任务和Bug是二级任务；工作量和完成信息定义在二级任务里面； （4）Backlog通过任务分类表进行一个分类； （5）Bug级别表中定义了一些参数，用于计算Bug工作量； （6）任务可能由多个人承担，例如前端和后端都需要开发的功能，所以可以分派给多个人；Bug只能分派给1个人； （7）任务和Bug中的负责人，用于提示此人负责任务的创建、跟踪更新及评价，完成表格的所有编辑工作，编辑工作不能开放给所有成员。任务分派给多人正好跨团队时，责任人等于创建人，否则等于团队负责人。Bug的负责人就是分派给的团队负责人； （8）周报的粒度是一级任务，每条一级任务每周汇报一次。 总结几点使用技巧： （1）下拉选择的字段，可以创建一个table，然后与其关联。这样一来可能增加可定制性，有变化时不用修改表定义，只需修改表数据，二来可以基于选择内容进行统计； （2）隐藏字段用处大，就像变量一样。有时必须借助隐藏字段才能实现功能，例如统计完成率，下面会有介绍；有时可以简化公式，还能增加复用性；还例如过滤功能不能进行复杂的逻辑组合，只能要么全部与，要么全部或，这时可以通过增加隐藏的公式字段来实现复杂逻辑。 指标设计 工作量统计 对于任务：由团队负责人给出任务估计工作量，如果任务由多人承担，则每个取平均值，然后在成员表和Backlog表中建立rollup字段，对任务表工作量进行sum，得到任务总工作量。 对于Bug：采用简化处理，工作量直接和Bug级别相关，等于{Bug级别权重}*{Bug工作量基数}，这两个参数定义在Bug级别表中，同样在成员表和Backlog表中建立rollup字段，对bug表工作量进行sum，得到Bug总工作量。 再分别建立公式字段，将上述两个字段相加得到总工作量。 这样从成员表和Backlog表都可以看到汇总的工作量情况。 负荷率统计 主要针对任务，衡量某个人的任务安排是否饱满合理，体现了团队负责人任务计划的能力。合理的负荷率应该是比100%稍高，如果负荷过高通常是估计工作量过高或计划完成时间不合理，也有可能没有考虑一个人分派多个任务的叠加情况。如果负荷过低只有一种可能，就是安排任务过少。 在任务表中建立公式字段计算任务周期，也就是任务从开始到结束持续的时长，这里将创建时间当成任务开始时间。 1IF(&#123;计划完成时间&#125;,(DATETIME_DIFF(&#123;计划完成时间&#125;,&#123;创建时间&#125;,&quot;seconds&quot;)/(60*60*24)),0) 然后建立公式字段，计算平均日工作量，单位小时。 1IF(&#123;任务周期(天)&#125;&lt;=0,0,IF(IS_AFTER(NOW(),&#123;计划完成时间&#125;),0,&#123;平均工作量(天)&#125;*8/&#123;任务周期(天)&#125;)) 然后在成员表中建立rollup字段，对任务表的平均日工作量sum，得到这个人的任务平均日工作量，最后建立公式字段，得到平均任务负荷率。 1&#123;任务平均日工作量(小时)&#125;/8 完成率统计 rollup的count聚合函数没有条件过滤特性，所以没法一步到位，需要借助中间字段。 首先在任务表和Bug表中建立公式字段，根据状态计算是否完成，完成则填入值，为完成为空。 然后在成员表和Backlog表中建立rollup字段，通过counta聚合函数计算完成数，同时建立count字段计算总数，最后建立公式字段，完成数除以总数，得到完成率。 评分系统 设计评分规则如下： 得分={评分}(只有任务有，Bug无，按时完成才能得到)+{工作量(天)}(完成才能得到)+{提前(天)}(负数表示超期，完成或超期时开始计入) 评分体现完成质量，前提是按时完成才能得到，超期不能得到，团队负责人要对任务进行验收，验收通过才能视为完成；工作量体现工作付出，必须完成才能得到，也就说要负责到底；提前体现了鼓励大家增量付出。评分规则的牵引方向是按时完成任务，鼓励提前，提前有加分，超期有扣分。 对于任务 提前： 1IF(&#123;计划完成时间&#125;,IF(&#123;Completed&#125;,DATETIME_DIFF(&#123;计划完成时间&#125;,&#123;状态最后更新时间&#125;,&quot;s&quot;),DATETIME_DIFF(&#123;计划完成时间&#125;,NOW(),&quot;s&quot;)),0) 得分： 1IF(AND(&#123;提前(秒)&#125;&gt;=0,&#123;Completed&#125;),&#123;评分&#125;,0)+IF(&#123;Completed&#125;,&#123;工作量(天)&#125;,0)+IF(OR(&#123;Completed&#125;,&#123;提前(秒)&#125;&lt;0),&#123;提前(秒)&#125;/(60*60*24),0) 对于Bug 提前： 1IF(&#123;计划解决时间&#125;,IF(&#123;Resolved&#125;,DATETIME_DIFF(&#123;计划解决时间&#125;, &#123;状态最后更新时间&#125;,&quot;s&quot;),DATETIME_DIFF(&#123;计划解决时间&#125;,NOW(),&quot;s&quot;)),0) 得分： 1IF(&#123;Resolved&#125;,&#123;工作量(天)&#125;,0)+IF(OR(&#123;Resolved&#125;,&#123;提前(秒)&#125;&lt;0),&#123;提前(秒)&#125;/(60*60*24),0) 然后在成员表和Backlog表中通过rollup字段对得分进行求和汇总就得到总得分。 总结 你是否也喜欢上Airtable，短短数小时之内，你就完全可以打造一款为你量身定做的表格系统。Airtable中已经包含许多适用于特性用途的模板，可以直接拿来使用，非常方便。 Airtable的统计功能主要是基于关联表和rollup类型字段，其设计非常简洁，甚至简陋，聚合函数功能并不算丰富，只能基于一列聚合，但是其设计又恰到好处，通过变通技巧还是可以实现复杂功能。另外Airtable提供了curl和JavaScript接口，可以集成第三方工具实现更加复杂的功能，例如和IFTTT或Zapier集成。 当然Airtable也不是万能的，它并不能取代项目管理系统，它还是一个表格系统，缺乏很多项目管理必要的特性，例如工作流管理等等。我主要是在管理团队中使用Airtable，其他人只读查看，因为编辑权限是base粒度的，不能放开给所有人。但是我的需要也不是一个完整的项目管理系统，我只需要能够灵活定制并呈现度量指标，Airtable完全可以胜任。此前我使用过JIRA、Redmine这类项目管理系统，它们也可以自定义字段，但是没有公式和聚合统计这种特性，通常需要通过插件实现，所以定制性受到很大限制，期待有一天它们能够吸收Airtable的特长，让鱼与熊掌可以兼得。]]></content>
      <categories>
        <category>management</category>
      </categories>
      <tags>
        <tag>airtable</tag>
        <tag>software devemopement management</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有用的c++内存相关库]]></title>
    <url>%2F2020%2F02%2F06%2F025-useful-cpp-memory-related-libraries%2F</url>
    <content type="text"><![CDATA[c++对内存的原始控制是其优势，同时带来一系列灾难性问题，例如野指针问题、内存泄漏问题、内存碎片问题，在c++世界这是非常常见而棘手的问题，其实这些问题早有成熟应对方案，就以boost为例，早就包含相关库，有些已经成为c++标准库。c++解决内存方面的技术有： （1）智能指针：解决野指针、内存泄漏问题； （2）内存池：提升内存分配效率，解决内存碎片问题； （3）flyweight：解决大量重复对象对内存的浪费。 下面通过一些实例介绍boost对上述技术的实现。 boost智能指针 boost智能指针已经成为c++11标准，是时候告别裸指针了。boost智能指针包括如下内容： 智能指针模板类 scoped_ptr，scoped_array shared_ptr，shared_array（deprecated） weak_ptr intrusive_ptr local_shared_ptr 实用函数和类 make_shared make_unique allocate_unique enable_shared_from_this pointer_to_other static_pointer_cast intrusive_ref_counter atomic_shared_ptr 使用示例 scoped_ptr scoped_ptr适用于作用域内的指针管理，所有权不能转移出去，而标准库中的unique_ptr可以转移，这是两者的主要区别。 先看不使用智能指针的例子，在每个退出或异常的地方需要不厌其烦的检查动态分配的对象是否释放，一旦漏掉一个地方就会导致内存或资源泄露。 1234567891011121314151617181920212223242526272829303132333435363738void fun()&#123; int *pInt = NULL; try &#123; pInt = new int(1); bool bOk = false; //do something if (!bOk) &#123; if (pInt != NULL) &#123; delete pInt; pInt = NULL; &#125; return; &#125; //do something //maybe throw exception delete pInt; pInt = NULL; &#125; catch(...) &#123; if (pInt != NULL) &#123; delete pInt; pInt = NULL; &#125; &#125; return;&#125; 使用智能指针改写后的例子，不但代码更加简洁，也完全杜绝了内存泄露的后顾之忧。 1234567891011121314151617181920212223242526void fun()&#123; int *pInt = NULL; try &#123; boost::scoped_ptr&lt;int&gt; pInt(new int(1)); bool bOk = false; //do something if (!bOk) &#123; return; &#125; //do something //maybe throw exception &#125; catch(...) &#123; &#125; return;&#125; 上述例子充分体现了智能指针中对RAII（resource acquisition is initialization）思想的运用，由一个临时对象持有裸指针，在临时对象销毁时同时释放裸指针指向的资源，而编译器可以很好的保证临时对象在各种情况下正确的销毁。从scoped_ptr的名字可以看出，它应用于离开某个作用域后需要资源自动释放的场景，因此scoped_ptr不像std::auto_ptr那样具有难以使用的所有权转让语义，也不像shared_ptr那样具有共享所有权语义，它将资源的生命周期仅仅限定在某个作用域内，这样的设计使它意图非常明确和简单，实际上scoped_ptr是不可复制的，当然就不能应用在容器中，因为它不需要转让所有权或共享所有权，因此在编译期就可以避免不正确的使用（不能将一个scoped_ptr赋值或拷贝构造给另一个scoped_ptr，也不能将scoped_ptr存储在容器中）。 选择使用scoped_ptr还是std::auto_ptr？scoped_ptr和auto_ptr很像，只是auto_ptr多了所有权转让语义，例如可以作为函数返回值，除此之外，他们都是用栈上的对象管理堆上的对象，都不能共享所有权，因此都不能保存在容器中，但是scoped_ptr作了严格的控制（赋值和拷贝构造函数是私有的），确保了使用者不会误入歧途，而对于auto_ptr，只要你愿意，你还是可以将其放入容器中。所以，一般情况下最好使用scoped_ptr，如果你确实需要所有权转让语义，可以使用auto_ptr，但必须非常小心。 shared_ptr 不使用智能指针的例子： 1234567891011121314151617181920212223242526272829303132333435363738std::vector&lt;int *&gt; vecIntPtr;vecIntPtr.push_back(new int(1));vecIntPtr.push_back(new int(2));boost::mutex mtx;//thread1&#123; &#123; boost::mutex::scoped_lock lock(mtx); for(std::vector&lt;int *&gt;::iterator it = vecIntPtr.begin(); it != vecIntPtr.end(); ++it) &#123; delete *it; &#125; vecIntPtr.clear(); &#125;&#125;//thread2&#123; int *pInt = NULL; &#123; boost::mutex::scoped_lock lock(mtx); if (!vecIntPtr.empty()) &#123; pInt = *vecIntPtr.begin(); &#125; //maybe need long time if (pInt) &#123; *pInt = 3; &#125; &#125;&#125; 使用shared_ptr改写后： 123456789101112131415161718192021222324252627282930313233343536typedef boost::shared_ptr&lt;int&gt; IntPtr;std::vector&lt;IntPtr&gt; vecIntPtr;vecIntPtr.push_back(IntPtr(new int(1)));vecIntPtr.push_back(boost::make_shared&lt;int&gt;(2));boost::mutex mtx;//thread1&#123; &#123; boost::mutex::scoped_lock lock(mtx); vecIntPtr.clear(); &#125;&#125;//thread2&#123; IntPtr pInt; &#123; boost::mutex::scoped_lock lock(mtx); if (!vecIntPtr.empty()) &#123; pInt = *vecIntPtr.begin(); &#125; &#125; //maybe need long time if (pInt) &#123; *pInt = 3; &#125;&#125; 上例展示了多线程情形下访问同一份指针的情况。看出两份代码之间的区别了吗？ 前面的代码需要显式的调用delete，而后面的代码不需要，其好处是显而易见的，在上述简单的例子中，也许你觉得不可能忘了delete 指针，但是在更复杂的系统中，忘了delete是屡见不鲜的，而查找此类问题需要花费高昂的代价； 前面的代码需要在使用指针的整个过程加锁，即使从容器中获取了某个指针，以防其他线程将其删除，在使用指针的整个过程都需要加锁，如果这个过程非常耗时，那么可能带来系统效率的低下，而后面的代码仅仅是在从容器获取指针的过程需要加锁，这是由容器的性质决定的，STLport容器保证多线程读同一个容器是安全的，但不保证多线程写同一个容器是安全的，因此从容器获取指针的过程必须人为保证其安全性，但是一旦获取到了指针，后面对指针的使用过程就不需要加锁了，这是如何保证的呢， shared_ptr为每个裸指针维护一个引用计数，所有shared_ptr对象（临时对象或容器中的对象）共享裸指针和这个引用计数，创建一个shared_ptr对象时引用计数加1，对象销毁时引用计数减1，当引用计数为0时就说明已经没有人需要此裸指针了，此时正是裸指针生命结束的时候。因此只要你获取了shared_ptr对象，在使用过程中引用计数就肯定不可能为0，所以你可以放心的使用此指针。 需要注意的是：shared_ptr确保了指针本身使用的安全，指针内部数据的安全性仍需要使用者自己来保证，这不是智能指针关注的事情。 另外注意到boost::make_shared的使用了吗？使用make_shared一方面可以去除new的显式调用，更重要的是可以获得性能的提升（原因见后），建议尽量使用make_shared，如果编译器支持右值引用，make_shared可以完美的将参数传给构造函数而没有任何性能的损失。 关于shared_ptr的使用有几点是需要注意的： 禁止some_operation (boost::shared_ptr(new T), return_int_operation())用法，因为参数求值顺序是不确定的，可能先执行new T，然后return_int_operation()，然后构造shared_ptr，如果return_int_operation()抛异常，那么就会出现内存泄漏； 避免对shared_ptr所管理内存直接操作，以免重复释放； 关于类型转换，只要 T* 能被隐式地转换到 U*，则 shared_ptr&lt;T&gt;就能被隐式地转换到shared_ptr&lt;U&gt;。特别是，shared_ptr&lt;T&gt;隐式转换到shared_ptr&lt;T const&gt;，当U是T的一个可访问基类的时候，还能转换到shared_ptr&lt;U&gt;，以及转换到shared_ptr&lt;void&gt;，另外可用如下函数类型转换： 12345678template&lt;class T, class U&gt;shared_ptr&lt;T&gt; static_pointer_cast(shared_ptr&lt;U&gt; const &amp; r); // never throwstemplate&lt;class T, class U&gt;shared_ptr&lt;T&gt; const_pointer_cast(shared_ptr&lt;U&gt; const &amp; r); // never throwstemplate&lt;class T, class U&gt;shared_ptr&lt;T&gt; dynamic_pointer_cast(shared_ptr&lt;U&gt; const &amp; r); // never throws 所有共享同一裸指针的shared_ptr必须同源，例如不能出现如下代码： 123int *p = new int(10); boost::shared_ptr&lt;int&gt; sp1(p); boost::shared_ptr&lt;int&gt; sp2(p); 应该是： 123int *p = new int(10); boost::shared_ptr&lt;int&gt; sp1(p); boost::shared_ptr&lt;int&gt; sp2 = sp1; 不要直接使用容器中的智能指针。 weak_ptr 看出下面的代码有什么问题了吗？初始后临时对象father和son的引用计数都是2，当father和son销毁后，引用计数变为1，之后再也没有可能变成0了，于是产生了内存泄露。 1234567891011121314151617181920class CFather;class CSon;typedef boost::shared_ptr&lt;CFather&gt; FatherPtr;typedef boost::shared_ptr&lt;CSon&gt; SonPtr;class CFather&#123;public: SonPtr m_son;&#125;;class CSon&#123;public: FatherPtr m_father;&#125;;FatherPtr father(new CFather);SonPtr son(new CSon);father-&gt;m_son = son;son-&gt;m_father = father; 为了解决循环引用带来的内存无法释放的问题，weak_ptr产生了，只要将循环引用中的一环改为weak_ptr，问题就迎刃而解了。 123456789101112131415161718192021222324252627282930class CFather;class CSon;typedef boost::shared_ptr&lt;CFather&gt; FatherPtr;typedef boost::shared_ptr&lt;CSon&gt; SonPtr;typedef boost::weak_ptr&lt;CSon&gt; SonWeakPtr;class CFather&#123;public: SonWeakPtr m_son;&#125;;class CSon&#123;public: FatherPtr m_father;&#125;;FatherPtr father(new CFather);SonPtr son(new CSon);father-&gt;m_son = son;son-&gt;m_father = father;//how to access weak_ptr&#123; SonPtr son = father-&gt;m_son.lock(); if (son) &#123; //do something &#125;&#125; 我们来看看它是如何工作的： weak_ptr必须由shared_ptr或其他weak_ptr初始化，weak_ptr只是作为shared_ptr的观察者，不会导致shared_ptr的引用计数加1； 在每次使用指针时，还是要获取shared_ptr，方法是调用lock成员函数，实际上，weak_ptr没有重载*和-&gt;，也没有提供get来获取裸指针，所以它是安全的。 那么重写后的代码，是不是解决了内存泄露的问题呢？初始后father的引用计数为2，son为1，son销毁后引用计数变为0，从而会释放son持有的CSon指针，同时CSon的成员m_father也会析构，father的引用计数变为1，father销毁后引用计数变为0，最终father持有的CFather指针得到释放。 intrusive_ptr 123456789101112131415161718192021222324252627282930313233343536373839404142class CSharedObject&#123;public: CSharedObject() : m_ulCnt(0) &#123; &#125; void AddRef() &#123; InterlockedIncrement((long *)&amp;m_ulCnt); &#125; unsigned long ReleaseRef() &#123; return InterlockedDecrement((long *)&amp;m_ulCnt); &#125;private: unsigned long m_ulCnt;&#125;;class CMyObject : public CSharedObject&#123;&#125;;typedef boost::intrusive_ptr&lt;CMyObject&gt; MyObjectPtr; void intrusive_ptr_release(CMyObject *p)&#123; if (p-&gt;ReleaseRef() == 0) &#123; delete p; &#125;&#125;void intrusive_ptr_add_ref(CMyObject *p)&#123; p-&gt;AddRef();&#125; intrusive_ptr用于存储带有侵入式引用计数对象的指针，使用时定义boost::intrusive_ptr&lt;T&gt;的同时，还要定义void intrusive_ptr_add_ref(T*) 和void intrusive_ptr_release(T *)，intrusive_ptr保证需要增加引用计数时调用intrusive_ptr_add_ref，需要减小引用计数时调用intrusive_ptr_release。 使用intrusive_ptr的主要原因有： 一些已有的 frameworks 和操作系统提供带有侵入式引用计数的对象； intrusive_ptr 的内存占用量和相应的裸指针一样； intrusive_ptr&lt;T&gt; 能够从任意一个类型为 T * 的裸指针构造出来。 scoped_array 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// scoped_arraytypedef boost::scoped_array&lt;int&gt; IntArrayPtr;IntArrayPtr intArrayPtr(new int[10]);for (int i = 0; i &lt; 10; i++)&#123; intArrayPtr[i] = i;&#125;intArrayPtr[10] = 10; //no check// scoped_ptr to vectortypedef boost::scoped_ptr&lt;std::vector&lt;int&gt; &gt; IntArrayPtr;IntArrayPtr intArrayPtr(new std::vector&lt;int&gt;(10));for (int i = 0; i &lt; 10; i++)&#123; (*intArrayPtr)[i] = i;&#125;(*intArrayPtr)[10] = 10; //no checkintArrayPtr-&gt;at(10) = 10; //have check// shared_arraytypedef boost::shared_array&lt;int&gt; IntArrayPtr;IntArrayPtr intArrayPtr(new int[10]);for (int i = 0; i &lt; 10; i++)&#123; intArrayPtr[i] = i;&#125;intArrayPtr[10] = 10; //no check// shared_ptr to vectortypedef boost::shared_ptr&lt;std::vector&lt;int&gt; &gt; IntArrayPtr;IntArrayPtr intArrayPtr(new std::vector&lt;int&gt;(10));for (int i = 0; i &lt; 10; i++)&#123; (*intArrayPtr)[i] = i;&#125;(*intArrayPtr)[10] = 10; //no checkintArrayPtr-&gt;at(10) = 10; //have check scoped_array和shared_array是专门管理用new T[]分配的指针的，它们都重载了[]操作符，没有重载*和-&gt;操作符，使用scoped_array和shared_ptr确保了在删除时相应的使用delete []。实际上scoped_array和shared_array完全可以用vector来代替C数组，见上述示例代码。 原理 用一个自己实现的简单版本来说明一下shared_ptr的原理： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768template&lt;typename T&gt;class MySmartPtr&#123;public: T * operator -&gt;() const &#123; return m_pData; &#125; T &amp; operator *() &#123; return *m_pData; &#125; MySmartPtr() &#123; m_pData = NULL; m_piCnt = NULL; &#125; explicit MySmartPtr(T * pData) &#123; m_pData = pData; m_piCnt = new int(0); InterlockedIncrement((long*)m_piCnt); &#125; MySmartPtr &amp; operator=(const MySmartPtr &amp; other) &#123; if (this != &amp;other &amp;&amp; m_pData != other.m_pData) &#123; destroy(); m_pData = other.m_pData; m_piCnt = other.m_piCnt; InterlockedIncrement((long*)m_piCnt); &#125; return *this; &#125; MySmartPtr(const MySmartPtr &amp; other) &#123; m_pData = NULL; m_piCnt = NULL; *this = other; &#125; ~MySmartPtr() &#123; destroy(); &#125;private: void destroy() &#123; if (m_piCnt &amp;&amp; *m_piCnt &gt; 0) &#123; if (InterlockedDecrement((long*)m_piCnt) == 0) &#123; delete m_pData; m_pData = NULL; delete m_piCnt; m_piCnt = NULL; &#125; &#125; &#125; T *m_pData; int *m_piCnt;&#125;; 智能指针所运用的基本原理包括： （1） 重载*和-&gt;操作符，是其用法和裸指针的使用方法一致； （2） 智能指针对象的析构函数中判断裸指针是否删除，RAII思想的运用； （3） shared_ptr使用一个共享的引用计数决定何时删除裸指针（见示例）。 性能 shared_ptr的性能是使用者比较关心的问题。而这个问题也早已经过广泛的讨论和试验，问题的核心集中在引用计数的实现方式，boost文档中记述的一些开发人员所做的试验，试验中测试了5种实现方式： Counted pointer using a heap allocated reference count, this is referred to as simple counted. Counted pointer using a special purpose allocator for the reference count - special counted. Counted pointer using an intrusive reference count - intrusive. Linked pointer as described above - linked. Cyclic pointer, a counted implementation using a std::deque for allocation with provision for weak pointers and garbage collection of cycles of pointers - cyclic. 从两个方面进行了试验： Two tests were run: the first aimed to obtain timings for two basic individual operations: Initial construction from raw pointer. An amortized copy operation consisting of half an assignment and half a copy construction - designed to reflect average usage. The second attempted to gain more insight into normal usage by timing the fill and sort algorithms for vectors and lists filled with the various smart pointers. 试验环境： on two compilers: MSVC 6.0 service pack 3, using default release optimization mode (/O2 - optimized for speed, no inlining of functions defined outside a class body unless specified as inline). gcc 2.95.2 using full optimization (-O3 -DNDEBUG). Additionally, generated pointer sizes (taking into account struct alignment) were compared, as were generated code sizes for MSVC mainly by manual inspection of generated assembly code - a necessity due to function inlining. All tests were run on a PII-200 running Windows NT version 4.0 第一个试验结果： 单位：纳秒 MSVC initialization copy operation simple counted 3000 +/- 170 104 +/- 31 special counted 1330 +/- 50 85 +/- 9 intrusive 1000 +/- 20 71 +/- 3 linked 970 +/- 60 136 +/- 10 cyclic 1290 +/- 70 112 +/- 12 dumb 1020 +/- 20 10 +/- 4 raw 1038 +/- 30 10 +/- 5 GCC initialization copy operation simple counted 4620 +/- 150 301 +/- 28 special counted 1990 +/- 40 264 +/- 7 intrusive 1590 +/- 70 181 +/- 12 linked 1470 +/- 140 345 +/- 26 cyclic 2180 +/- 100 330 +/- 18 dumb 1590 +/- 70 74 +/- 12 raw 1430 +/- 60 27 +/- 11 第二个试验结果： 单位：秒 GCC vector list fill sort fill sort simple counted 46.54 2.44 47.09 3.22 special counted 14.02 2.83 7.28 3.21 intrusive 12.15 1.91 7.99 3.08 linked 12.46 2.32 8.14 3.27 cyclic 22.60 3.19 1.63 3.18 raw 11.81 0.24 27.51 0.77 MSVC vector list fill sort fill sort simple counted 1.83 2.37 1.86 4.85 special counted 1.04 2.35 1.38 4.58 intrusive 1.04 1.84 1.16 4.29 linked 1.08 2.00 1.21 4.33 cyclic 1.38 2.84 1.47 4.73 raw 0.67 0.28 1.24 1.81 结论： The timing results mainly speak for themselves: clearly an intrusive pointer outperforms all others and a simple heap based counted pointer has poor performance relative to other implementations. The selection of an optimal non-intrusive smart pointer implementation is more application dependent, however. Where small numbers of copies are expected, it is likely that the linked implementation will be favoured. Conversely, for larger numbers of copies a counted pointer with some type of special purpose allocator looks like a win. Other factors to bear in mind are: - Deterministic individual, as opposed to amortized, operation time. This weighs against any implementation depending on an allocator. Multithreaded synchronization. This weighs against an implementation which spreads its information as in the case of linked pointer. 根据以上试验可以看出相比于裸指针，shared_ptr性能地损失主要来自两方面：一方面是第一次初始化时需要额外分配引用计数，另一方面是赋值或拷贝时引用计数的更新等，而前者的损失是主要的。试验结果表明带有侵入式引用计数的实现胜过其他实现，但是大多数应用更依赖于非侵入式引用计数的实现。boost::shared_ptr就是非侵入式实现，其默认实现属于simple counted，通过传入自定义的分配器，也可以实现special counted方式引用计数，除非对时间特别关键的应用，默认的实现完全可以满足要求。另外在初始化时使用工厂方法boost::make_shared(或boost::allocate_shared)可以获得和侵入式接近的性能，因为boost::make_shared使用了一个placement new来分配T，这样相当于节省了分配引用计数的时间。关于是否可以将智能指针作为函数参数传递，上述试验中的simple counted实现正是用了一个默认的boost::shared_ptr，其拷贝的时间确实比裸指针多很多（在PII-200机器上是104纳秒，裸指针是10纳秒），但是毕竟也是纳秒级别，实际使用时的性能损失应该基本觉察不到。boost::shared_ptr的默认实现是采用lock-free的整数原子操作进行的引用计数增减，试验并未评估在复杂的多线程或异步环境中对系统造成的性能损失。 boost内存池 内存池用于管理大量小对象，避免频繁在堆上分配。boost内存池包含如下内容： pool object_pool singleton_pool pool_allocator 使用示例 pool 1234567891011121314151617boost::pool&lt;&gt; FixSizeMemPool(sizeof(int), 32);int *pInt = (int *)FixSizeMemPool.malloc();FixSizeMemPool.free(pInt);pInt = (int *)FixSizeMemPool.malloc();FixSizeMemPool.ordered_free(pInt);int *pIntArray = (int *)FixSizeMemPool.ordered_malloc(10);FixSizeMemPool.ordered_free(pIntArray, 10);pIntArray = (int *)FixSizeMemPool.ordered_malloc(10);FixSizeMemPool.free(pIntArray, 10); boost::pool是用来快速分配固定大小内存快的内存池，构造函数的第一个参数是希望从内存池中每次获取的区块(chunk)的大小，第二个参数表示每次空间不够后再次分配的区块个数（next_chunk_num），为默认参数，默认值为 32。第一次调用malloc或ordered_malloc时进行第一次分配，分配能容纳next_chunk_num个区块的内存快(block)，然后将next_chunk_num乘以2，也就是说下一次分配的内存快能容纳的区块个数翻倍，依次类推。malloc是从内存池中分配一个区块，如果失败返回0；ordered_malloc是从内存池中分配连续的区块，如果失败也是返回0；分配的区块不再使用后通过free返还给内存池；特别的ordered_free保证对返还后的空闲区块排序，以保证之后使用ordered_malloc分配连续区块的机会更大，所以ordered_free的时间复杂度不是O(1)，如果你经常会使用ordered_malloc，最好在释放时使用ordered_free。FixSizeMemPool销毁时保证所有分配的内存得到释放，即使没有调用free或ordered_free，也因此boost::pool不是线程安全的，它不是设计用来多个模块共享的。 object_pool 12345678910111213struct X&#123;public: X(int i) &#123; &#125;&#125;;boost::object_pool&lt;X&gt; ObjectPool(32);X *pX = ObjectPool.construct(1);ObjectPool.destroy(pX); boost::object_pool是从boost::pool继承而来，构造函数只有一个参数即next_chunk_num，和boost::pool的主要区别是boost::object_pool在获取到区块后，使用了一次placement new对区块进行了构造，之后返回了T *而不是void *，这是通过调用construct进行的，construct默认支持3个参数，如果想传入更多参数，需要修改boost/pool/detail/pool_construct_simple.inc文件。相应的调用destroy，会先进行析购，然后回收内存。当然也可以调用malloc/free（没有ordered_malloc/ordered_free），但不推荐，因为ObjectPool在销毁时会自动调用没有free的区块的析购函数，如果在malloc后，用户自己没有对区块进行构造，而又没有调用free，那么最后在其上调用一次析购函数可能产生错误。 singleton_pool 123456789101112131415struct MyPoolTag&#123;&#125;;typedef boost::singleton_pool&lt;MyPoolTag, sizeof(int)&gt; MyPool;int *pInt = (int *)MyPool::malloc();MyPool::free(pInt);int *pIntArray = (int *)MyPool::ordered_malloc(10);MyPool::ordered_free(pIntArray, 10);struct OtherPoolTag&#123;&#125;;typedef boost::singleton_pool&lt;OtherPoolTag, 10, boost::default_user_allocator_new_delete, boost::mutex, 32&gt; OtherPool; boost::singleton_pool是被设计用来在多个模块间共享的，所以是线程安全的。使用singleton模式实现，并且其singleton静态对象也是线程安全的。使用singleton_pool时不用定义对象，malloc/ordered_malloc/free/ordered_free都是静态的，调用方法和boost::pool相同。OtherPool的定义展示了boost::singleton_pool的全貌，10表示请求区块大小；boost::default_user_allocator_new_delete为分配器，默认值也是它，用户可定义其他的分配器；boost::mutex是为了保证线程安全使用的锁，默认为details::pool::default_mutex；32为next_chunk_num。 pool_allocator 12345678910std::vector&lt;int, boost::pool_allocator&lt;int&gt; &gt; vecInt;for (int i = 0; i &lt; 10000; i++)&#123; vecInt.push_back(i);&#125;boost::singleton_pool&lt;boost::pool_allocator_tag, sizeof(int)&gt;::release_memory();std::vector&lt;int, boost::pool_allocator&lt;int, boost::default_user_allocator_new_delete, boost::mutex, 32&gt; &gt; otherVecInt; boost::pool_allocator提供了符合标准的分配器，可用于STL容器。boost::pool_allocator内部实际是使用singleton_pool进行的内存分配，所以如果想手工释放内存可以像上例中使用boost::singleton_pool&lt;boost::pool_alocator_tag, sizeof(int)&gt;::release_memory()。 boost智能指针和内存池的结合使用 12345678910111213141516171819202122232425262728template &lt;typename T&gt;class SmartPool : public boost::noncopyable&#123;public: typedef boost::shared_ptr&lt;T&gt; SmartObjectPtr; SmartObjectPtr ConstructSmartObject() &#123; boost::mutex::scoped_lock lock(m_mtx); return SmartObjectPtr(m_objpool.construct(), boost::bind(&amp;SmartPool::DestroySmartObject, this, _1), boost::pool_allocator&lt;boost::detail::sp_counted_base&gt;()); &#125;private: void DestroySmartObject(T *p) &#123; m_objpool.destroy(p); &#125; boost::object_pool&lt;T&gt; m_objpool; boost::mutex m_mtx;&#125;;struct X&#123;&#125;;SmartPool&lt;X&gt; XSmartPool;SmartPool&lt;X&gt;::SmartObjectPtr objPtr = XSmartPool.ConstructSmartObject(); 上例展示了将boost智能指针和内存池结合使用的方法。上述方法的限制是要求X有默认构造函数，另外必须XSmartPool在所有智能指针之后释放，当然这两个限制是有方法解决的，大家可以自己想一想。 原理 对于固定大小内存池的实现原理并不复杂，即预先分配一些内存块，每个内存快被划分成相同大小的区块，这些区块被链接在一个空闲链条中，当要分配内存时，从空闲区块头取出一个，如果没有空闲的，则分配新的内存快，当不再使用区块时，将其加入到空闲链表中即可，所以保证了分配和释放的时间复杂度是常量的（除第一次分配或之后追加分配时），肯定比直接使用new分配要快。boost内存池的增长方式是，第一次调用malloc*时分配一个内存快，可容纳若干（可配）个区块，以后每次不够时，再分配一个内存快，大小是前一次的两倍。boost内存池还提供了分配连续区块的接口，为了保证每次分配连续区块成功的机会更大，在释放时提供了排序的释放方法，当然其时间复杂度就不是常量了。boost内存池还保证了内存对齐，可广泛适用于不同平台。 内存布局 1234567891011121314151617 struct Pool&#123; void *firstBlock; void *firstFreeChunk;&#125;;struct Block&#123; union Chunk &#123; void *nextChunk; char chunk[chunk_size]; &#125; Chunks[chunk_num]; void *nextBlock; size_type nextBlockSize;&#125;; 每个内存快（Block）的布局如上所示（上述代码仅是为了说明问题的伪代码，真实源代码并不是这样），首先是若干个连续的区块，然后是下一个内存快的指针，最后是下一个内存快的大小。这里比较有意思的一个技巧是复用了区块中的内存存储了下一个区块的指针，将区块链接起来。 内存对齐 boost内存对齐使用了最小公倍数方法（具体推导过程见boost文档），保证了在各种复杂环境下内存的对齐。这一点也是我们自己实现内存池容易忽视的一个问题。 boost::flyweight 在《设计模式》一书中描述了flyweight模式，boost::flyweight得名于此。 使用示例 1234567891011121314151617181920212223242526struct user_entry&#123; std::string first_name; std::string last_name; int age;&#125;;struct user_entry&#123; boost::flyweight&lt;std::string&gt; first_name; boost::flyweight&lt;std::string&gt; last_name; int age;&#125;;user_entry ue1;ue1.first_name = "zhang";ue1.last_name = "xxx";user_entry ue2;ue2.first_name = "zhang";ue2.last_name = "yyy";user_entry ue3;ue3.first_name = "zhang";ue3.last_name = "yyy"; boost::flyweight的使用非常简单，大多数时候只需要修改一下结构体的定义，就能带来内存的节省，当冗余度越大时，内存节省越明显，例如上例，使用boost::flyweight后，内存中只有一份“zhang”，当有成千上万个这样的对象时，姓相同的比例非常大，这样就能节省很多内存。]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>boost</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode中对python进行修改符号名重构时总是失败]]></title>
    <url>%2F2020%2F01%2F31%2F023-python-refactor-rename-failur-in-vscode%2F</url>
    <content type="text"><![CDATA[最近在vscode中重构python代码，修改符号名时总是失败，报如下错误： Refactor failed. expected string or buffer [(‘refactor.py’, 294, ‘watch’, ‘self._process_request(self._input.readline())’), (‘refactor.py’, 275, ‘_process_request’, “request[‘start’]), request[‘name’], int(request[‘indent_size’]))”), (‘refactor.py’, 206, ‘_rename’, ‘refactor.refactor()’), (‘refactor.py’, 117, ‘refactor’, ‘self.onRefactor()’), (‘refactor.py’, 139, ‘onRefactor’, ‘changes = renamed.get_changes(self._newName, task_handle=self.handle)’), (‘D:\Programs\Anaconda2\lib\site-packages\rope\refactor\rename.py’, 97, ‘get_changes’, 'new_content = rename_in_module(finder, new_name, resource=file)’), … 从报错中可以看到重命名用到了rope，推测可能是rope有bug，经过一番尝试，有两种解决方法。 方法1 vscode：1.41.1 python extensions：2020.1.58038 rope：0.10.5 默认安装的rope版本为0.10.5，尝试升级rope： $ pip install -U rope Collecting rope Using cached https://files.pythonhosted.org/packages/fa/a0/98c936091acad7fe96af3a945a5e4a1ddab9f4a2ba4e6eb56fe469c9457c/rope-0.16.0-py2-none-any.whl Installing collected packages: rope Found existing installation: rope 0.10.5 Uninstalling rope-0.10.5: Successfully uninstalled rope-0.10.5 Successfully installed rope-0.16.0 最新版本为0.16.0，再次尝试修改符号名，问题消失，说明rope确实有bug，已经解决。 方法2 python extensions有两种语言服务器：jedi language server和microsoft python language server，后者是2018.10开始新加入，未来将取代前者，目前默认为前者，前者正是使用rope进行的改名。 尝试切换到后者，去勾选jedi enabled。 重启vscode，会自动下载microsoft python language server，等下载完成后，再次尝试修改符号名，问题消失，在新的language server没有此问题。 新的language server发展时间不长，可能没有老的稳定，大家慎重使用。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>vscode</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conan一年使用总结]]></title>
    <url>%2F2020%2F01%2F11%2F022-one-year-usage-summary-of-conan%2F</url>
    <content type="text"><![CDATA[关注C++的包管理有一段时间了，一直非常羡慕python、java的同学，有非常完善的包管理工具，使用第三方库非常方便。2016年底开始接触到conan，后来又发现vcpkg，两者都是github开源项目，前者后来得到jfrog的支持，其artifactory也增加了对conan包的支持，而后者是大牌microsoft维护的。 conan和vcpkg的出现终于让人眼前一亮，经过1年的发展，到2018年它们相对成熟起来，而我们的产品也急剧膨胀，迫切需要包管理的机制，因此我下定决心将包管理引入到产品中。 到底选择conan还是vcpkg，有两点使我们必须选择conan，一是跨平台，不仅要支持windows，还要支持linux，二是要能自建包服务器。当时的vcpkg还只能支持windows，当然后来的版本也支持linux了，而自建包服务器，vcpkg一直都不可以。后来证明这个选择是正确的。 项目背景 先说下我们的产品，这是一个CS结构通信设备管理系统，2010年开始研发，服务端C++实现，采用了分布式计算框架ice，是一个分布式系统。架构上分为平台层和产品层，平台层实现了一些公共框架和服务。截止到2018年中，代码规模产品层达到188W行，86个工程，平台层达到300W行，245个工程。 最初产品层和平台层的代码是放在一个SVN库里面的，cmake为一个VC解决方案，整体进行编译打包。这种方式的弊端显而易见，平台和产品的边界很容易打破，模块间的依赖关系无法控制，甚至会出现平台对产品的依赖。为了解决这个问题，从2015年开始就不断对开发架构进行调整，经历了两个阶段： （1）SVN分离，拆分成产品和平台两个SVN，产品层通过SVN外链链接到平台层SVN，从产品层来看还是一个完整的SVN。将平台层对外头文件和库拆分到单独目录，通过cmake模块控制平台层不能include产品层头文件且产品层只能include平台层对外头文件。cmake为产品层和平台层两个VC解决方案，分别单独编译； （2）解除产品层对平台外链，平台层编译后将对外头文件和库打包为SDK压缩包，存放在文件服务器，产品层开发人员下载平台SDK压缩包后解压到相应目录，然后进行产品层开发。 此时基本控制住了产品和平台的代码边界，但是仍有诸多不便和问题： （1）产品层开发人员需要手动下载平台SDK压缩包，使用不方便，而且版本对应关系容易出错； （2）平台层仍然是一个整体，新版本开发需要整体分支，无法做到细粒度的复用，而且总是整体编译打包，非常耗时； （3）第三方库是手工编译好之后将头文件和库放到SVN上的，没有源代码，如需升级版本或修改源码或调试就非常麻烦。 要解决上述问题，只能引入包管理机制。 成功的关键 在一个大型组织中引入一项新技术还真不是一件易事，特别是对于一个正在成长中的技术。但是我知道这项技术对于我们非常重要，因此我们迫切的只花了很短时间将平台层的300W行代码全部包化，从2018年8月开始准备，2018年11月我带领团队花费3周时间将平台层完全包化。 引入包管理主要是解决平台的使用问题，所以要包化的是平台层，产品层是包的消费者。那么我们是怎么将平台层几百万行代码快速包化的呢，成功的关键在于如下几点。 种子选手 包化的过程不仅需要懂得如何使用，还包括如何拆分和设计包，包的粒度应该多大。起初，为了快速引入包管理机制，在后者上我们考虑的不够，吃了一些亏。 在动手之前要对conan做到了然于心，最好的办法是阅读conan的文档，我起码研究了不下三遍，吃透了conan能够提供的方方面面，所以在后面我们需要一种方法将所有包一起本地编译时，我马上想到工作区的方法并成功实现了。 另外还需要一定python经验，conan自身还是存在一些问题，有时需要修改其源代码。也可能需要修改源代码满足一些特化需求。好在我非常喜欢python，有多年的使用经验了，这一点在后面起到重要作用。 我自己首先做到成竹在胸，然后再培育一批种子选手，对它们进行培训。这些准备工作是后面快速完成包化的关键。 制定规则、隐藏细节 技术本身往往具有很多灵活性，这是为了适应各种不同场景需要。但是要落地到具体应用，最好加以封装，避免暴露过多细节。如果引入conan后，随之带来的变化会造成困扰，那么再好的东西也会招致反感。不要以为就是敲一个命令行的事情，开发人员都会嫌麻烦，这是在我的组织里面普遍存在的。所以我们尽量做到保持以往的开发习惯，屏蔽掉一些细节，在这方面做了大量工作。下面梳理一下引入包管理后带来的一些变化，以及我们是如何做的。 （1）开发环境 通过一键脚本初始化conan开发环境，包括registry设置，profile设置； 构建的工具也包化，例如cmake，总之一切都是包。 （2）开发过程 包名的名命规则：conan中每个包的名字遵循name/version@user/channel格式，我们将user和channel赋予了特定含义，user用于表示子系统，如ext表示第三方库，core表示自研的公共库等等，channel用于区分开发版本，主干开发版为dev，某分支开发版为xxx_dev，xxx为分支名。 包的拆分原则：包的拆分要遵循两个基本原则：最小依赖原则和最大复用原则。最大复用就是，对于复用的要尽量细，对于不复用的，可以粗放一些。最小依赖就是一个包可以精确的依赖必须依赖的。conan并不限制一个包里面包含多少库，但是包是用来复用的，粒度越小复用度也越大，最理想的情况是一个库一个包，除了少数例外：有些库是一系列库的组合，它们的源代码本生没有模块化拆分，例如boost，ice，不好做成多个包；有一些不是用来复用的，例如平台层的公共服务，为了最终产品打包方便，也做成了包。起初为了尽快引入包管理，按照原来的目录结构，做成了大包，也就是一个包包含多个库，这带来了一系列问题，首先造成了依赖关系扩大化，也就是依赖了不该依赖的包，导致编译效率下降，因为头文件搜索路径和导入库数量显著增加了；其次，会遇到一些循环依赖问题，这时必须通过进一步拆分解除循环。 产品层如何消费包：对于包的消费方，需要一种方便的方法消费包，我定义了一个特殊的包，集成包，将平台层所有包require到集成包，产品层只需要依赖集成包。 包的存储和上传：包的存储采用了商业的artifactory，我们的maven以及npm已经使用了artifactory，前面说了他也支持了conan。包的上传使用开发者自己的账号，这样可以追溯谁修改了包，通过artifactory集成AD可以容易做到这点。conan本身也提供一个简单的服务端，可满足基本使用。 版本管理：包化只是一个开始，包化之后变成了一个个可以独立编译的包，每个包可以独立版本演进，这给版本管理带来了更大的挑战。版本管理需要重点考虑几点：不同开发分支的包不会互相影响；什么情况升版本号，升哪些包的版本号；升版本号的规则是怎么样的；如何跟踪版本的变化。我们采取的措施是：通过channel隔离每个分支，彼此互不干扰，每个分支根目录下建立channel.txt文件填入channel名；如果进入集测阶段或释放的产品版本依赖的平台包有修改，就需要升级版本号，版本号升级规则按照SemVer规范，如果修改是二进制向前兼容的，那么依赖它的其他包不需要升版本号，也不需要重编。如果修改不是二进制向前兼容的，那么依赖它的其他包也需要升级版本号。任何包有版本升级后，集成包也需要升级版本号，我们会维护一个版本地图，包含从产品版本到集成包的关系以及集成包包含的所有包版本的关系。 整体编译：包的好处是每个包都可以独立开发，但是在新版本开发阶段，往往很多包会一起大量修改（我们将很多并不是为了开发时复用的服务也包化了），为了提高开发效率，最好能将所有包一起本地整体编译，这时conan的工作区就派上用场了，我们使用的1.6.1版本的工作区特性还不完善，我们对源代码做了一些修改。同时产品层编译也做了一些适配，不使用conan缓存，直接使用工作区中的头文件和库。 （3）运行发布 持续集成：主要的变化是监控的粒度变成了一个个包，但是包之间是有关联的。考虑过几种解决方案，方案一，一个包一个job，job触发受影响包的job；方案二，参数化的job，通过pipeline代码并发构建所有受影响包，代码scm的hook触发job；方案三，通过conan工作区机制，整体构建所有包，监控粒度实际变成了所有包。方案一要建立很多job，不可取。方案二是最合理的，但是如果一些底层包变化，整体编译时间还是很长，另外需要scm回调ci支持。方案三的问题是即使没有变化的包也会编译上传。目前，我们采用的方案三。 方案一、方案二中判断受影响包的方法都是通过conan info -bo来获取的，这仅仅是通过依赖关系计算的，实际上，有时一个包修改了，并不一定需要重新编译依赖它的包，但是目前没有办法识别这一点。 运行：分安装包运行和开发环境运行，安装包是通过conan import将包里面的二进制程序拷贝到运行目录，然后打成安装包的。开发环境运行，如果每次都拷贝到运行目录，一来速度慢，二来浪费大量磁盘空间，conan提供了一个virtualrunenv的generator，但是因为我们的包太多了，导致命令行长度过长，无法使用这个特性，最终我们想了一个办法，在运行目录中建立了包中二进制程序的软链接，这是在开发环境启动脚本中自动完成的。 压缩源码：每个安装包，同时需要打包对应的源码，便于以后调试使用。conan包对应的源代码也存储在包服务器上，打包的时候可以从其下载源代码。 离线使用conan：有些环境无法访问包服务器，需要能够离线使用conan，那么就需要将conan缓存目录也打包。 及时响应调整 有很多情况是一开始没有想到的，完成包化之后，在使用过程中我们不断进行调整。那段时间我天天盯着微信群，有任何问题都会第一时间处理掉，消除大家的疑问。 例如一开始我们设想的产品层应该都是基于二进制的包去访问平台层，但是实际情况是此时平台层也在大量的开发，经常出现平台层没有及时上传包而导致产品层编译不过。还有平台层改动一个包之后，常常连锁的需要重编其他包上传，开发效率相比之前显著下降，大家迫切需要像以前一样，cmake为一个VC解决方案，整体进行编译。幸好conan的工作区特性正是应对这种场景，但是我们使用的1.6.1版本还存在一些问题，我对源代码进行了一些修改，另外还做了一些优化。 这种例子还非常多，幸好都一一及时化解。到如今已经稳定运行一年多。 常见问题总结 以下总结了在使用conan过程中遇到的一些具体问题，希望对遇到相似问题的同学有所帮助。我们使用的conan版本是1.6.1，有些问题在后续版本已经解决，如果你使用的更高版本，可能不会遇到。 开发环境统一问题 初期刚引入conan时，经常报找不到包和registry连不上问题。找不到包通常是开发环境的conan全局编译参数CONAN_USER_HOME/profiles/default设置的不对，而连不上registry通常是因为，默认的conan-center排在第一个位置，内网环境连不上。 解决措施：在构建脚本中通过-s和-o参数将编译参数固定，不依赖开发者环境上的全局配置。构建脚本中通过-r参数固定registry，不依次查询所有registry。 整体构建问题 在开发过程中，多个包会一起开发，每个包单独构建，时间太慢，而且conan create构建包是在conan缓存目录中，每次你得切换到缓存目录，在build目录找到VC工程，每次conan create默认又会删除上次的构建，所以非常麻烦。解决这个问题使用了conan工作区的特性，工作区有一个限制是只能用于cmake集成，工作区是就地构建的，不会拷贝到conan缓存构建，并且可以将工作区中的所有包cmake为一个工程。1.6.1版本的工作区还不完善，我们解决了如下问题： （1）自动生成conanws.yml文件 要使用工作区，需要在工作区根目录编写coannws.yml文件，其中定义了每个包的目录、头文件目录和导出库目录以及谁是根包。每当增加新的包或者头文件目录有变化的时候，就需要同步修改这个文件，但是这些信息实际上在conanfile.py文件中都有，所以我做了一个python脚本自动生成conanws.yml文件。 原理很简单，就是解析conanfile.py文件，提取package_info方法中的头文件目录信息，而根包就是集成包。这里要注意的一点是，conanws.yml中包的顺序是有意义的，必须按照依赖的顺序，这可以通过conan info -bo ALL获取，python脚本中用OrderedDict存储，然后dump到yml文件。 （2）依赖包的导出库没有加入到当前工程 工作区里面cmake的时候，因为还没有编译过，还没有生成lib文件或so文件，pckage_info中通过collect_libs()是收集不到库文件的，另外conan的源代码也需要修改。 解决措施：conanfile.py文件的package_info显式设置self.cpp_info.libs，不要使用tools.collect_libs()，修改conan源代码，在定义target的时候，如果lib目录不包含.conan（也就是不是从缓存中来时）直接拼出库路径，不使用find_library。 client/generators/cmake_common.py： （3）偶尔依赖包的头文件目录和导入库目录没有加入到当前工程 原因和上面类似，cmake阶段目录还没生成，空目录被过滤掉了。 1.13.0版本已经修复，release说明如下，工作区特性已经彻底重构： Feature: Re-implement Workspaces based on Editable packages. (#4481). Docs: 📃 关键代码build_info.py： 我的临时解决方案： client/generators/__init__.py （4）生成VC解决方案时，工程间依赖没有建立 产生这个问题的原因要先说一下conan是如何设置一个包的导入库的，generator为cmake时，conan install时会生成conanbuildinfo.cmake文件，其中定义了conan_target_link_libraries函数，假设A包依赖B包，那么A的CMakeLists.txt中包含conanbuildinfo.cmake，然后调用conan_target_link_libraries(A)，或者如果启用了TARGETES，就是target_link_libraries(A CONAN_PKG::B)，这样就自动将B包的库导入到A包库上，conan_target_link_libraries中实际上就是调用的target_link_libraries，传输的参数是库名。 target_link_libraries的参数可以是target名，也可以是库名，当为target名时，会自动加入工程的依赖，但是当为库名时，就不会自动加入也做不到自动。但是当库名和target名一样时，依赖是可以加上的，大多数情况其实是一样的。可是偏巧我们的库名和target名就不一样，所以才有此问题。 解决措施：修改conan源码，在conan_target_link_libraries中显式调用add_dependencies加入依赖，我们的库名和target名是有对应关系的，从库名得到target名，然后调用add_dependencies。 （5）会出现副作用，包不能单独编译了 使用整体构建后，会出现有人通过…跳出包的根目录，访问其他包的头文件，这时是可以成功编译了，从而掩盖了问题。但是当单独编译包时问题暴露了。 解决这个问题的方法是将add_library等方法封装了一个版本，所有包统一调用封装的，然后在封装版本中检查target的include_directories。这个办法只能检查include_directories属性，头文件里面#include的无法检查。 （6）工作区的其他限制 build方法不能有编译逻辑，包括向CMake传递变量，只能直接调用CMake，所有编译逻辑必须在CMakeLists.txt中； 每个包导出的目录结构（二进制包中的结构）和源码本身的目录结构必须一致。所谓包导出的目录结构，就是package方法中拷贝的目的目录。如果不一致，一个补救方法是在package_info方法中的self.cpp_info.includedirs.append加入和导出目录不一致的本地目录。 例如：某包，导出目录有include/gsclient，但本地目录为gsclient/include，要么调整本地目录，也变成include/gsclient，要么加入self.cpp_info.includedirs.append(“gsclient/include”)。 不能通过self.deps_cpp_info[xxx].rootpath访问lib、bin外的其他目录，因为在缓存中二进制包的布局是所有内容都在rootpath下，但是工作区中，只有二进制内容在rootpath下，其他在源码目录下。要想在缓存和工作区都能工作的方法，只能通过self.deps_cpp_info[xxx].include_paths，然后…跳到上一级访问。 编译时间变慢问题 前面说明过包的拆分原则，对于复用的包，最好一个库一个包，因为直接的一个问题是会导致编译时间变长，原因是头文件搜索路径变多，导入库变多。例如假设A包有2个库a_1、a_2，a_1需要依赖B包，a_2需要依赖C包，B和C各有10个头文件路径，那么a_1和a_2的搜索路径就都是20个，不是10个。因为默认情况下conan的头文件路径和库都是以包为粒度的。 一个缓解的办法是启用TARGETES，原来是调用conan_basic_setup()，现在变成调用conan_basic_setup(TARGETES)，然后原来a_1、a_2是调用conan_target_link_libraries(a_1)、conan_target_link_libraries(a_2)，现在变成了target_link_libraries(a_1 CONAN_PKG::B)、target_link_libraries(a_2 CONAN_PKG::C)。这样a_1和a_2的搜索路径就都是10个。 另外尽量避免因为依赖传递而导致依赖不需要的库，可使用private依赖。当只有cpp依赖某个包，导出头文件不需要依赖时，可以使用private依赖，这样这个依赖的包就不会扩散出去。 以上只是缓解的办法，最好的办法还是一个库一个包，做到依赖最小化，精确的依赖。 磁盘空间问题 我们的开发环境采用的云桌面，每个人只有40G磁盘空间，所以磁盘空间成了一个重要问题。按照常规的做法，消费包的一方，只能从conan缓存中获取。这样采用工作区编译后，还需要export-pkg到缓存，然后运行的时候还需要import到运行目录，这样就会同样的内容有三份。我们的程序用的debug版，带pdb文件，更加重磁盘负担，上述40G空间远远不够。 为了解决上述问题，首先消费包的一方能否直接从工作区访问头文件和导出库，我想了一个办法，直接将工作区中集成包的conanbuildinfo.cmake、conanbuildinfo.txt、conaninfo.txt文件拷贝到消费包，不运行conan install，打通了第一关。其次运行时能否不用import，前面提到了virtualrunenv用不了，最后使用的办法是解析conanbuildinfo.txt文件，将依赖的二进制文件建立软链接到运行目录下，windows下是mklink命令。这样就确保了包的二进制文件只存在一份。 兼容性问题 package_id conan是用package_id唯一标识一个二进制包，同一个包的不同二进制包的package_id是不一样的，它们也是不兼容的。package_id是控制兼容性的一种手段，如果不兼容了，那么我们就要让package_id产生新的。如果package_id没有变化，那么之前upload到remote的二进制包还可以使用，也就意味着承认是兼容的。 package_id有一个默认的生成规则，根据如下内容的变化： （1）自身settings或options变化； （2）require包的增减，也就是name变化； （3）require包的版本变化，这时默认是按照SemVer规范，1.0.0之前总是影响，1.0.0之后只有主版本号变化才影响。 上述（2）（3）不仅影响自己，下游的包都会受影响，所谓下游，例如A依赖B，A就是下游，那么自身和下游全部都需要重新编译上传，了解了上述规则之后，会帮助我们解决两类问题： （1）修改conanfile.py文件后，导致找不到某些二进制包，就是因为package_id发生了变化，需要重新编译上传； （2）如果所有包的版本严格按照SemVer规范，那么按照默认的package_id生成规则是可以良好运行的，但是也会有一些特殊情况，conan提供了方法修改上述默认规则，例如可以让require包的minor或patch版本号也参与计算。 升级包版本 升级某个包依赖的包的版本之后，自身的版本号是否需要升级，这也是困扰我们的问题。 用一个例子来说明，假设A/1.0.0，A/1.1.0都依赖C/1.0.0，C/1.0.0依赖D/1.0.0。A/1.0.0是之前释放的版本，A/1.1.0是正在开发的版本，C/1.0.0，D/1.0.0是公共库。在开发A/1.1.0的过程中升级D为1.1.0，C的代码没有改动。那么此时C的版本是否需要升级？ 方案 问题 解决措施 是否推荐 方案一 C升级版本号为1.1.0 代码没有任何改动却升级版本号 否 方案二 C不升级版本号但require改为D/1.1.0 导致A/1.0.0再次编译时使用了新版本D，但是A/1.0.0是已经释放的 否 方案三 C不升级版本号且不修改require，在A/1.1.0中override为D/1.1.0 从C的recipe无法create出依赖新版D的包(这个例子中D从1.0.0到1.1.0是兼容的，不需要重编C，如果D变成2.0.0就需要重编C) 下游包使用–build=missing 是 总之，只有修改了代码才升级版本号。 override 前面提到上游包或者消费包中可以override下游包的版本。 require还可以增加override参数，写法是self.requires(“X/1.0.0@user/channel”, override=True)或者requires = ((“X/1.0.0@user/channel”, “override”))。 那么带上override参数是什么作用呢，其实是针对条件化require的，如果存在override的包，就override版本号，如果不存在，就不会引入依赖。所以这个override名字取得不太好，其实不带override参数也是override的，只是这时即使上游没有这个包，也会引入依赖。 override一般是新版本，那么可以回退版本吗？答案是编译虽然可以通过，但是可能出现兼容性问题，最好只用于升级版本。 还有会出现同时存在两个版本包吗？答案是不会，conan会报错，这时必须在上游包override版本号。 ERROR: Conflict in CC/1.2.0@zhongpan/testing Requirement DD/1.1.0@zhongpan/testing conflicts with already defined DD/1.0.0@zhongpan/testing Keeping DD/1.0.0@zhongpan/testing To change it, override it in your base requirements 追踪包上传日志 有时会出现某个人提交一个包导致产品编译不过，如何追踪谁提交过包，通过artifactory提供的aql接口，可以很容易查询到，下面的例子查询了非unm_ci用户提交的，近几天满足条件的上传记录，注意这些记录是以文件为粒度的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#!/usr/bin/python# -*- coding: UTF-8 -*-import jsonimport requestsimport argparsefrom collections import OrderedDicturl = "http://10.170.3.50:8040/artifactory/api/search/aql"headers = &#123;"Content-Type":"text/plain","X-JFrog-Art-Api":"AKCp5dLCa9UmtgDdQGJzm87onMPyKHxpGJ7YDNTbQomQ8aEzEW8fXcFWMzbSJwufckLkSe6oU"&#125;data = '''items.find( &#123; "$and":[ &#123;"$or":[ &#123;"@conan.package.user":&#123;"$eq":"ext"&#125;&#125;, &#123;"@conan.package.user":&#123;"$eq":"core"&#125;&#125;, &#123;"@conan.package.channel":&#123;"$eq":"stable"&#125;&#125;, &#123;"@conan.package.channel":&#123;"$match":"%s"&#125;&#125; ]&#125;, &#123;"$rf":[ &#123;"$or":[ &#123;"property.key":&#123;"$eq":"conan.package.name"&#125;&#125;, &#123;"property.key":&#123;"$eq":"conan.package.version"&#125;&#125;, &#123;"property.key":&#123;"$eq":"conan.package.user"&#125;&#125;, &#123;"property.key":&#123;"$eq":"conan.package.channel"&#125;&#125;, &#123;"property.key":&#123;"$eq":"conan.settings.os"&#125;&#125;, &#123;"property.key":&#123;"$eq":"conan.settings.arch"&#125;&#125;, &#123;"property.key":&#123;"$eq":"conan.settings.build_type"&#125;&#125;, &#123;"property.key":&#123;"$eq":"conan.settings.compiler"&#125;&#125; ]&#125; ]&#125;, &#123;"modified":&#123;"$last":"%dd"&#125;&#125;, &#123;"modified_by":&#123;"$ne":"unm_ci"&#125;&#125; ] &#125;).include("modified_by","modified","name","type","path","archive").sort(&#123;"$desc" : ["modified"]&#125;)''' if __name__ == "__main__": parser = argparse.ArgumentParser() parser.add_argument("--channel", "-ch", help="channel in package recipe", type=str, default="*") parser.add_argument("--days", "-d", help="modified last days", type=int, default="1") args = parser.parse_args() try: r = requests.post(url, headers=headers, data=data % (args.channel, args.days)) if r.status_code == 200: results = OrderedDict() items = r.json()['results'] for item in items: paths = item['path'].split('/') if len(paths) &lt; 4: continue recipe = "%s/%s@%s/%s" % (paths[1], paths[2], paths[0], paths[3]) results[item['modified']] = (recipe, item['modified_by']) print u"最近%d天上传包列表：" % args.days for key in results.keys(): print "%s, %s, %s" % (results[key][0],results[key][1],key) except Exception,e: print e 集成IncrediBuild，加速编译速度 client/build/cmake.py： 全局编译链接参数 通过增加cmake模块，将add_library等封装，所有包使用封装版本，在封装版本中增加全局编译链接参数。 其他一些Bug （1）包压缩文件中没有包含文件修改时间，导致每次install下来时间改变，导致重编 client/remote_manager.py： （2）reading .count-file文件编码引起的问题 新版本已经修改，见github。 我的修改方法，util/locks.py： （3）环境变量处理异常 在少数环境遇到过此问题，不是所有环境都有，做了如下修改。 client/tools/env.py： 包的设计 为了快速引入包管理，我们将平台层整体进行了包化，其中实际上包含一些并不是为了复用的模块，这是对现状的妥协，包化的过程并没有对原有模块结构进行调整，只是将它们简单聚合为包。 回过头再来想，如果从零开始设计，最关键的还是划分好模块，然后才考虑哪些用conan管理。我们的系统是一个分布式系统，共有70多个服务，代码耦合严重： （1）分成了平台层和产品层2个SVN，代码揉在一起，服务不能单独编译，不能独立维护； （2）由于上面代码揉在一起，模块间的依赖关系错综复杂，完全失控； 这样一锅粥的代码可想而知维护是很痛苦的，最大的问题是分支多、合并难、测试工作量大。当下流行的微服务的理念正是拯救我们的良药，何况我们本来就是一个分布式系统。我设想的需要做如下拆分： （1）按照服务将代码拆分到不同的Group，每个Group有若干相关的project，每个project可以独立编译； （2）每个服务能够独立部署、独立运行； （3）服务之间不能有编译时的依赖，只依赖接口契约或API； 理想的情况服务之间只通过接口锲约访问，不依赖服务内部业务对象定义，由接口锲约自动生成代码，编译到客户端服务里面。对于一些复杂的场景，例如需要增加一些缓存机制，优化客户端的访问效率，可以封装为本地API，供其他服务调用。 （4）每个服务访问不同的数据库； （5）服务独享的库放到服务里面，只有多个服务共享的库放到独立的公共库中；极端情况下，公共库都是业务无关的，凡业务相关的通过服务提供RPC接口； （6）不同服务运行时可以使用不同版本的公共库。 按照这样的模块划分之后，并不是所有project都需要使用conan管理，只有复用的project需要，也就接口契约、API、公共库这些，服务就作为包消费者就可以了。 最后 以上就是我使用conan一年来的一些总结和思考，希望对C++的同学有所帮助。conan是一个快速成长的开源项目，迭代非常快，我个人觉得很可能成为C++包管理的事实标准，非常看好它。另外C++本身在今年将迎来一个重量级版本C++20，其中模块、协程相信是大家期待已久的特性。随着C++自身的不断革新和周边工具的充实，我相信这些会”Make CPP great again!“。]]></content>
      <categories>
        <category>包管理</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>conan</tag>
        <tag>包管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用traefik反向代理k8s dashboard]]></title>
    <url>%2F2019%2F11%2F28%2F021-ingress-k8s-dashboard-with-traefik%2F</url>
    <content type="text"><![CDATA[生产环境下，k8s集群对外暴露服务主要有LoadBalancer和Ingress两种方式： LoadBalancer：需要云厂商支持，使用k8s service的负载均衡能力，也就是依靠iptables/ipvs的能力，可用于各种协议 Ingress：相对更加灵活，通过反向代理服务器实现负载均衡，仅用于http/https协议，这种场景下需要额外的反向代理服务以及ingress controller，nginx是大家熟知的反向代理，在k8s时代，出现了nginx-ingress，就是nginx+ingress controller的组合，ingress controller负责根据ingress资源生成nginx配置，当配置有变化时重启nginx。同时也出现了云原生的反向代理traefik，它相当于把ingress controller包含到其中合为一体，并且能够动态感知路由规则变化，不需重启。 traefik是一个相对较新的反向代理，网上相关资料不是特别丰富，研究了好几天，才成功访问到k8s dashboard，将其中的关键点记录于此。 安装traefik 使用helm安装，最新chart使用的traefik 1.7.19： 1helm install stable/traefik -f traefik-values.yaml traefik-values.yaml： 1234567891011121314151617181920212223rbac: enabled: truedashboard: enabled: true # 启用traefik dashboard ingress: annotations: traefik.ingress.kubernetes.io/rule-type: PathPrefixStripdeployment: hostPort: httpEnabled: true # traefik pod所在node上开启80端口 httpsEnabled: true # traefik pod所在node上开启443端口 dashboardEnabled: true # traefik pod所在node上开启8080端口，共traefik dashboard使用ssl: insecureSkipVerify: true # frontend不验证https的benkend enabled: true # 启用https入口extraVolumes: - name: traefik-ssl hostPath: path: /share/k8s/traefik/ssl # 其中存放https入口的证书和key，名字必须为tls.crt,tls.key type: DirectoryOrCreateextraVolumeMounts: - name: traefik-ssl mountPath: /ssl # traefik pod从/ssl目录读取上述tls.crt,tls.key 详细的配置方法见官方文档，上述关键点如下： 开启https入口，设置ssl.enabled=true，然后提供证书和key，上述通过从node节点本地目录mount到pod的方式，所以每个node节点要先放好证书和key，更好的方式是通过k8s secret，创建secret然后mount到pod 如何访问到入口，我是通过在node上打开端口，这时通过pod所在node就可以访问到入口，通过http://nodeip或https://nodeip；还可以使用NodePort类型service，这样通过http://any-nodeip:http-nodeport或https://any-nodeip:https-nodeport访问，value设置 serviceType: NodePort 路由匹配规则我使用的PathPrefixStrip，默认是host名匹配 因为启用了traefik dashboard，安装traefik会自动创建dashboard的ingress： 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: traefik.ingress.kubernetes.io/rule-type: PathPrefixStrip labels: app: traefik chart: traefik-1.82.1 heritage: Tiller release: traefik name: traefik-dashboard namespace: defaultspec: rules: - host: traefik.example.com http: paths: - backend: serviceName: traefik-dashboard servicePort: dashboard-http traefik是通过标签app: traefik选择到需要感知的ingress。自己添加的ingress注意包含这个标签。上述annotations和host是从value而来。因为我不想配host，所以用PathPrefixStrip路由规则，我修改了上述ingress如下: 12345678spec: rules: - http: paths: - backend: serviceName: traefik-dashboard servicePort: dashboard-http path: /traefik 这样当使用http://nodeip/traefik就可以访问到dashboard，因为在node上也开启了dashboard端口，也可以通过http://nodeip:8080访问。 代理k8s dashboard 目前最新的k8s dashboard(v2.0.0-beta6)安装在kubernetes-dashboard namespace： 1234kubectl get svc -n kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdashboard-metrics-scraper ClusterIP 10.254.238.13 &lt;none&gt; 8000/TCP 21dkubernetes-dashboard LoadBalancer 10.254.253.226 &lt;pending&gt; 443:30223/TCP 21d 增加ingress： 1234567891011121314151617apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: traefik.ingress.kubernetes.io/rule-type: PathPrefixStrip labels: app: traefik name: kubernetes-dashboard namespace: defaultspec: rules: - http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 443 path: /k8s 代理https后端 k8s dashboard只支持https访问，首先卡住的问题是如何代理https服务，frontend到backend的路由会出现以下几种情形： http-&gt;http http-&gt;https https-&gt;http https-&gt;https 当backend为https时，无论frontend是http或https，也就是2和4，都会报500错误，因为frontend无法验证backend，此时解决方法： 要么设置insecureSkipVerify，这样比较简单，如果采用这种方式frontend最好总是采用https，也就是设置redirect 要么设置ingress tls，配置host的tls证书信息 我采用的设置insecureSkipVerify的方法。一般最佳的使用方式也是入口总是用https，然后终结tls，后端是否https不重要。 代理不同namespace服务 解决上述问题后，接下来遇到k8s dashboard服务无法访问问题，在traefik dashboard中显示为红色，原因是helm安装traefik默认在default namespace中，而k8s dashboard安装在kubernetes-dashboard namespace中，不能跨namespace访问到服务，解决方法： 要么将traefik安装到和k8s dashboard同一空间 要么通过ExternalName将dashboard service引入到default namespace 1234567891011121314apiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: defaultspec: ports: - name: https port: 443 protocol: TCP targetPort: 443 sessionAffinity: None type: ExternalName externalName: kubernetes-dashboard.kubernetes-dashboard.svc.cluster.local 我采用的ExternalName方法。service的完整域名是servicename.namespace.svc.cluster.local，cluster.local是kubelet中配置的。 基于path路由 服务可以访问了，但是又出现了MIME type is not a supported stylesheet MIME type错误。 一开始以为是traefik在reponse header中加入了 X-Content-Type-Options: nosniff，但是发现traefik默认是不加入的。 最后发现是url路径问题，我的ingress仅使用path路由，没有使用host。 当使用https://nodeip/k8s访问k8s dashboard时，因为路由规则是PathPrefixStrip，到后端的请求是https://nodeip，这时得到主页，文件名是k8s，主页面k8s中的css,js等文件路径是相对于当前文档路径的，所以request url是https://nodeip/xxx.css，这时就匹配不上路由规则，出现上述错误。 如果使用https://nodeip/k8s/访问dashboard，就一切正常了。 所以使用路径匹配路由时是存在一定风险的，和主页中的资源路径定义有关： 主页中css,js等资源路径定义方式 说明 没有定义base，资源路径不以./或…/或/开头或&lt;base href=&quot;./&quot;&gt;，资源路径以./开头 1.匹配/path时，只能通过https://xxxx/path/访问2.匹配/path，并且后端重定向到sub/，这时通过https://xxxx/path/或https://xxxx/path都可以访问 &lt;base href=&quot;/&quot;&gt;，资源路径不以./或…/或/开头 只能匹配/，其他路径匹配都无法正常工作确实碰到这种情况，例如monocular 所以最好的方式还是通过host匹配路由。 dashboard认证 了解上述问题后，终于进入到dashboard登陆界面： 一开始我是使用的http入口，使用Token方式登陆，没有任何响应，通过开发者工具查看，发现问题是在使用http入口时，header中没有携带jweToken，导致认证失败，必须使用https入口。 回想起之前通过kubectl proxy，即http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/也是登陆不了，其实是一样的问题。 使用http入口登陆失败： 使用https入口时，jweToken是携带了，登陆成功： ) 所以果断设置frontend总是https，values增加traefik.ingress.kubernetes.io/redirect-entry-point: https，然后helm upgrade，自己增加的ingress需要自己修改： 1234567dashboard: enabled: true ingress: annotations: traefik.ingress.kubernetes.io/rule-type: PathPrefixStrip traefik.ingress.kubernetes.io/redirect-entry-point: https # 不要使用ingress.kubernetes.io/ssl-redirect: "true"，因为会丢掉path 这样无论使用http://nodeip/k8s/还是https://nodeip/k8s/都可以成功登陆。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>traefik</tag>
        <tag>k8s dashboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes版本号是怎么生成的]]></title>
    <url>%2F2019%2F11%2F19%2F020-kubernetes-version%2F</url>
    <content type="text"><![CDATA[通过源码编译kubernetes时，可以使用go build(或go install)单独编译某个组件，例如对于apiserver，可以cd到k8s.io/kubernetes/cmd/kube-apiserver，然后执行： 1go install -i -v -gcflags='-N -l' 编译结果安装到GOBIN下，即GOBIN/kube-apiserver，使用这种方式编译时有一个小问题，版本号是一段奇怪的字符串： 12kube-apiserver --versionKubernetes v0.0.0-master+$Format:%h$ 在遇到一些需要依赖kubernetes版本号的场景就会有问题，例如使用helm安装chart时，有些chart对kubernetes版本号有要求，就会无法安装。 有哪些版本号 kubernetes在很多场合都会看到版本号，我们先梳理一下。 –version 每个组件有–version参数，这时输出本组件的版本号。 kubectl version 123kubectl versionClient Version: version.Info&#123;Major:"", Minor:"", GitVersion:"v0.0.0-master+$Format:%h$", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"1970-01-01T00:00:00Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"&#125;Server Version: version.Info&#123;Major:"1", Minor:"17+", GitVersion:"v1.17.0-alpha.3.227+7d13dfe3c34f44-dirty", GitCommit:"7d13dfe3c34f44ff505afe397c7b05fe6e5414ed", GitTreeState:"dirty", BuildDate:"2019-11-18T14:42:24Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"&#125; Client Version即kubectl的版本号，Server Version即apiserver的版本号。 kubectl get nodes 1234kubectl get nodesNAME STATUS ROLES AGE VERSION10.10.10.15 Ready &lt;none&gt; 12d v1.17.0-alpha.3.227+7d13dfe3c34f44-dirty10.10.10.16 Ready &lt;none&gt; 11d v1.17.0-alpha.3.227+7d13dfe3c34f44-dirty 此处的版本号为kubelet的版本号。 版本号的含义 上述长长的一串版本号v1.17.0-alpha.3.227+7d13dfe3c34f44-dirty，看起来非常复杂，其实熟悉git的同学一眼就能看出正是git的源码版本信息。 分解字段 说明 v1.17.0-alpha.3 上一次tag，表示v1.17.0的第3个alpha版本 227 上一次tag之后提交数 7d13dfe3c34f44 最新一次提交id dirty 源码是否修改 这里描述了kubernetes的版本管理规则，据此可以看出上述版本一定是在master分支上。 版本号如何生成 那么版本号是如何写入到程序中的，主要有两个步骤： 从git获取源码版本信息 v1.17.0-alpha.3.227+7d13dfe3c34f44这一段是通过如下命令获取，并稍作转换： 12git describe --tags --match='v*' --abbrev=14v1.17.0-alpha.3-227-g7d13dfe3c34f44 如果tag之后没有再提交过，则后面就没有提交次数和最新提交id，只有tag，这通常发生在release分支上最后释放时。 -dirty是通过如下命令获取，如果源码修改过则版本号添加-dirty： 1git status --porcelain 通过ldflags将上述获取版本信息写入程序 golang中在链接阶段可以覆盖程序中的全局变量，通过在go build时增加如下参数。 1-ldflags -X importpath.name=value 其中importpath为包的导入路径，name为变量名，value为新的值。 对于kubernetes，版本信息相关的变量，服务端定义在k8s.io/component-base/version包中，源码见k8s.io/component-base/version/base.go，客户端定义在k8s.io/client-go/pkg/version中，源码见k8s.io/client-go/pkg/version/base.go，两个base.go的内容是一样的： 12345678910111213141516171819202122232425262728var ( // TODO: Deprecate gitMajor and gitMinor, use only gitVersion // instead. First step in deprecation, keep the fields but make // them irrelevant. (Next we'll take it out, which may muck with // scripts consuming the kubectl version output - but most of // these should be looking at gitVersion already anyways.) gitMajor string = "" // major version, always numeric gitMinor string = "" // minor version, numeric possibly followed by "+" // semantic version, derived by build scripts (see // https://git.k8s.io/community/contributors/design-proposals/release/versioning.md // for a detailed discussion of this field) // // TODO: This field is still called "gitVersion" for legacy // reasons. For prerelease versions, the build metadata on the // semantic version is a git hash, but the version itself is no // longer the direct output of "git describe", but a slight // translation to be semver compliant. // NOTE: The $Format strings are replaced during 'git archive' thanks to the // companion .gitattributes file containing 'export-subst' in this same // directory. See also https://git-scm.com/docs/gitattributes gitVersion string = "v0.0.0-master+$Format:%h$" gitCommit string = "$Format:%H$" // sha1 from git, output of $(git rev-parse HEAD) gitTreeState string = "" // state of git tree, either "clean" or "dirty" buildDate string = "1970-01-01T00:00:00Z" // build date in ISO8601 format, output of $(date -u +'%Y-%m-%dT%H:%M:%SZ')) 一开始出现的奇怪的字符串v0.0.0-master+$Format:%h$就是出在这里。当然直接修改这个文件也可以得到正确的版本号，但是每次源码版本变化都需要修改，非常麻烦，采用ldflags就非常简单： 1go build -ldflags='-X k8s.io/component-base/version.gitVersion=v1.17.0-alpha.3.227+7d13dfe3c34f44-dirty -X k8s.io/client-go/pkg/version.gitVersion=v1.17.0-alpha.3.227+7d13dfe3c34f44-dirty' 上述命令就将gitVersion变量修改为从git中得到的源码版本号，这个过程通过脚本完全可以做到自动化，这个脚本就是k8s.io/kubernetes/hack/lib/version.sh。 go build时植入版本号 在使用make编译kubernetes时会自动从git获取版本信息并通过ldflags植入程序中，但是通过go build编译时kubernetes并没有提供脚本实现上述过程，这就导致了版本号总是v0.0.0-master+$Format:%h$。 为了解决上述问题，只需要对k8s.io/kubernetes/hack/lib/version.sh稍作改造，复用其逻辑即可： 首先修改version.sh，因为启用了go module，如果你没有显示启用可以不需要修改： 1234ldflags+=( "-X '$&#123;KUBE_GO_PACKAGE&#125;/vendor/k8s.io/client-go/pkg/version.$&#123;key&#125;=$&#123;val&#125;'" "-X '$&#123;KUBE_GO_PACKAGE&#125;/vendor/k8s.io/component-base/version.$&#123;key&#125;=$&#123;val&#125;'") 改为： 1234ldflags+=( "-X '$&#123;KUBE_GO_PACKAGE&#125;/client-go/pkg/version.$&#123;key&#125;=$&#123;val&#125;'" "-X '$&#123;KUBE_GO_PACKAGE&#125;/component-base/version.$&#123;key&#125;=$&#123;val&#125;'") 然后在k8s.io/kubernetes/hack目录下添加脚本文件goinstall如下： 12345678#!/usr/bin/env bashbasepath=$(cd `dirname $0`; pwd)KUBE_ROOT=$&#123;basepath&#125;/..source "$&#123;KUBE_ROOT&#125;/hack/lib/version.sh"KUBE_GO_PACKAGE="k8s.io"ldflags=$(kube::version::ldflags)echo ldflags:$&#123;ldflags&#125;go install -i -v -gcflags='-N -l' -ldflags="$&#123;ldflags&#125;" 将k8s.io/kubernetes/hack目录添加到path，然后在单个组件目录下就可以直接执行： 1goinstall]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>version</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解kubelet认证和授权]]></title>
    <url>%2F2019%2F11%2F12%2F019-access-pod-forbidden%2F</url>
    <content type="text"><![CDATA[在通过kubectl访问pod信息，例如执行kubectl logs，常常会遇到类似如下错误： Error from server (Forbidden): Forbidden (user=system:anonymous, verb=get, resource=nodes, subresource=proxy) ( pods/log tiller-deploy-6b5ffb6f-lg9jb) 网上搜索可以通过启用anonymous访问，也就是使用–anonymous-auth=true或者配置文件添加： 123authentication: anonymous: enabled: true 但是设置之后错误依旧，为此我探究了一下kubelet的认证机制，终于将问题解决，其实很简单，答案后面揭晓。 我们知道kubectl只会和apiserver交互，对于kubectl logs、kubectl exec等需要访问pod的这些命令，实际上是apiserver调用kubelet接口完成的，上述错误正是出在这个过程，而不是kubectl到apiserver的过程。 kubelet通过port指定的端口（默认10250）对外暴露服务，这个服务是需要TLS认证的，同时也可以通过 readOnlyPort 端口（默认10255，0表示关闭）对外暴露只读服务，这个服务是不需要认证的。apiserver通过–kubelet-https参数指定调用哪个服务，true为前者，false为后者，此时只能执行只读操作。下面主要说一下前者。 认证过程 配置认证方式 有三种可配置认证方式： TLS认证，这也是默认的 1234567authentication: anonymous: enabled: false webhook: enabled: false x509: clientCAFile: xxxx 允许anonymous，这时可不配置客户端证书 123authentication: anonymous: enabled: true webhook，这时可不配置客户端证书 123authentication: webhook: enabled: true 这时kubelet通过bearer tokens，找apiserver认证，如果存在对应的serviceaccount，则认证通过。 如果2开启，则忽略x509和webhook认证；否则，如果1和3同时开启，则按1、3的顺序依次认证，任何一个认证通过则返回通过，否则认证不通过。 通过kubectl命令行访问kubelet时，无法传递bearer tokens，所以无法使用webhook认证，这时只能使用x509认证。 证书配置 kubelet对外暴露https服务，必须设置服务端证书，如果通过x509证书认证客户端，那么还需要配置客户端证书。下面说明证书配置的三种方法： 手工指定证书 假设ca的证书和key：ca.pem，ca-key.pem 用上述ca生成kubelet服务端证书和key：kubelet-server.pem、kubelet-server-key.pem 用上述ca生成apiserver使用的客户端证书和key：kubelet-client.pem、kubelet-client-key.pem，证书CN为kubelet-client 修改kubelet的配置文件： 12345tlsCertFile: kubelet-server.pemtlsPrivateKeyFile: kubelet-server-key.pemauthentication: x509: clientCAFile: ca.pem 修改apiserver参数： 1--kubelet-certificate-authority=ca.pem --kubelet-client-certificate=kubelet-client.pem --kubelet-client-key=kubelet-client-key.pem 授权kubelet-client用户： 1kubectl create clusterrolebinding kubelet-admin --clusterrole=system:kubelet-api-admin --user=kubelet-client 经过上面5步，认证的过程实际已经OK了，第6步是为授权过程服务的，kubelet的授权是通过webhook委托给apiserver的。 自签名证书和key 实际上是上述过程的特化，不指定tlsCertFile和tlsPrivateKeyFile时，kubelet会自动生成服务端证书保存在–cert-dir指定目录中，文件名为kubelet.crt和kubelet.key，这个证书是自签名的，所以apiserver不需要指定–kubelet-certificate-authority，其他配置是一样的。 通过TLS bootstrap机制 还可以通过TLS bootstrap机制分配kubelet服务证书。跟配置分配访问apiserver的客户端证书方法是一样的（参考官方文档），额外的配置如下： 修改kubelet配置文件： 1serverTLSBootstrap: true 授权允许创建kubelet服务端证书 首先创建system:certificates.k8s.io:certificatesigningrequests:selfnodeserver，默认是没有创建的。 selfnodeserver.yaml文件如下： 12345678910# A ClusterRole which instructs the CSR approver to approve a node requesting a# serving cert matching its client cert.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: ["certificates.k8s.io"] resources: ["certificatesigningrequests/selfnodeserver"] verbs: ["create"] 1kubectl create -f selfnodeserver.yaml 然后创建绑定： 1kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes 手工批准证书请求： 先查询证书请求： 123[vagrant@localhost etc]$ kubectl get csrNAME AGE REQUESTOR CONDITIONcsr-2jxvn 76s system:node:10.10.10.16 Pending 然后批准证书： 1kubectl certificate approve csr-2jxvn 之后在相应kubelet的–cert-dir目录可以看到服务端证书已经生成。 配置客户端证书和前面的方法是一样的，上面3步只是生成服务端证书。 选择哪种方式 客户端证书配置是免不了的，区别是在服务端证书，显然自动生成更加方便，TLS bootstrap相对于自签名证书更加安全，集群统一使用信任的CA签名。 TLS bootstrap还可以配置证书过期后自动重新生成，方法是修改kubelet配置文件： 1rotateCertificates: true TLS bootstrap分配证书的有效期可以通过kube-controller-manager如下参数修改，默认8760h0m0s，也就是1年： 1--experimental-cluster-signing-duration 授权过程 配置授权方式 可配置两种授权方式： AlwaysAllow：从字面意思就可知道 12authorization: mode: AlwaysAllow Webhook：这是默认模式 12authorization: mode: Webhook 这时授权过程是委托给apiserver的，使用apiserver一样的授权模式，也就是RBAC。 配置权限 如果通过Webhook授权，就需要通过RBAC为用户配置权限。 首先要弄清楚通过认证的用户是什么，通过x509证书认证的用户名是客户端证书中的CN字段，用户组为O字段；通过webhook认证的用户是token对应的serviceaccount；没有通过认证或使能anonymous，则用户为system:anonymous。 其次要弄清楚应该授权什么权限，系统已经存在一个system:kubelet-api-admin角色，这是最高的权限，可以根据需要创建低权限角色。 12345678910111213[vagrant@localhost kube-apiserver]$ kubectl describe clusterrole system:kubelet-api-adminName: system:kubelet-api-adminLabels: kubernetes.io/bootstrapping=rbac-defaultsAnnotations: rbac.authorization.kubernetes.io/autoupdate: truePolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- nodes/log [] [] [*] nodes/metrics [] [] [*] nodes/proxy [] [] [*] nodes/spec [] [] [*] nodes/stats [] [] [*] nodes [] [] [get list watch proxy] 最后将用户和角色绑定，即完成权限的配置。 总结 如何配置kubelet的认证和授权，归结起来常用如下2种做法： 省事型，可用于开发环境 12345authentication: anonymous: enabled: trueauthorization: mode: AlwaysAllow 一开始出现的Forbidden问题就是没有配置AlwaysAllow，默认是Webhook。 安全型，生产环境使用 12345authentication: anonymous: enabled: falseauthorization: mode: Webhook 服务端证书通过TLS bootstrap，客户端证书需要手工配置。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是kubernetes服务端打印]]></title>
    <url>%2F2019%2F10%2F30%2F018-output-change-kubectl-get-cs%2F</url>
    <content type="text"><![CDATA[喜欢尝鲜的同学可能会注意到最新的kubernetes在执行kubectl get cs时输出内容有一些变化，以前是这样的： 12345&gt; kubectl get componentstatusesNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;"health":"true"&#125; 现在变成了： 12345&gt; kubectl get componentstatusesNAME Age controller-manager &lt;unknown&gt;scheduler &lt;unknown&gt;etcd-0 &lt;unknown&gt; 起初可能会以为集群部署有问题，通过kubectl get cs -o yaml发现status、message等信息都有，只是没有打印出来。原来是kubectl get cs的输出格式有变化，那么为什么会有此变化，我们来一探究竟。 定位问题原因 尝试之前的版本，发现1.15还是正常的，调试代码发现对componentstatuses的打印代码在k8s.io/kubernetes/pkg/printers/internalversion/printers.go中，其中的AddHandlers方法负责把各种资源的打印函数注册进来。 123456789101112// AddHandlers adds print handlers for default Kubernetes types dealing with internal versions.// TODO: handle errors from Handlerfunc AddHandlers(h printers.PrintHandler) &#123;...... componentStatusColumnDefinitions := []metav1beta1.TableColumnDefinition&#123; &#123;Name: "Name", Type: "string", Format: "name", Description: metav1.ObjectMeta&#123;&#125;.SwaggerDoc()["name"]&#125;, &#123;Name: "Status", Type: "string", Description: "Status of the component conditions"&#125;, &#123;Name: "Message", Type: "string", Description: "Message of the component conditions"&#125;, &#123;Name: "Error", Type: "string", Description: "Error of the component conditions"&#125;, &#125; h.TableHandler(componentStatusColumnDefinitions, printComponentStatus) h.TableHandler(componentStatusColumnDefinitions, printComponentStatusList) 对AddHandlers的调用位于k8s.io/kubernetes/pkg/kubectl/cmd/get/humanreadable_flags.go(正在迁移到staging中，如果找不到就去staging中找)中，如下32行位置： 12345678910111213141516171819202122232425262728293031323334353637// ToPrinter receives an outputFormat and returns a printer capable of// handling human-readable output.func (f *HumanPrintFlags) ToPrinter(outputFormat string) (printers.ResourcePrinter, error) &#123; if len(outputFormat) &gt; 0 &amp;&amp; outputFormat != "wide" &#123; return nil, genericclioptions.NoCompatiblePrinterError&#123;Options: f, AllowedFormats: f.AllowedFormats()&#125; &#125; showKind := false if f.ShowKind != nil &#123; showKind = *f.ShowKind &#125; showLabels := false if f.ShowLabels != nil &#123; showLabels = *f.ShowLabels &#125; columnLabels := []string&#123;&#125; if f.ColumnLabels != nil &#123; columnLabels = *f.ColumnLabels &#125; p := printers.NewTablePrinter(printers.PrintOptions&#123; Kind: f.Kind, WithKind: showKind, NoHeaders: f.NoHeaders, Wide: outputFormat == "wide", WithNamespace: f.WithNamespace, ColumnLabels: columnLabels, ShowLabels: showLabels, &#125;) printersinternal.AddHandlers(p) // TODO(juanvallejo): handle sorting here return p, nil&#125; 查看humanreadable_flags.go文件的修改历史，发现是在2019.6.28日特意去掉了对内部对象的打印，影响版本从1.16之后。 为什么修改 我没有查到官方的说明，在此做一些个人猜测，还原整个过程： 最初对api resource的表格打印都是在kubectl中实现 这样对于其他客户端需要做重复的事情，而且可能实现的行为不一致，因此有必要将表格打印放到服务端，也就是apiserver中 服务端打印的支持经过一个逐步的过程，所以客户端并没有完全去除，是同时支持的，kubectl判断服务端返回的Table就直接打印，否则使用具体对象的打印，客户端和服务端对特定对象的打印都是调用k8s.io/kubernetes/pkg/printers/internalversion/printers.go来实现 1.11版本将kubectl的命令行参数--server-print默认设置为true 到了1.16版本，社区可能认为所有的对象都移到服务端了，这时就去除了客户端kubectl中的打印 但实际上componentstatuses被遗漏了，那么为什么遗漏，可能主要是因为componentstatuses对象跟其他对象不一样，它是每次实时获取，而不是从缓存获取，其他对象，例如pod是从etcd获取，对结果的格式化定义在k8s.io/kubernetes/pkg/registry/core/pod/storage/storage.go中，如下15行位置： 12345678910111213141516// NewStorage returns a RESTStorage object that will work against pods.func NewStorage(optsGetter generic.RESTOptionsGetter, k client.ConnectionInfoGetter, proxyTransport http.RoundTripper, podDisruptionBudgetClient policyclient.PodDisruptionBudgetsGetter) PodStorage &#123; store := &amp;genericregistry.Store&#123; NewFunc: func() runtime.Object &#123; return &amp;api.Pod&#123;&#125; &#125;, NewListFunc: func() runtime.Object &#123; return &amp;api.PodList&#123;&#125; &#125;, PredicateFunc: pod.MatchPod, DefaultQualifiedResource: api.Resource("pods"), CreateStrategy: pod.Strategy, UpdateStrategy: pod.Strategy, DeleteStrategy: pod.Strategy, ReturnDeletedObject: true, TableConvertor: printerstorage.TableConvertor&#123;TableGenerator: printers.NewTableGenerator().With(printersinternal.AddHandlers)&#125;, &#125; componentstatuses没有用到真正的Storge，而它又相对不起眼，所以被遗漏了。 暂时解决办法 如果希望打印和原来类似的内容，目前只有通过模板： 1kubectl get cs -o=go-template='&#123;&#123;printf "|NAME|STATUS|MESSAGE|\n"&#125;&#125;&#123;&#123;range .items&#125;&#125;&#123;&#123;$name := .metadata.name&#125;&#125;&#123;&#123;range .conditions&#125;&#125;&#123;&#123;printf "|%s|%s|%s|\n" $name .status .message&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;' 输出结果： 1234|NAME|STATUS|MESSAGE||controller-manager|True|ok||scheduler|True|ok||etcd-0|True|&#123;&quot;health&quot;:&quot;true&quot;&#125;| 深入打印处理流程 kubectl通过-o参数控制输出的格式，有yaml、json、模板和表格几种样式，上述问题是出在表格打印时，不加-o参数默认就是表格打印，下面我们详细分析一下kubectl get的打印输出过程。 kubectl kubectl get的代码入口在k8s.io/kubernetes/pkg/kubectl/cmd/get/get.go中，Run字段就是命令执行方法： 123456789func NewCmdGet(parent string, f cmdutil.Factory, streams genericclioptions.IOStreams) *cobra.Command &#123;...... Run: func(cmd *cobra.Command, args []string) &#123; cmdutil.CheckErr(o.Complete(f, cmd, args)) cmdutil.CheckErr(o.Validate(cmd)) cmdutil.CheckErr(o.Run(f, cmd, args)) &#125;,...... &#125; Complete方法完成了Printer初始化，位于k8s.io/kubernetes/pkg/kubectl/cmd/get/get_flags.go中： 1234567func (f *PrintFlags) ToPrinter() (printers.ResourcePrinter, error) &#123;...... if p, err := f.HumanReadableFlags.ToPrinter(outputFormat); !genericclioptions.IsNoCompatiblePrinterError(err) &#123; return p, err &#125;......&#125; 不带-o参数时，上述方法返回的是f.HumanReadableFlags.ToPrinter(outputFormat)，最后返回的是HumanReadablePrinter对象，位于k8s.io/cli-runtime/pkg/printers/tableprinter.go中： 1234567// NewTablePrinter creates a printer suitable for calling PrintObj().func NewTablePrinter(options PrintOptions) ResourcePrinter &#123; printer := &amp;HumanReadablePrinter&#123; options: options, &#125; return printer&#125; 再回到命令执行主流程，Complete之后主要是Run，其中完成向apiserver发送http请求并打印结果的动作，在发送http请求前，有一个很重要的动作，加入服务端打印的header，服务端打印可以通过--server-print参数控制，从1.11默认为true，这样服务端如果支持转换就会返回metav1beta1.Table类型，置为false也可以禁用服务端打印： 1234567891011121314func (o *GetOptions) transformRequests(req *rest.Request) &#123;...... if !o.ServerPrint || !o.IsHumanReadablePrinter &#123; return &#125; group := metav1beta1.GroupName version := metav1beta1.SchemeGroupVersion.Version tableParam := fmt.Sprintf("application/json;as=Table;v=%s;g=%s, application/json", version, group) req.SetHeader("Accept", tableParam)......&#125; 最后打印是调用的HumanReadablePrinter.PrintObj方法，先判断服务端如果返回的metav1beta1.Table类型就直接打印，其次如果是metav1.Status类型也有专门的处理器，最后就会到默认处理器： 123456789101112131415161718192021222324func (h *HumanReadablePrinter) PrintObj(obj runtime.Object, output io.Writer) error &#123;...... // Parameter "obj" is a table from server; print it. // display tables following the rules of options if table, ok := obj.(*metav1beta1.Table); ok &#123; return printTable(table, output, localOptions) &#125; // Could not find print handler for "obj"; use the default or status print handler. // Print with the default or status handler, and use the columns from the last time var handler *printHandler if _, isStatus := obj.(*metav1.Status); isStatus &#123; handler = statusHandlerEntry &#125; else &#123; handler = defaultHandlerEntry &#125;...... if err := printRowsForHandlerEntry(output, handler, eventType, obj, h.options, includeHeaders); err != nil &#123; return err &#125;...... return nil&#125; 默认处理器会打印Name和Age两列，因为componetstatuses是实时获取，没有存储在etcd中，没有创建时间，所以Age打印出来就是unknown。 apiserver 再来看服务端的处理流程，apiserver对外提供REST接口实现在k8s.io/apiserver/pkg/endpoints/handlers目录下，kubectl get cs会进入get.go中ListResource方法，如下列出关键的三个步骤： 12345678910111213func ListResource(r rest.Lister, rw rest.Watcher, scope *RequestScope, forceWatch bool, minRequestTimeout time.Duration) http.HandlerFunc &#123; return func(w http.ResponseWriter, req *http.Request) &#123;...... outputMediaType, _, err := negotiation.NegotiateOutputMediaType(req, scope.Serializer, scope)...... result, err := r.List(ctx, &amp;opts)...... transformResponseObject(ctx, scope, trace, req, w, http.StatusOK, outputMediaType, result)...... &#125;&#125; NegotiateOutputMediaType中根据客户端的请求header设置服务端的一些行为，包括是否服务端打印；r.List从Storage层获取资源数据，具体实现在k8s.io/kubernetes/pkg/registry下；transformResponseObject将结果返回给客户端。 先说transformResponseObject，其中就会根据NegotiateOutputMediaType返回的outputMediaType的Convert字段判断是否转为换目标类型，如果为Table就会将r.List返回的具体资源类型转换为Table类型： 12345678910111213141516func doTransformObject(ctx context.Context, obj runtime.Object, opts interface&#123;&#125;, mediaType negotiation.MediaTypeOptions, scope *RequestScope, req *http.Request) (runtime.Object, error) &#123;...... switch target := mediaType.Convert; &#123; case target == nil: return obj, nil ...... case target.Kind == "Table": options, ok := opts.(*metav1beta1.TableOptions) if !ok &#123; return nil, fmt.Errorf("unexpected TableOptions, got %T", opts) &#125; return asTable(ctx, obj, options, scope, target.GroupVersion())......g &#125;&#125; 上述asTable最终调用scope.TableConvertor.ConvertToTable完成表格转换工作，在本文的问题中，就是因为mediaType.Convert为空而没有触发这个转换，那么为什么为空呢，问题就出在NegotiateOutputMediaType，它最终会调用到k8s.io/apiserver/pkg/endpoints/handlers/rest.go的AllowsMediaTypeTransform方法，是因为scope.TableConvertor为空，最终转换为Table也是调用的它： 12345678910func (scope *RequestScope) AllowsMediaTypeTransform(mimeType, mimeSubType string, gvk *schema.GroupVersionKind) bool &#123;...... if gvk.GroupVersion() == metav1beta1.SchemeGroupVersion || gvk.GroupVersion() == metav1.SchemeGroupVersion &#123; switch gvk.Kind &#123; case &quot;Table&quot;: return scope.TableConvertor != nil &amp;&amp; mimeType == &quot;application&quot; &amp;&amp; (mimeSubType == &quot;json&quot; || mimeSubType == &quot;yaml&quot;)......&#125; 进一步跟踪，RequestScope是在apiserver初始化的时候创建的，每类资源一个，比如componentstatuses有一个全局的，pod有一个全局的，初始化的过程如下： apiserver初始化入口在k8s.io/kubernetes/pkg/master/master.go的InstallLegacyAPI和InstallAPIs方法中，前者主要针对一些老的资源，具体有哪些见下面的NewLegacyRESTStorage方法，其中就包含componentStatuses，其他资源通过InstallAPIs初始化： 12345// InstallLegacyAPI will install the legacy APIs for the restStorageProviders if they are enabled.func (m *Master) InstallLegacyAPI(c *completedConfig, restOptionsGetter generic.RESTOptionsGetter, legacyRESTStorageProvider corerest.LegacyRESTStorageProvider) error &#123; legacyRESTStorage, apiGroupInfo, err := legacyRESTStorageProvider.NewLegacyRESTStorage(restOptionsGetter)......&#125; 初始化Storage： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859func (c LegacyRESTStorageProvider) NewLegacyRESTStorage(restOptionsGetter generic.RESTOptionsGetter) (LegacyRESTStorage, genericapiserver.APIGroupInfo, error) &#123; apiGroupInfo := genericapiserver.APIGroupInfo&#123; PrioritizedVersions: legacyscheme.Scheme.PrioritizedVersionsForGroup(&quot;&quot;), VersionedResourcesStorageMap: map[string]map[string]rest.Storage&#123;&#125;, Scheme: legacyscheme.Scheme, ParameterCodec: legacyscheme.ParameterCodec, NegotiatedSerializer: legacyscheme.Codecs, &#125;...... restStorageMap := map[string]rest.Storage&#123; &quot;pods&quot;: podStorage.Pod, &quot;pods/attach&quot;: podStorage.Attach, &quot;pods/status&quot;: podStorage.Status, &quot;pods/log&quot;: podStorage.Log, &quot;pods/exec&quot;: podStorage.Exec, &quot;pods/portforward&quot;: podStorage.PortForward, &quot;pods/proxy&quot;: podStorage.Proxy, &quot;pods/binding&quot;: podStorage.Binding, &quot;bindings&quot;: podStorage.LegacyBinding, &quot;podTemplates&quot;: podTemplateStorage, &quot;replicationControllers&quot;: controllerStorage.Controller, &quot;replicationControllers/status&quot;: controllerStorage.Status, &quot;services&quot;: serviceRest, &quot;services/proxy&quot;: serviceRestProxy, &quot;services/status&quot;: serviceStatusStorage, &quot;endpoints&quot;: endpointsStorage, &quot;nodes&quot;: nodeStorage.Node, &quot;nodes/status&quot;: nodeStorage.Status, &quot;nodes/proxy&quot;: nodeStorage.Proxy, &quot;events&quot;: eventStorage, &quot;limitRanges&quot;: limitRangeStorage, &quot;resourceQuotas&quot;: resourceQuotaStorage, &quot;resourceQuotas/status&quot;: resourceQuotaStatusStorage, &quot;namespaces&quot;: namespaceStorage, &quot;namespaces/status&quot;: namespaceStatusStorage, &quot;namespaces/finalize&quot;: namespaceFinalizeStorage, &quot;secrets&quot;: secretStorage, &quot;serviceAccounts&quot;: serviceAccountStorage, &quot;persistentVolumes&quot;: persistentVolumeStorage, &quot;persistentVolumes/status&quot;: persistentVolumeStatusStorage, &quot;persistentVolumeClaims&quot;: persistentVolumeClaimStorage, &quot;persistentVolumeClaims/status&quot;: persistentVolumeClaimStatusStorage, &quot;configMaps&quot;: configMapStorage, &quot;componentStatuses&quot;: componentstatus.NewStorage(componentStatusStorage&#123;c.StorageFactory&#125;.serversToValidate), &#125;...... apiGroupInfo.VersionedResourcesStorageMap[&quot;v1&quot;] = restStorageMap return restStorage, apiGroupInfo, nil&#125; 注册REST接口的handler，handler中包含RequestScope，RequestScope中的TableConvertor字段是从storage取出，也就是上述NewLegacyRESTStorage创建的资源对应的storage，例如componentStatuses，就是componentstatus.NewStorage(componentStatusStorage{c.StorageFactory}.serversToValidate)，提取的方法是类型断言storage.(rest.TableConvertor)，也就是storage要实现rest.TableConvertor接口，否则取出来为空： 123456789101112131415161718192021222324252627282930func (a *APIInstaller) registerResourceHandlers(path string, storage rest.Storage, ws *restful.WebService) (*metav1.APIResource, error) &#123;...... tableProvider, _ := storage.(rest.TableConvertor) var apiResource metav1.APIResource...... reqScope := handlers.RequestScope&#123;...... // TODO: Check for the interface on storage TableConvertor: tableProvider,...... for _, action := range actions &#123;...... switch action.Verb &#123;...... case "LIST": // List all resources of a kind. doc := "list objects of kind " + kind if isSubresource &#123; doc = "list " + subresource + " of objects of kind " + kind &#125; handler := metrics.InstrumentRouteFunc(action.Verb, group, version, resource, subresource, requestScope, metrics.APIServerComponent, restfulListResource(lister, watcher, reqScope, false, a.minRequestTimeout))...... &#125; // Note: update GetAuthorizerAttributes() when adding a custom handler. &#125;......&#125; 具体看componentStatuses的storage，在k8s.io/kubernetes/pkg/registry/core/componentstatus/rest.go中，确实没有实现rest.TableConvertor接口，所以componentStatuses的handler的RequestScope中的TableConvertor字段就为空，最终导致了问题： 12345678910type REST struct &#123; GetServersToValidate func() map[string]*Server &#125;// NewStorage returns a new REST.func NewStorage(serverRetriever func() map[string]*Server) *REST &#123; return &amp;REST&#123; GetServersToValidate: serverRetriever, &#125;&#125; 代码修复 找到了根本原因之后，修复就比较简单了，就是storage需要实现rest.TableConvertor接口，接口定义在k8s.io/apiserver/pkg/registry/rest/rest.go中： 123type TableConvertor interface &#123; ConvertToTable(ctx context.Context, object runtime.Object, tableOptions runtime.Object) (*metav1beta1.Table, error)&#125; 参照其他资源storage代码，修改k8s.io/kubernetes/pkg/registry/core/componentstatus/rest.go代码如下，问题得到解决，kubectl get cs打印出熟悉的表格，如果使用kubectl get cs --server-print=false仍会只打印Name、Age两列： 123456789101112131415type REST struct &#123; GetServersToValidate func() map[string]*Server tableConvertor printerstorage.TableConvertor&#125;// NewStorage returns a new REST.func NewStorage(serverRetriever func() map[string]*Server) *REST &#123; return &amp;REST&#123; GetServersToValidate: serverRetriever, tableConvertor: printerstorage.TableConvertor&#123;TableGenerator: printers.NewTableGenerator().With(printersinternal.AddHandlers)&#125;, &#125;&#125;func (r *REST) ConvertToTable(ctx context.Context, object runtime.Object, tableOptions runtime.Object) (*metav1beta1.Table, error) &#123; return r.tableConvertor.ConvertToTable(ctx, object, tableOptions)&#125; 最后 本想提交一个PR给kubernetes，发现已经有人先一步提了，解决方法和我一摸一样，只是上述tableConvertor字段是大写开头，我觉得小写更好，有点遗憾。而这个问题在2019.9.23已经有人提出，也就是1.16刚发布的时候，9.24就有人提了PR，解决速度非常之快，可见开源软件的优势以及k8s热度之高，有无数的开发者为其贡献力量，k8s就像聚光灯下的明星，无数双眼睛注目着。不过这个PR现在还没有合入主干，还在代码审查阶段，这个bug相对来讲不是很严重，所以优先级不那么高，要知道现在还有1097个PR。虽然最后有一点小遗憾，不过在解决问题的过程中对kubernetes的理解也更进一步，还是收获良多。在阅读代码的过程中，随处可见各种TODO，发现代码不断在重构，今天代码在这里，明天代码就搬到另一个地方了，k8s这么一个冉冉升起的新星，虽然从2017年起就成为容器编排的事实标准，并被广泛应用到生产环境，但它本身还在不断进化，还需不断完善。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>go template</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CA证书和自签名证书]]></title>
    <url>%2F2019%2F10%2F25%2F017-what-is-a-self-signed-certificate%2F</url>
    <content type="text"><![CDATA[在kubernetes集群的搭建过程中，需要使用许多证书，例如运行kube-apiserver需要指定客户端CA证书、服务端证书和私钥，而客户端组件如kubectl需要指定自己的证书、私钥以及服务端CA证书。 1kube-apiserver --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem kube-controller-manager组件还需要指定CA私钥，用于为kubelet签名证书。 1kube-controller-manager --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem 这些文件在TLS认证过程中发挥什么作用，关键需要理解CA证书以及自签名证书。 对称加密和非对称加密 对称加密：如AES，速度快，但密钥管理困难 非对称加密：如RSA，速度慢，两种用途：(1)加密传输过程：公钥加密，私钥解密；(2)签名过程：私钥加密，公钥解密 一般传输数据用对称加密，对对称密钥的传输用非对称加密，例如TLS协议握手阶段通过非对称加密交换一些信息，从而生成对称密钥，后续数据传输就用此密钥。 签名和证书 签名的作用 确保信息不被篡改 确保信息来自发布者 签名的原理 对信息HASH后得到摘要，然后用私钥加密得到签名，接收方能够用公钥解密说明是对方发送的，然后再次对信息HASH得到相同的摘要值说明信息没有被修改。 证书 简单来说就是对公钥签名，上述签名过程的漏洞在于接收方持有的公钥如果被人替换，就可以被假冒者回复信息，解决的办法是对公钥进行认证，也就是通过CA密钥对公钥进行加密得到证书，接收人不直接拿到公钥，而是证书，接收人通过CA的公钥可以从证书中验证得到发送人的公钥。只要CA私钥不泄露，就不可能被其他人冒充。 CA证书 CA是一个机构，专门为其他人签发证书，这个证书保存其他人的公钥，确保了公钥的来源且没有被篡改。 CA本身有自己的公私钥对，私钥用于签发其他证书，公钥用于验证证书，CA公钥同样需要保护，这就是CA证书。 那么CA证书是谁签发呢，从签名的原理来看，必然存在CA自己给自己签名，这就是根CA证书。 根CA是非常宝贵的，通常出于安全的考虑会签发一些中间CA证书，然后由中间CA签发最终用户证书，这样就构成了一个信任链。接收者信任某个CA证书，那么由此CA签发的证书就都被信任。 公信的根CA全球只有有限的一些，它们通过第三方机构审计，具有权威性和公正性，通常操作系统或浏览器已经内置安装，由这些根CA签发的证书都会被信任。 用户也可以自行安装信任的证书，其风险需要用户自己承担，一定要确保证书来源可靠。 自签名证书 所以根证书一定是自签名的。 公信的CA是非常宝贵的资源，从其申请证书是需要付费的，并且有些场景并不需要公信的证书，这时就会自建CA，需要用到自签名，下面通过两张图说明自签名证书和非自签名证书的区别。 自签名证书 非自签名证书 kubernetes集群就是采用自建的CA，用自建CA签发各组件交互需要的证书，CA证书起到验证对端身份的作用，交互的双方都需要知道对方的CA证书。kube-controller-manager需要为kubelete签发证书，所以它还需要CA的私钥。]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>certificate</tag>
        <tag>self-signed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用vscode远程调试golang]]></title>
    <url>%2F2019%2F10%2F21%2F016-remote-debug-golang-with-vscode%2F</url>
    <content type="text"><![CDATA[在学习一些大型golang项目时，例如kubernetes，通过调试器一边跟踪代码一边学习其原理是很好的方法，一般k8s部署在远程环境，在本机进行开发，很自然对远程调试有迫切需求，这也是和调试一般golang程序最主要的诉求区别点，本文重点介绍远程调试这种场景。 vscode远程调试 vscode的成功很重要的一点是进程隔离的插件设计，构建出了丰富的生态环境，而vscode自身保持轻量并提供稳定流畅的编辑体验。 vscode实现远程调试有两种方法： 第一种是网络上搜索出来最多的，通过dlv的远程调试功能，在服务端启动dlv server，客户端vscode连接dlv server 这种方法存在明显缺陷： 只能attach调试 浏览的代码是本地的，远程修改后要同步到本地 第二种是使用vscode的远程开发功能，该功能最早是2019.5月发布，当时只在insider版本包含，后续stable版是从1.35.1开始包含。这个功能也是通过插件实现的，有Remote-SSH、Remote-WSL、Remote-Containers三个插件，适应三种不同的开发环境，名字一目了然 这种方法整个开发环境就在远程主机上，本地机器上只是UI，它能实现的不只是调试，而是整个开发过程，所以这个功能叫远程开发（Remote Development） attach或launch都可以，和本地调试体验一致 attach时mode有local和remote两种，前者vscode自动dlv attach，后者手工运行dlv attach，一般在被调试的进程没有权限attach时使用 本地不需要源代码，直接修改远程主机源代码 第二种方法无疑是当前最好的选择。当然无论采用哪种方法，最终调试其实都是通过dlv。 vscode Remote-SSH使用方法 下面详细介绍一下Remote-SSH的使用方法，vscode请使用1.35及以上版本。 建立Local Host到Remote Host的SSH通道 如果使用vagrant，它已经为我们配置好，运行vagrant ssh-config会显示ssh的端口、密钥文件等信息。 $ vagrant ssh-config Host master HostName 127.0.0.1 User vagrant Port 2222 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile D:/k8s/.vagrant/machines/master/virtualbox/private_key IdentitiesOnly yes LogLevel FATAL 验证ssh登陆： 1ssh -i D:/k8s/.vagrant/machines/master/virtualbox/private_key -p 2222 vagrant@localhost 如果可以连通虚拟机ip，也可以直接： 1ssh -i D:/k8s/.vagrant/machines/master/virtualbox/private_key vagrant@ip 如果自己配置密钥登陆，需要使用ssh-keygen生成公钥和密钥，将公钥内容写入服务端的authorized_keys文件中，客户端使用密钥文件登陆。 安装Remote - SSH插件 添加Host定义，编辑%USERPROFILE%/.ssh/config文件，添加 1234Host k8s_master HostName 10.10.10.14 User vagrant IdentityFile D:\k8s\.vagrant\machines\master\virtualbox\private_key 或者 12345Host k8s_master HostName 127.0.0.1 User vagrant IdentityFile D:\k8s\.vagrant\machines\master\virtualbox\private_key Port 2222 k8s_master是host的别名，后续ssh客户端可以直接用别名连接，不用再指定User，HostName和IdentityFile等参数，例如： 1ssh k8s_master vscode连接远程主机，点击左下角，弹出命令中选择Connect to Host，选择上述定义的Host 后续打开文件或文件夹就都是远程主机上的，和操作本地文件是一样的 编译源码 在远程主机搭建好完整的golang开发环境，虽然vscode好像运行在Local Host，但实际上整个开发环境都是用的Remote Host上的。 对于大型的golang程序，编译慢是一个头疼的问题，这里针对优化编译速度给出一些建议： 启用go module，并设置一个速度快的GOPROXY，例如https://goproxy.cn 先执行go install，将编译结果缓存在全局缓存，后续go build时源码没有变化将直接使用全局缓存中内容 使用go build -p指定编译命令并行数 要调试golang程序，还需要加上“-N -l&quot;编译参数，上述go install/go build都要加上。 -N disable optimizations -l disable inlining 调试 vscode进行调试主要是设置launch.json文件，切换到debug视图，点击Add Configuration…，如果当前打开过go文件，会自动生成launch.json，否则需要选择go环境： launch.json的configurations字段是一个数组，可以添加多个配置，通过request和mode的组合用于不同的调试场景，包括以下组合： request mode program remotePath processId args launch debug、test（也可用auto） 源文件或包绝对路径 / / 命令行参数列表 launch exec（也可用auto） 编译好的二进制程序绝对路径 / / 同上 launch remote（废除，使用attach） / / / / attach local / / 进程ID / attach remote / 被调试源码文件绝对路径 / / 特别说明一下 当mode为auto时自动根据program判断是哪种情况 attach+remote这种情况，remotePath非常重要，例如设置在workspaceFolder/path/to/debuged_file的断点，dlv服务端是打在remotePath/path/to/debuged_file上的，一旦设置不对就会导致断点断不到，并会导致dlv服务出问题（见最后说明） 不是所有字段在每种场景都需要，/表示这种情况不需要 上述表格只列出了主要的一些字段，每种场景支持哪些字段，可以通过智能提示（默认快捷键Ctrl+Space，如果快捷键和系统冲突，需要设置其他快捷键）调出 vscode内置了一些代码片段可以满足大部分常用场景，也是通过智能提示调出。 Attach to local process 1234567&#123; "name": "Launch file", "type": "go", "request": "launch", "mode": "debug", "program": "$&#123;file&#125;"&#125; Connect to server 123456789&#123; "name": "Connect to server", "type": "go", "request": "attach", "mode": "remote", "remotePath": "$&#123;workspaceFolder&#125;", "port": 2345, "host": "127.0.0.1"&#125; Launch file 1234567&#123; "name": "Launch file", "type": "go", "request": "launch", "mode": "debug", "program": "$&#123;file&#125;"&#125; Launch package 1234567&#123; "name": "Launch Package", "type": "go", "request": "launch", "mode": "debug", "program": "$&#123;workspaceFolder&#125;"&#125; Launch test function 1234567891011&#123; "name": "Launch test function", "type": "go", "request": "launch", "mode": "test", "program": "$&#123;workspaceFolder&#125;", "args": [ "-test.run", "MyTestFunction" ]&#125; Launch test package 1234567&#123; "name": "Launch test package", "type": "go", "request": "launch", "mode": "test", "program": "$&#123;workspaceFolder&#125;"&#125; 如上所示，其中用到几个环境变量： file：当前文件绝对路径 workspaceFolder：工作区目录绝对路径 fileDirname：当前文件所在目录绝对路径 dlv存在的问题 [vagrant@localhost ~]$ dlv version Delve Debugger Version: 1.3.0 Build: $Id: 2f59bfc686d60989dcef9de40b480d0a34aa2fa5 $ dlv在使用中存在一些不是特别方便的地方： attach+local时，vscode退出调试后被调试进程也退出，无法实现deattach的效果 attach+remote时，vscode退出调试后dlv服务端也退出 attach+remote时，如果remotePath设置不对，dlv服务端就无法正常退出也无法再正常使用（可能是一个Bug)，只能kill -9杀掉，如果dlv不杀掉，被调试进程也无法退出，这时如果强行kill掉被调试进程，它就会变成僵尸进程，如果恰好被调试进程是systemd启动的服务，那么要消除僵尸进程就只有重启机器了 vscode server是运行在ssh所使用的用户下的，如果被调试的进程当前用户没有权限是无法使用attach+local模式的，这时只有用attach+remote模式，手工启动dlv attach，remotePath设为${workspaceFolder}]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>dlv</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang中如何通过包的importpath定位源码目录]]></title>
    <url>%2F2019%2F10%2F09%2F015-find-code-dir-from-package-importpath%2F</url>
    <content type="text"><![CDATA[golang标准库中的go/build包是用来收集包的一些信息的，例如获取包的源码位置。用来学习golang的交互式教程tour就是使用了go/build定位tour源代码位置，从而在运行时使用其中的文件。但是go/build是有一些使用限制的，主要是在某些情况下无法定位go module的源码位置，那么还有哪些方法可以根据包的importpath获取到包的源代码位置呢，本文详细进行探讨。 起因 golang 1.12.9，设置了GO111MODULE=on，也就是显式开启go module。 按照tour的使用方法，首先： 1go get golang.org/x/tour 然后到GOPATH/bin目录下运行tour程序，报如下错误： Couldn’t find tour files: could not find go-tour content; check $GOROOT and $GOPATH 而在设置GO111MODULE=on之前一切都是正常的。 通过阅读tour的源代码发现它运行时需要使用源代码目录下的文件，这是通过标准库go/build.Context.Import来定位源代码的路径的，具体代码如下，见local.go文件： 1234567891011121314151617181920const ( basePkg = "golang.org/x/tour" socketPath = "/socket")func findRoot() (string, error) &#123; ctx := build.Default p, err := ctx.Import(basePkg, "", build.FindOnly) if err == nil &amp;&amp; isRoot(p.Dir) &#123; return p.Dir, nil &#125; tourRoot := filepath.Join(runtime.GOROOT(), "misc", "tour") ctx.GOPATH = tourRoot p, err = ctx.Import(basePkg, "", build.FindOnly) if err == nil &amp;&amp; isRoot(tourRoot) &#123; gopath = tourRoot return tourRoot, nil &#125; return "", fmt.Errorf("could not find go-tour content; check $GOROOT and $GOPATH")&#125; p.Dir即为包对应的源代码路径，其中关键代码是调用了go/build.Context.Import： 1func (ctxt *Context) Import(path string, srcDir string, mode ImportMode) 分析go/build.Context.Import的源代码发现，当srcDir为空时总是无法获取go module包信息。在进一步分析原因之前我们先了解一下go module相关知识。 go module go module是golang 1.11引入的包管理机制，可以通过GO111MODULE环境变量控制是否启用，有三个值： off：关闭go module on：总是使用go module auto：默认值，如果当前工程包含go.mod文件则启用 go module，否则使用旧的GOPATH和vendor机制 当启用go module时，一个变化是下载的依赖包不再放在GOPATH/src目录下，而是放在GOPATH/pkg/mod（未来可能迁移到GOCACHE/mod）下，并且目录带有版本号，以tour为例，启用go module之前源码位置为： 1GOPATH/src/golang.org/x/tour 启用之后变为： 1GOPATH/pkg/mod/golang.org/x/tour@v0.0.0-20191002171047-6bb846ce41cd module和package module和package是集合和元素的关系，一个module可以包含0个、1个或多个package，module的Path和其中package的ImportPath前缀一定是相同的，例如golang.org/x/tools是一个module，其中的golang.org/x/tools/go/packages是一个package。 import一定是package，而不是module，当前module中执行go mod tidy命令可以根据源码中的import补齐go.mod中缺失的module以及删除多余的module。 回到问题的原因 go/build在最初没有引入go module之前，总是从GOROOT/src和GOPATH/src下寻找包的，这时是没有问题的，当启用go module后，包缓存变到了GOPATH/pkg/mod下，这时就找不到了，case 26504提出了这个问题并做了修正，但是出于效率的考虑，只有在特定的情况下才会针对go module进行处理，然后通过执行go list命令实现的。特定的情况是： the AllowBinary and IgnoreVendor flags are not set, Context describes the local file system (all the helper functions are nil), srcDir is outside GOPATH/src (or GO111MODULE=on), there’s a go.mod in srcDir or above, the release tags are unmodified from Default.Context, and path does not name a standard library package. 当srcDir为空的时候，是没有办法判断上述条件3和4的，就会直接返回失败，所以这时go/build仍然无法获得相应module的信息，tour在调用go/build.Context.Import时传入的srcDir正是为空。 详解go/build.Context.Import过程 1func (ctxt *Context) Import(path string, srcDir string, mode ImportMode) path：模块的Path或包的ImportPath srcDir：当前工程的源码路径，下面会讲在哪些情况会用到 mode：FindOnly，AllowBinary，ImportComment，IgnoreVendor Import的主要过程如下： 如果为本地包，也就是path为相对路径，相对的正是srcDir，这时先根据srcDir得到绝对路径，然后根据此绝对路径依次在GOROOT/src和GOPATH/src中寻找，绝对路径必须为其子目录，找到第一个符合即Import返回找到包，这种情况下srcDir不能为空，最后p.Dir的结果是Join(srcDir, path) 如果非本地包： 首先针对module尝试使用go list命令获取包信息，这时有一系列限制条件，具体见上文，其中需要判断srcDir不在GOROOT/src和GOPATH/src下且其路径上有go.mod文件，所以srcDir为空则直接寻找失败继续下一步，否则执行go list，命令的当前路径为srcDir，srcDir必须为module的路径，类似如下命令，如果成功则Import返回找到包，最后p.Dir的结果是go list返回的Dir 1go list -f &#123;&#123;.Dir&#125;&#125; path 然后尝试在vendor目录中寻找，这时srcDir也不能为空，使用srcDir依次在GOROOT/src和GOPATH/src中寻找，srcDir必须为其子目录，找到第一个符合即Import返回找到包，最后p.Dir的结果是Join(Abs(srcDir), “vendor”, path)，否则下一步 然后尝试在GOROOT/src中寻找，成功则Import返回找到包，p.Dir的结果为Join(GOROOT, “src”, path)，否则下一步 然后尝试依次在所有GOPATH/src中寻找，找到第一个符合即Import返回找到包，p.Dir的结果为Join(GOPATH, “src”, path) 以上过程都没有定位成功则Import返回失败 根据上述过程，srcDir在下述三种情况下起到重要作用，如果为空则会导致找不到包： 本地包，此时path是相对srcDir的 module，此时还需要srcDir不能在GOROOT/src或GOPATH/src中，且路径上包含go.mod文件，srcDir需要为此module的路径 vendor目录中的包，此时path在Join(srcDir, “vendor”)目录下 定位包源码位置的方法梳理 上述go/build在定位包源码位置时存在种种限制，那么是否还有其他的方法，答案是肯定的，共有如下方法： 它们之间是有依赖关系的，go/build对模块的支持需要调用go list；golang.org/x/tools/go/packages默认是通过go list来获取包或模块信息的，下面具体说明其使用方法。 go list 1go list [-f format] [-json] [-m] [list flags] [build flags] [packages] go list用于列出当前目录主包或模块及其依赖。当GO111MODULE=on且没有指定-m时，如果当前目录找不到，还会从GOPROXY指定地址下载，此时不能使用通配符。 packages：当指定-m时为模块的path列表，否则为包的importpath列表；为空时只列出主模块或主包，不为空时则列出匹配的，可以用…通配符；有一些特殊的path，对于模块可以使用all，表示列出当前主模块及所有依赖模块，对于包有all/std/cmd几个，all表示列出GOPATH中所有包，std列出标准库的包，cmd会列出go仓库中的命令及内部库；对于模块还可以使用模块查询，例如可以通过path@version带上版本；更多内容可参考go help packages和go help modules -f format：可以使用go模板定制输出内容 模块有如下字段 1234567891011121314151617type Module struct &#123; Path string // module path Version string // module version Versions []string // available module versions (with -versions) Replace *Module // replaced by this module Time *time.Time // time version was created Update *Module // available update, if any (with -u) Main bool // is this the main module? Indirect bool // is this module only an indirect dependency of main module? Dir string // directory holding files for this module, if any GoMod string // path to go.mod file for this module, if any Error *ModuleError // error loading module&#125;type ModuleError struct &#123; Err string // the error itself&#125; 包有如下字段 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758type Package struct &#123; Dir string // directory containing package sources ImportPath string // import path of package in dir ImportComment string // path in import comment on package statement Name string // package name Doc string // package documentation string Target string // install path Shlib string // the shared library that contains this package (only set when -linkshared) Goroot bool // is this package in the Go root? Standard bool // is this package part of the standard Go library? Stale bool // would 'go install' do anything for this package? StaleReason string // explanation for Stale==true Root string // Go root or Go path dir containing this package ConflictDir string // this directory shadows Dir in $GOPATH BinaryOnly bool // binary-only package: cannot be recompiled from sources ForTest string // package is only for use in named test Export string // file containing export data (when using -export) Module *Module // info about package's containing module, if any (can be nil) Match []string // command-line patterns matching this package DepOnly bool // package is only a dependency, not explicitly listed // Source files GoFiles []string // .go source files (excluding CgoFiles, TestGoFiles, XTestGoFiles) CgoFiles []string // .go source files that import "C" CompiledGoFiles []string // .go files presented to compiler (when using -compiled) IgnoredGoFiles []string // .go source files ignored due to build constraints CFiles []string // .c source files CXXFiles []string // .cc, .cxx and .cpp source files MFiles []string // .m source files HFiles []string // .h, .hh, .hpp and .hxx source files FFiles []string // .f, .F, .for and .f90 Fortran source files SFiles []string // .s source files SwigFiles []string // .swig files SwigCXXFiles []string // .swigcxx files SysoFiles []string // .syso object files to add to archive TestGoFiles []string // _test.go files in package XTestGoFiles []string // _test.go files outside package // Cgo directives CgoCFLAGS []string // cgo: flags for C compiler CgoCPPFLAGS []string // cgo: flags for C preprocessor CgoCXXFLAGS []string // cgo: flags for C++ compiler CgoFFLAGS []string // cgo: flags for Fortran compiler CgoLDFLAGS []string // cgo: flags for linker CgoPkgConfig []string // cgo: pkg-config names // Dependency information Imports []string // import paths used by this package ImportMap map[string]string // map from source import to ImportPath (identity entries omitted) Deps []string // all (recursively) imported dependencies TestImports []string // imports from TestGoFiles XTestImports []string // imports from XTestGoFiles // Error information Incomplete bool // this package or a dependency has an error Error *PackageError // error loading package DepsErrors []*PackageError // errors loading dependencies &#125; 下面通过一些例子来说明其使用，假设当前目录为tour源码根目录： 列出主模块目录 1go lis -m -f &#123;&#123;.Dir&#125;&#125; 列出主模块和所有依赖模块目录 12go list -m -f &#123;&#123;.Dir&#125;&#125; allgo list -m -f &#123;&#123;.Dir&#125;&#125; ... 上述两种效果是一样的。 列出符匹配条件的模块目录 1go list -m -f &#123;&#123;.Dir&#125;&#125; golang.org/x/... 列出主包目录 1go list -f &#123;&#123;.Dir&#125;&#125; 列出主包和所有依赖包目录 1go list -f &#123;&#123;.Dir&#125;&#125; ... 列出GOPATH所有包目录 1go list -f &#123;&#123;.Dir&#125;&#125; all 列出符匹配条件的包目录 1go list -f &#123;&#123;.Dir&#125;&#125; golang.org/x/... golang.org/x/tools/go/packages 对于本文一开始提到的tour的问题，可以修改findRoot方法，用packages.Load最后再尝试一次定位golang.org/x/tour的源码位置。 123456789101112131415func findRoot() (string, error) &#123; cfg := &amp;packages.Config&#123;Mode: packages.LoadFiles&#125; pkgs, _ := packages.Load(cfg, basePkg) for _, pkg := range pkgs &#123; for _, goFile := range pkg.GoFiles &#123; root := filepath.Dir(goFile) if isRoot(root) &#123; return root, nil &#125; &#125; &#125; return "", fmt.Errorf("could not find go-tour content; check $GOROOT and $GOPATH")&#125; packages.Load最后通过调用go list获得包信息，命令如下： 1go list -e -json -compiled -test=false -export=false -deps=false -find=true -- golang.org/x/tour 因为packages.Config中我们没有指定Dir（这个Dir会作为上述命令执行的目录），所以上述命令使用运行tour的当前目录，也就是WORKSPACE/bin作为当前目录，这时当前目录是找不到包的，但是因为开启了GO111MODULE=on，所以上述命令会尝试从GOPROXY下载golang.org/x/tour模块，如果此时无法联网，最终也找不到包。 packages.Load返回的包信息可使用字段如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// A Package describes a loaded Go package.type Package struct &#123; // ID is a unique identifier for a package, // in a syntax provided by the underlying build system. // // Because the syntax varies based on the build system, // clients should treat IDs as opaque and not attempt to // interpret them. ID string // Name is the package name as it appears in the package source code. Name string // PkgPath is the package path as used by the go/types package. PkgPath string // Errors contains any errors encountered querying the metadata // of the package, or while parsing or type-checking its files. Errors []Error // GoFiles lists the absolute file paths of the package's Go source files. GoFiles []string // CompiledGoFiles lists the absolute file paths of the package's source // files that were presented to the compiler. // This may differ from GoFiles if files are processed before compilation. CompiledGoFiles []string // OtherFiles lists the absolute file paths of the package's non-Go source files, // including assembly, C, C++, Fortran, Objective-C, SWIG, and so on. OtherFiles []string // ExportFile is the absolute path to a file containing type // information for the package as provided by the build system. ExportFile string // Imports maps import paths appearing in the package's Go source files // to corresponding loaded Packages. Imports map[string]*Package // Types provides type information for the package. // Modes LoadTypes and above set this field for packages matching the // patterns; type information for dependencies may be missing or incomplete. // Mode LoadAllSyntax sets this field for all packages, including dependencies. Types *types.Package // Fset provides position information for Types, TypesInfo, and Syntax. // It is set only when Types is set. Fset *token.FileSet // IllTyped indicates whether the package or any dependency contains errors. // It is set only when Types is set. IllTyped bool // Syntax is the package's syntax trees, for the files listed in CompiledGoFiles. // // Mode LoadSyntax sets this field for packages matching the patterns. // Mode LoadAllSyntax sets this field for all packages, including dependencies. Syntax []*ast.File // TypesInfo provides type information about the package's syntax trees. // It is set only when Syntax is set. TypesInfo *types.Info // TypesSizes provides the effective size function for types in TypesInfo. TypesSizes types.Sizes&#125; 其中并没有包的Dir，所以只有从GoFiles中获取Dir，如果GoFiles为空，这时就无法获取到包的Dir，这似乎是一个问题，建议golang.org/x/tools/go/packages能够从go list返回这个字段。 go/build go/build也是tour所采用的方法，和上述方法的不同之处在于，这个包一次只处理一个包，传入的path不能有通配符，必须是一个明确的path，而上述两种方法是可以批量的。也因此go/build比较注重效率，在case 26504中已经做了阐述。这个包的使用在传入srcDir参数是也有一些讲究，上文已经详细探讨。 总结 目前并没有完美的方法从包的importpath定位到源码目录，虽然golang.org/x/tools/go/packages或go list可以全面的支持包或模块的检索，但是在不知道源码位置的时候，必须启用GO111MODULE=on并联网，而且这时获取到的版本并不一定是需要的版本，而go/build在GO111MODULE=on时对模块中的包是无法获取源码位置的。 综上对于模块中的包目前暂且可用的方案也只有golang.org/x/tools/go/packages或go list。对于非模块中的包最好用go/build。 模块中的包，存在版本号的问题，无法像非模块包那样直接从GOROOT和GOPATH中定位即可，你无法用相同的方法在GOPATH/pkg/mod中定位，因为可能存在多个版本的包，这是难以仅根据包的importpath定位源码位置的问题根源。]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>go module</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用vagrant搭建分布式开发环境]]></title>
    <url>%2F2019%2F09%2F21%2F014-setup-dev-env-with-vagrant%2F</url>
    <content type="text"><![CDATA[基本需求 VM能够上网：用于VM安装软件 Host能够访问VM，且VM有固定IP：用于Host远程调试VM VM之间能够互相访问：用于构成分布式环境 VM间文件共享 VirtualBox的网络模式 VirtualBox支持四种网络模式，常用的是NAT和Host-only两种，下图对四种网络模式进行了对比： NAT Bridged Internal Host-only(默认) Host-only(共享) Host-only(桥接) VM-&gt;Host y y n y(到Host-only Adapter) y(虚拟机配置Host-only Adapter为网关) y(无线网卡不行) Host-&gt;VM n y n y(从Host-only Adapter) y(从Host-only Adapter) y VM-&gt;Other Host y y n n y y Other Host-&gt;VM n y n n n y VM&lt;-&gt;VM n y 同网络名下可以 y y y NAT NAT相关有NAT和NAT 网络两个选项，其区别是前者使用内置的NAT网络，网段为10.0.2.0/24，用户无法修改；后者可以用户定制，通过全局设定中进行配置，如下图。 Host-only Host-only比较复杂，通过配置可以实现各种不同效果，有三种情况： 默认 共享 这种情况在默认的基础上，为VM增加了连通外网效果。 桥接 这种情况和VirtualBox的桥接网络模式实现了一样的效果。这时HOST的网卡不能为无线网卡，在启用802.1x认证的网络也不行。 Vagrant的网络模式 Vagrant有三种网络模式，并不会覆盖具体虚拟机provider的所有网络模式。 网络模式 示例 对应VirtualBox网络模式 Forwarded port config.vm.network “forwarded_port”, guest: 80, host: 8080, protocol: “udp” NAT Private network config.vm.network “private_network”, ip: “192.168.50.4” Host-only Public network config.vm.network “public_network”, :bridge =&gt; ‘en1: Wi-Fi (AirPort)’ Bridged 如果只配置一个Private network，则vagrant会自动创建一个NAT和一个Host-only网络。 如果配置多余一个Private network，则vagrant就不会自动创建NAT，而是对应的多个Host-only网络。 虚拟机间文件共享 最简单的方式是将Host的某个文件夹映射到所有虚拟机中，通过Vagrant很容易做到这点： 12config.vm.synced_folder &quot;./share&quot;, &quot;/share&quot;, create:true, mount_options:[&quot;dmode=775&quot;,&quot;fmode=664&quot;]config.vm.synced_folder &quot;.&quot;,&quot;/vagrant&quot;,disabled:true 默认情况下，vagrant总是将Vagrantfile所在目录文件通过rsync同步到虚拟机的/vagrant目录，这个同步是单向的，上述配置禁用了这一点。然后配置了Vagrantfile所在目录下的share目录映射到虚拟机的/share目录，这是mount进去的，因此是双向的。 要映射文件夹，还需要在虚拟机中安装VirtualBox Guest Additions，以centos7为例，安装方法如下： 安装内核开发文件： 1sudo yum install &quot;kernel-devel-uname-r == $(uname -r)&quot; 如果找不到对应内核开发文件，则更新内核： 12sudo yum update kernel -yinit 6 安装编译工具： 1sudo yum install kernel-devel gcc make -y 下载VirtualBox Guest Additions： 1curl -L -O http://download.virtualbox.org/virtualbox/6.0.10/VBoxGuestAdditions_6.0.10.iso 安装VirtualBox Guest Additions： 1234sudo mkdir -p /mnt/isosudo mount -o loop VBoxGuestAdditions_6.0.10.iso /mnt/isocd /mnt/isosudo ./VBoxGuestAdditions.run 开发环境搭建 为了实现最开始的基本需求，我们只需要选择private_network即可，这时vagrant会为我们创建一个NAT网络和一个Host-only网络，前者可用于上网，后者用于虚拟机之间以及虚拟机和Host的互连，具体Vagrantfile文件如下： 12345678910111213141516171819202122# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(&quot;2&quot;) do |config| config.vm.synced_folder &quot;./share&quot;, &quot;/share&quot;, create:true, mount_options:[&quot;dmode=775&quot;,&quot;fmode=664&quot;] config.vm.synced_folder &quot;.&quot;,&quot;/vagrant&quot;,disabled:true config.vm.define &quot;master&quot; do |master| master.vm.box = &quot;centos/7&quot; master.vm.network &quot;private_network&quot;, ip: &quot;10.10.10.14&quot;, netmask: &quot;255.255.0.0&quot; end config.vm.define &quot;node1&quot; do |node1| node1.vm.box = &quot;centos/7&quot; node1.vm.network &quot;private_network&quot;, ip: &quot;10.10.10.15&quot;, netmask: &quot;255.255.0.0&quot; end config.vm.define &quot;node2&quot; do |node2| node2.vm.box = &quot;centos/7&quot; node2.vm.network &quot;private_network&quot;, ip: &quot;10.10.10.16&quot;, netmask: &quot;255.255.0.0&quot; end end 虚拟机选用的centos7 使用了multi-machine特性，在一个Vagrantfile中定义多个虚拟机，如上定义了3个虚拟机 在VirtualBox中创建Host-only Adapter，其网段为10.10.0.0/16，这样以上虚拟机会自动选择这个Host-only Adapter 如果虚拟机的ip+netmask和Host-only Adapter不是一个网段，则vagrant会自动创建一个Host-only Adapter，要避免这种情况，以便所有虚拟机使用的一个Host-only Adapter 使用方法： 启动所有虚拟机 1vagrant up 关闭所有虚拟机 1vagrant halt 启动某个虚拟机 1vagrant up master 关闭某个虚拟机 1vagrant halt master 登陆某个虚拟机 1vagrant ssh master]]></content>
      <categories>
        <category>devtool</category>
      </categories>
      <tags>
        <tag>vagrant</tag>
        <tag>virtualbox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VirtualBox使用Host-Only报"Failed to attach the network LNU"错误]]></title>
    <url>%2F2019%2F09%2F18%2F013-virtualbox-host-only-problem%2F</url>
    <content type="text"><![CDATA[Windows 10，VirtualBox 6.0.10，原本使用正常。 安装Wireshark（包括Npcap）后，使用Host-Only时报&quot;Failed to attach the network LNU&quot;错误。 原因 NDIS6 driver问题导致。 可能是安装了Npcap后导致。 解决方法 方法一：安装virtualbox时增加参数：-msiparams NETWORKTYPE=NDIS5 方法二：确保Host-Only网卡安装了”VirtualBox NDIS6 Bridged Networking Driver&quot;，然后将网卡禁用再启用一下]]></content>
      <categories>
        <category>devtool</category>
      </categories>
      <tags>
        <tag>vagrant</tag>
        <tag>virtualbox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下使用OpenSSH搭建ssh服务端]]></title>
    <url>%2F2019%2F08%2F09%2F012-using-openssh-to-build-ssh-server-upder-windows%2F</url>
    <content type="text"><![CDATA[产品中需要用到sftp，sftp实际上就是ssh的一部分，ssh在Linux系统中是自带的，运行sshd之后就同时也支持sftp了，也是通过22端口连接。但我们的产品需要支持跨平台，需要同时支持Linux和Windows，因此需要寻找一个跨平台的ssh实现，通过寻找发现实际上在Win10 1809和Windows Server 2019开始已经内置了，使用的正是OpenSSH。 OpenSSH OpenSSH是ssh协议的开源实现，是OpenBSD的子项目。 OpenSSH套件包含以下工具： 远程操作使用ssh、scp和sftp完成。 使用ssh-add、ssh-keysign、ssh-keyscan和ssh-keygen进行密钥管理。 服务端由sshd、sftp服务器和ssh代理组成。 源码：https://github.com/openssh/openssh-portable Windows安装 Windows官方将OpenSSH移植到Windows下，源码：https://github.com/PowerShell/openssh-portable，可以从上述源码构建安装，如果你使用的Win10 1809或Windows Server 2019，也可以通过Windows可选程序安装，以Window 10 1903为例： 在“应用和功能”中点击&quot;可选功能&quot; 点击&quot;添加功能&quot; 点击”OpenSSH 服务器“，然后点击“安装”，稍等片刻后就安装成功了 安装后程序位于：C:\Windows\System32\OpenSSH Win10 1903使用的7.7.2.1版本。 启动服务，命令行执行如下命令 1net start sshd OpenSSH配置 配置文件 %PROGRAMDATA%\ssh\sshd_config 默认是密码认证和公钥认证都开启的，也可以修改上述文件进行控制： 12PubkeyAuthentication yesPasswordAuthentication no 密码登录 使用Windows账户密码登陆。 1ssh user@host 公钥登陆 客户端 创建密钥和公钥 1ssh-keygen -t rsa -f myssh 运行上述命令后在%USERPROFILE%\.ssh下生成myssh和myssh.pub文件，前者是密钥，后者为公钥。 向ssh代理注册私钥 12net start ssh-agentssh-add myssh 服务端 创建%USERPROFILE%\.ssh\authorized_keys文件，将上述客户机myssh.pub的内容拷贝到其中，检查authorized_keys文件权修改，确保只有System, Administrators and owner可以访问 1icacls %USERPROFILE%\.ssh\authorized_keys 如果登陆用户属于administrators组，还需要创建%PROGRAMDATA%\ssh\administrators_authorized_keys文件，同样拷贝客户机myssh.pub的内容拷贝到其中，这个文件只能SYSTEM和Administrators组访问，可以执行： 123icacls administrators_authorized_keys /inheritance:ricacls administrators_authorized_keys /grant SYSTEM:`(F`)icacls administrators_authorized_keys /grant BUILTIN\Administrators:`(F`) 重启sshd服务 12net stop sshdnet start sshd 客户端登陆 1ssh -i %USERPROFILE%\.ssh\myssh user@host 使用putty访问 putty是常用的ssh客户端工具，功能更加丰富，使用密钥登陆需要生成putty的密钥格式。 运行puttygen.exe，点击Load，载入上述客户机密钥文件%USERPROFILE%\.ssh\myssh，然后点击Save private key，假设保存为C:\myssh.ppk。 然后在putty中就可以用上述ppk文件进行登陆认证。]]></content>
      <categories>
        <category>devtool</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>OpenSSH</tag>
        <tag>putty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决k8s集群外无法访问NodePort问题]]></title>
    <url>%2F2019%2F08%2F06%2F011-nodeport-cannot-be-accessed-outside-the-k8s-cluster%2F</url>
    <content type="text"><![CDATA[centos7.6从yum源二进制安装了k8s集群，建立NodePort类型service后无法从集群外访问service，本文记录了问题解决过程。 环境 centos7.6 k8s v1.5.2 dashbaord 1.6.0 master：10.10.10.14 node1：10.10.10.15 node2：10.10.10.16 集群外机器：10.10.10.1 问题分析 [vagrant@localhost ~]$ kubectl get pod -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE kubernetes-dashboard-1801235744-v6r9t 1/1 Running 2 3d 172.16.91.2 node2 [vagrant@localhost ~]$ kubectl get svc -n kube-system NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard 10.254.106.243 80:30389/TCP 3d 我遇到的问题是通过https://10.10.10.16:30389从集群外机器(10.10.10.1)无法访问dashboard，其他service情况是一样的，在集群内部或者通过clusterip是可以访问的，排除集群本生问题。k8s的service是通过iptables实现的，初步怀疑和iptables有关。 问题定位 这里通过增加trace日志的方法来跟踪数据包在iptables中哪一步处理出现问题的。在node2上执行如下命令，对tcp协议并且目的端口为30389的数据包从PREROUTING链增加TRACE日志，日志会输出到/var/log/messages。 [vagrant@localhost ~]$ sudo iptables -t raw -A PREROUTING -p tcp --dport 30389 -j TRACE 详细分析日志信息，发现到FORWARD链的filter表就没有后续包了。 [vagrant@localhost ~]$sudo cat /var/log/messages Sep 10 07:24:50 localhost kernel: TRACE: raw:PREROUTING:policy:2 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) Sep 10 07:24:50 localhost kernel: TRACE: nat:PREROUTING:rule:1 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) Sep 10 07:24:50 localhost kernel: TRACE: nat:KUBE-SERVICES:rule:4 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) Sep 10 07:24:50 localhost kernel: TRACE: nat:KUBE-NODEPORTS:rule:3 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) Sep 10 07:24:50 localhost kernel: TRACE: nat:KUBE-MARK-MASQ:rule:1 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) Sep 10 07:24:50 localhost kernel: TRACE: nat:KUBE-MARK-MASQ:return:2 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: nat:KUBE-NODEPORTS:rule:4 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: nat:KUBE-SVC-XGLOHA7QRQ3V22RZ:rule:1 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: nat:KUBE-SEP-7MDAIXCESNWFCQ4R:rule:2 IN=eth1 OUT= MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=10.10.10.16 LEN=52 TOS=0x00 PREC=0x00 TTL=128 ID=43914 DF PROTO=TCP SPT=58026 DPT=30389 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: filter:FORWARD:rule:1 IN=eth1 OUT=docker0 MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=172.16.91.2 LEN=52 TOS=0x00 PREC=0x00 TTL=127 ID=43914 DF PROTO=TCP SPT=58026 DPT=9090 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: filter:DOCKER-ISOLATION:return:1 IN=eth1 OUT=docker0 MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=172.16.91.2 LEN=52 TOS=0x00 PREC=0x00 TTL=127 ID=43914 DF PROTO=TCP SPT=58026 DPT=9090 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: filter:FORWARD:rule:2 IN=eth1 OUT=docker0 MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=172.16.91.2 LEN=52 TOS=0x00 PREC=0x00 TTL=127 ID=43914 DF PROTO=TCP SPT=58026 DPT=9090 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: filter:DOCKER:return:1 IN=eth1 OUT=docker0 MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=172.16.91.2 LEN=52 TOS=0x00 PREC=0x00 TTL=127 ID=43914 DF PROTO=TCP SPT=58026 DPT=9090 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x4000 Sep 10 07:24:50 localhost kernel: TRACE: filter:FORWARD:policy:6 IN=eth1 OUT=docker0 MAC=08:00:27:43:31:95:0a:00:27:00:00:0c:08:00 SRC=10.10.10.1 DST=172.16.91.2 LEN=52 TOS=0x00 PREC=0x00 TTL=127 ID=43914 DF PROTO=TCP SPT=58026 DPT=9090 SEQ=73790228 ACK=0 WINDOW=64240 RES=0x00 SYN URGP=0 OPT (020405B40103030801010402) MARK=0x400 查看FORWARD链发现其policy是DROP，问题就出在这里。 [vagrant@localhost ~]$ sudo iptables -nL FORWARD Chain FORWARD (policy DROP) target prot opt source destination DOCKER-ISOLATION all – 0.0.0.0/0 0.0.0.0/0 DOCKER all – 0.0.0.0/0 0.0.0.0/0 ACCEPT all – 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED ACCEPT all – 0.0.0.0/0 0.0.0.0/0 ACCEPT all – 0.0.0.0/0 0.0.0.0/0 解决方法 将FORWARD链改为ACCEPT的。 [vagrant@localhost ~]$ sudo iptables -P FORWARD ACCEPT]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++日志框架基准测试及源码分析]]></title>
    <url>%2F2019%2F07%2F04%2F010-cpp-logging-framework-benchmark%2F</url>
    <content type="text"><![CDATA[C++日志框架有非常多，如何进行选择，其中性能是一个非常重要的考量因素，本文对几种有代表性的日志框架进行基准测试，并深入到源码和实现原理进行分析对比，便于大家进行选择。 测试对象 log4cplus 2.0.2 spdlog 1.3.1 boost.log 1.70 log4cplus属于仿log4j一类，也是起步较早的日志框架，早在2010年，出于log4j的名气，我在项目中就采用了log4cplus；spdlog属于比较现代化的日志框架，专注于性能；boost.log是1.54进入boost库的，当年选择log4cplus时还在评审阶段，boost可以说是准C++标准库，能够进入boost库自然质量不低，但是其设计相对也比较复杂，boost.log使用起来着实不太方便，它不太像直接面向用户使用，而更像是作为底层库供用户封装为更上层库。 日志框架对比 三种日志库在一些特性和实现方式上存在比较大的差别，为更好理解他们，先做一个简单对比： log4cplus spdlog boost.log 线程安全 默认线程安全 分mt和st 分mt和st 异步支持 从1.1.0开始增加asyncappender，可配置队列深度，线程数不可配，总为1，深度满后阻塞调用。每个日志对象所有。log4cplus并没有提供显式flush的接口，只有shutdown日志对象或close appender间接flush。（log4cplus的同步appender构造函数都有一个immediateFlush参数，表示C++标准输出流或文件流的flush） 可设置队列深度和线程数，深度满后有阻塞和丢弃老的两种策略；使用全局或每个日志独有都可以。 队列有限制深度/不限制深度以及fifo/ordering组合起来四种，限制深度的队列满后有阻塞和丢弃新的两种策略；线程数总为1个；sink frontend所有。 日志对象和sink的关系 sink在log4cplus中称为appender，每个日志对象显式指定appender。 每个日志对象显式指定sink。 无法显式指定日志对象(boost.log称为log source)对应到哪个sink，换言之每个日志对象对应所有sink，要实现某个日志对象只输出到某个sink，需要通过在sink上设置filter来实现。具体可参考：use-channel-hiearchy-of-boost-log-for-severity-and-sink-filtering，connecting a logger to a specific sink 日志继承 支持，类似log4j，日志继承有一个不好的地方，日志会自动写到父日志，而root日志是所有日志的祖先，如果配置了root日志，总会写到其中，这样会导致写入效率劣化严重 不支持 不支持 配置文件 支持，类似log4j 不支持 支持 自定义日志级别 支持 不支持 支持 boost.log boost.log相对复杂，单独做一个简单介绍。boost.log使用了很多新的概念，设计上非常模块化，可以灵活组装和扩展。对于初次使用boost.log，上手不会特别顺畅，需要理解清楚以下几个核心概念： Log source：就是其他框架中的日志对象，使用者直接面对的，其特别之处是日志级别也是完全自定义的； Log sink frontend：异步和同步在这一层实现，sink backend作为参数传给它，创建之后add到core中； Log sink backend：代表日志输出的载体，如控制台、文件等，这是我们比较熟悉的概念，把sink分为frontend和backend，我想是为了各层都可以独立扩展，非常灵活； Logging core：它是单例的，建立source和sink之间的连接，就像一个hub一样，还可以设置一些全局fiter和属性。 以下列出目前boost.log支持的所有source和sink： 1234567891011121314151617181920212223242526272829303132333435363738394041namespace logging = boost::log;namespace sinks = boost::log::sinks;namespace src = boost::log::sources;namespace expr = boost::log::expressions;namespace attrs = boost::log::attributes;namespace keywords = boost::log::keywords;// Log source，mt表示线程安全版本，不带mt表示非线程安全版本src::loggersrc::logger_mtsrc::severity_loggersrc::severity_logger_mtsrc::channel_loggersrc::channel_logger_mtsrc::severity_channel_loggersrc::severity_channel_logger_mt// Log sink frontendsinks::unlocked_sinksinks::synchronous_sinksinks::asynchronous_sink// 对于异步sink frontend有如下队列//record queueing strategysinks::unbounded_fifo_queuesinks::unbounded_ordering_queuesinks::bounded_fifo_queuesinks::bounded_ordering_queue//overflow strategies:sinks::drop_on_overflowsinks::block_on_overflow// Log sink backendsinks::basic_text_ostream_backendsinks::text_file_backendsinks::text_multifile_backendsinks::text_ipc_message_queue_backendsinks::syslog_backendsinks::basic_debug_output_backendsinks::basic_simple_event_log_backendsinks::basic_event_log_backend 测试工具&amp;代码 benchmark 1.5.0 对benchmark的使用遇到一点小障碍，因为我希望将flush的时间也统计在内，但是benchmark的默认计时方式是无法在最后一个迭代，加入对flush的调用，因为最后一个迭代会自动停止计时。 12345678910111213141516171819for (auto _ : state) &#123; function_to_be_measured();&#125;// 等价于for (auto it = state.begin(), e = state.end(); it != e; ++it) &#123; function_to_be_measured();&#125;// 我希望这样，但是没有成功，因为!=操作符中，如果到达迭代最后，会自动停止计时for (auto it = state.begin(), e = state.end();;) &#123; if (!(it != e)) &#123; log.flush(); break; &#125; log.info(...); ++it;&#125; 因此，最终只有选择UseManualTime计时方式。 测试代码：&lt;https://github.com/zhongpan/cpp-logging-benchmark 时间和速率统计包含flush时间，也就是保证日志全部输出到控制台或文件； 异步写入都是采用1个线程，队列深度都设置为8192，队列满后都采用阻塞策略； 写入消息长度最小选择32，避免string的小字符串性能优化，能够代表更通常的场景； 测试结果 测试环境： Intel® Core™ i5-7200U CPU @ 2.50GHz Micron 1100 SATA 256G 测试结果： a_c_32 a_c_512 s_c_32 s_c_512 log4cplus 298.557/s 47.8782/s 313.306/s 50.7682/s spdlog 14.4967k/s 8.49778k/s 15.5747k/s 6.09162k/s boost.log 6.55519k/s 3.86561k/s 4.98684k/s 3.22393k/s a_f_32 a_f_512 s_f_32 s_f_512 log4cplus 15.8821k/s 5.48616k/s 11.2635k/s 4.91378k/s spdlog 180.147k/s 109.091k/s 229.206k/s 135.489k/s boost.log 29.3718k/s 25.9405k/s 15.7826k/s 14.6118k/s 说明： 32/512表示消息字节数； a/s表示异步或同步； c/f表示控制台或文件； 结果分析 所有的日志框架都表现出文件的写入效率远高于控制台，因此尽量使用文件方式写日志； 同步和异步两种方式，如果把flush的时间统计在内，则效率区别不大，但是异步方式不会阻塞应用，如果考虑对应用的效率影响，异步方式显著优于同步，但是可能存在丟日志的情况，在应用退出时必须主动flush； 当消息长度增加时，写入效率都有所下降，spdlog当消息大于500字节时下降显著（见下面的源码分析），boost.log下降不明显； 各种情况写入效率都是：spdlog&gt;boost.log&gt;log4cplus，并且log4cplus的控制台写入效率非常非常低，spdlog的效率显著高于其他日志框架，特别是文件方式写入效率，让人难以置信👍，boost.log在复杂的设计下仍然保持良好的性能，也非常厉害。 spdlog性能优化源码分析 std::string_view的使用避免字符串复制时内存分配 123456789101112131415// spdlog/details/logger_impl.hinline void spdlog::logger::log(source_loc source, level::level_enum lvl, const char *msg)&#123; if (!should_log(lvl)) &#123; return; &#125; try &#123; details::log_msg log_msg(source, &amp;name_, lvl, spdlog::string_view_t(msg)); sink_it_(log_msg); &#125; SPDLOG_CATCH_AND_HANDLE&#125; 12345678// spdlog/common.h// string_view type - either std::string_view or fmt::string_view (pre c++17)#if defined(FMT_USE_STD_STRING_VIEW)using string_view_t = std::string_view;#elseusing string_view_t = fmt::string_view;#endif 实现fmt::memory_buffer进行字符串格式化，500长度以内使用内部数组，避免堆内存分配，并且比sstream更快 1234567// spdlog/sinks/basic_file_sink.h void sink_it_(const details::log_msg &amp;msg) override &#123; fmt::memory_buffer formatted; sink::formatter_-&gt;format(msg, formatted); file_helper_.write(formatted); &#125; 1234567891011// spdlog/fmt/bundled/format.h// The number of characters to store in the basic_memory_buffer object itself// to avoid dynamic memory allocation.enum &#123; inline_buffer_size = 500 &#125;;template &lt;typename T, std::size_t SIZE = inline_buffer_size, typename Allocator = std::allocator&lt;T&gt; &gt;class basic_memory_buffer: private Allocator, public internal::basic_buffer&lt;T&gt; &#123; private: T store_[SIZE]; 缓存时间字符串，避免重复格式化 1234567891011121314151617181920212223242526272829303132333435363738// spdlog/details/pattern_formatter.h void format(const details::log_msg &amp;msg, const std::tm &amp;tm_time, fmt::memory_buffer &amp;dest) override &#123; using std::chrono::duration_cast; using std::chrono::milliseconds; using std::chrono::seconds;#ifndef SPDLOG_NO_DATETIME // cache the date/time part for the next second. auto duration = msg.time.time_since_epoch(); auto secs = duration_cast&lt;seconds&gt;(duration); if (cache_timestamp_ != secs || cached_datetime_.size() == 0) &#123; cached_datetime_.clear(); cached_datetime_.push_back('['); fmt_helper::append_int(tm_time.tm_year + 1900, cached_datetime_); cached_datetime_.push_back('-'); fmt_helper::pad2(tm_time.tm_mon + 1, cached_datetime_); cached_datetime_.push_back('-'); fmt_helper::pad2(tm_time.tm_mday, cached_datetime_); cached_datetime_.push_back(' '); fmt_helper::pad2(tm_time.tm_hour, cached_datetime_); cached_datetime_.push_back(':'); fmt_helper::pad2(tm_time.tm_min, cached_datetime_); cached_datetime_.push_back(':'); fmt_helper::pad2(tm_time.tm_sec, cached_datetime_); cached_datetime_.push_back('.'); cache_timestamp_ = secs; &#125; fmt_helper::append_buf(cached_datetime_, dest); 控制台输出使用CRT接口，相对于ostream更快 1234567891011121314// spdlog/details/console_globals.hstruct console_stdout&#123; static std::FILE *stream() &#123; return stdout; &#125;#ifdef _WIN32 static HANDLE handle() &#123; return ::GetStdHandle(STD_OUTPUT_HANDLE); &#125;#endif&#125;; 123456// corecrt_wstdio.h in windows sdk 10.0.17763.0_ACRTIMP_ALT FILE* __cdecl __acrt_iob_func(unsigned _Ix);#define stdin (__acrt_iob_func(0))#define stdout (__acrt_iob_func(1))#define stderr (__acrt_iob_func(2)) 文件写入使用的CRT接口，相对于ofstream快 12345678910// sdplog/details/file_helper.h void write(const fmt::memory_buffer &amp;buf) &#123; size_t msg_size = buf.size(); auto data = buf.data(); if (std::fwrite(data, 1, msg_size, fd_) != msg_size) &#123; throw spdlog_ex("Failed writing to file " + os::filename_to_str(_filename), errno); &#125; &#125; 总结 三种日志框架各有所长，我觉得日志框架最重要的还是性能和接口易用性，为了调试bug，必须打必要的日志，但是开了日志，又影响到程序性能，甚至本来必现的问题，因为效率变差的原因都不出来了，这就尴尬了。经过以上分析对比，spdlog在性能和接口易用性上完胜其他日志框架，是日志框架的首选，其次boost.log也是一个不错选择，但是其接口用起来不是特别顺畅，对于log4cplus还是不推荐大家使用了，已经用了的尽快切换吧，当然spdlog使用了大量C++新特性，你必须使用C++11以上的新标准，但这似乎不是什么问题。]]></content>
      <categories>
        <category>loging</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择最佳的Windows远程桌面替代工具]]></title>
    <url>%2F2019%2F07%2F01%2F009-windows-remote-control-tool%2F</url>
    <content type="text"><![CDATA[开发调试过程中少不了使用远程桌面访问其他机器，最近Windows的远程桌面无法使用，只得寻找其他替代方案，经过一番搜索试用之后，有几款软件基本能够达到和Windows远程桌面类似效果，选择主要考量如下因素： 免费解决方案，商业的首先排除，如Teamviewer，AnyDesk，Radmin等； 清晰度和速度基本接近Windows的RDP，这里不得不说Windows下的RDP确实是最好的，没有之一； UltraVNC 说到远程访问，首先一定会想到VNC，VNC在Windows访问Linux中用的比较多，当然也可以Windows访问Windows，VNC的实现有几个主要的软件，包括：RealVNC、TightVNC（还有一个源于它的TigerVNC）、UltraVNC，它们都基于最初的AT&amp;T开源的VNC，协议上彼此兼容，各有所长，也有相互吸收，其中功能性上UltraVNC最为强大，支持文件传输等功能。 首先试用了UltraVNC，但是结果让人大跌眼镜，速度慢的一塌糊涂。转而找到了TigerVNC，号称高性能，但是速度和UltraVNC差不多。经过一番搜索之后，原来UltraVNC需要配合Video Mirror Driver一起使用才能有更好的速度，安装方法如下： 下载Mirror Driver SDK，根据操作系统不同选择不同的驱动，例如Windows 7 64位可选择driver\vista64目录下的 执行setupdrv.exe install或相应批处理脚本进行安装，安装成功后会多出显示设备 UltraVNC服务端设置，勾选Mirror Driver。注意UltraVNC必须和Mirror Driver的位数一致，如果是64位操作系统，就必须都是64位。 点击check the Mirror Driver，会显示： 如果有客户端连接上，再点击check the Mirror Driver，会显示： 启用Mirror Driver后，确实速度流畅了许多，基本满足使用，但是相比RDP，在流畅度上还是差一些。 TrueRemote 无意中发现了这款软件，非常轻量，软件压缩包只有151K，但是流畅度上让人映像非常深刻，并且支持文件传输，完全满足基本的使用需求。但是这款软件最后更新是2012年，作者后续已经没有维护了。这款软件是只支持Windows的，官方主页：http://blog.x-row.net/?p=47，使用方法非常简单： 服务端 Control选择Off就是只读控制，Service选择Entry后会安装成Windows服务。 客户端 注意Password不会自动保存，可以修改tureremote.ini文件保存。Scroll类似UltraVNC的Scale to window，选Off表示不缩放，就会出现滚动条，但是这时是最清晰的，否则就会缩放到合适大小，但是会出现稍许模糊。Codec貌似只有TrueRemote和Toshiba YUV411两个可用，效果差不多。 连接上之后效果如上，标题栏菜单中加入了一些控制菜单，发送文件可以从这里操作。 NoMachine NoMachine分for everybody和enterprise两个版本，for everybody版本是免费的。NoMachine实现了自有的nx协议，速度很快，流畅程度接近RDP。 安装后需要重启，服务端已经自动安装为Windows服务，默认使用4000端口。服务端参数设置如下： 打开客户端如下： 建立连接： 协议支持NX和SSH两种，认证方式支持如下三种： 登陆之后效果如下，我发一个细节，屏幕会随着鼠标移动自动滚动，非常人性化，给一个大大的赞👍: 通过点击右上角或者快捷键Ctrl+Alt+0可以调出设置界面，功能非常丰富完善： 例如关于显示设置如下： 关于设备设置如下： 总结 流畅度 功能 轻量 跨平台 UltraVNC ⭐️⭐️⭐️需要启用mirror driver ⭐️⭐️⭐️⭐️ ⭐️⭐️⭐️ ❌仅支持Windows TrueRemote ⭐️⭐️⭐️ ⭐️⭐️⭐️不支持全屏，无法发送Ctrl+Alt+Delete ⭐️⭐️⭐️⭐️ ❌仅支持Windows NoMachine ⭐️⭐️⭐️ ⭐️⭐️⭐️⭐️ ⭐️⭐️⭐️ ✔️还支持mobile 通过以上对比，首选推荐NoMachine，各方面都比较出色，特别是多平台支持。其次是TrueRemote，基本满足使用，其他特点是小巧轻量。UltraVNC使用过程中出现过客户端挂死情况，不是特别稳定，其用户界面也不太简洁和友好。]]></content>
      <categories>
        <category>devtool</category>
      </categories>
      <tags>
        <tag>remote control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMake使用Ninja加速C++代码构建过程]]></title>
    <url>%2F2019%2F06%2F26%2F008-cmake-with-ninja%2F</url>
    <content type="text"><![CDATA[Ninja的小巧、快速、稳定确实惊艳到我，在使用Ninja之前，我们一直使用Incredi Build加速我们的产品构建，IB确实是一款非常优秀的商业软件，直到最近在使用IB编译时，经常性导致服务器死机，我不得不寻求其他加速构建过程的工具，这时我找到了Ninja，我们的代码是采用CMake构建的，而CMake刚好支持ninja的generator，所以引入Ninja没有花费太多时间，让人敬佩的是，此后就再也没有死机过了，真的是再有没有👍。 CMake如何使用Ninja 如下以Windows为例： 下载ninja.exe，放到某个目录，假设D:\tool，并将D:\tool加入PATH环境变量； 假设源代码目录为D:\code，其中包含CMakeLists.txt文件，那么构建过程如下： 12345cd /d D:\codemkdir buildcd buildcmake .. -G "Ninja" -DCMAKE_MAKE_PROGRAM=D:\tool\ninja.exeninja Ninja和MSVC的关系 在Windows上一般使用的编译器是MSVC，Ninja和MSVC的关系就像make和gcc的关系，当然Ninjia也可以用在Linux下和gcc配合。所以Ninja只是负责调用编译器干活，本生并不负责编译，需要弄清楚。 如何使用预编译头文件 CMake并没有内置预编译头文件的支持，需要通过target_compile_options、set_source_files_properties这些已有的命令设置预编译参数，从github上可以找到一些封装，可以开箱即用，例如：cmake-precompiled-header、CMakePCHCompiler，其中CMakePCHCompiler的目标就是成为CMake的标准模块。 但是这些封装中都没有考虑到使用ninjia在预编译时存在一个问题，预编译的过程是首先需要根据预编译头文件生成pch文件(类似obj文件)，然后再编译其他cpp文件，也就是其他cpp文件需要依赖这个pch文件，他们有编译的先后顺序，如果使用MSVS编译，它会保证这个顺序，但是换成ninja后，这个依赖关系并不会自动生成到build.ninja等文件中，这样编译的过程就会出现pch还没生成就开始编译其他cpp文件，自然就编译不过。 这个问题我研究了很长时间，才发现在CMakeLists.txt中增加一些编译参数设置才行： 在预编译头文件对应的cpp文件上，主要关注OBJECT_OUTPUTS属性1set_source_files_properties($&#123;header_src&#125; PROPERTIES COMPILE_FLAGS "/Yc$&#123;header_path&#125; /FI$&#123;header_path&#125; /Fp$&#123;win_pch&#125;" OBJECT_OUTPUTS $&#123;win_pch&#125;) 在其他每个cpp文件上，主要关注OBJECT_DEPENDS属性1set_source_files_properties($&#123;src&#125; PROPERTIES OBJECT_DEPENDS $&#123;win_pch&#125;) 这样生成的ninjia构建文件才会生成正确的依赖关系。 Ninja和MSVC共享中间文件 Ninja加速编译过程是非常好的选择，但是开发调试还是需要在IDE下，Windows下就是MSVS，所以还是希望CMake也生成MSVS工程，然后打开MSVS开发调试。但是Ninja和MSVS两个generator只能二选一，有没有办法能够两者同时使用呢，我做了一些尝试，解决了中间一些问题，还有待继续： 首先要解决的是生成2个generator，CMake是没有办法在同一个build目录生成两次不同的generator的，所以需要用不同的目录，还是上面的例子： 12345cd /d D:\codemkdir buildcd buildcmake .. -G &quot;Ninja&quot; -Bninja -DCMAKE_MAKE_PROGRAM=D:\tool\ninja.execmake .. -G &quot;Visual Studio 15 2017 Win64&quot; 这样ninja的工程生成在build\ninja目录下，MSVS工程生成在build目录下。 其次要解决ninja和MSVC生成的中间文件的位置不一样问题，如obj文件。ninja是一个单配配置构建系统，而MSVS是一个多配置构建系统，他们的中间文件路径是不一样的。 针对这个问题，我对CMake源代码进行了一些修改，将ninjia输出的中间文件路径改为和MSVS一样，参见我的github仓库：https://github.com/zhongpan/CMake 最后要解决MSVC和ninjia要能够认彼此生成的编译中间文件，切换后不会触发重编，可能中间有一些彼此特有的文件需要生成，这一步还有待继续💪。]]></content>
      <categories>
        <category>devtool</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>ninja</tag>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代化OJ系统dmoj搭建问题总结及使用方法]]></title>
    <url>%2F2019%2F06%2F17%2F007-dmoj-set-up%2F</url>
    <content type="text"><![CDATA[dmoj介绍 dmoj是一个开源的Online Judge(简称OJ)系统，正如其简介中描述的，它是一个现代的OJ系统，相比其他老牌的OJ系统，其主要特点是： 支持的语言非常丰富，其他OJ系统主要是Java、C++、Python，而dmoj几乎覆盖了所有主流的语言，其中比较吸引我的还支持JavaScript； 其judge系统的是高度可扩展的，首先是可方便的引入新语言的支持，其次是题目的评判过程可高度定制； 其功能性上非常完备，覆盖OJ系统必备的所有功能，比如有些OJ不支持组织，只有用户的概念，而dmoj都是支持的； 国际化、本地化支持非常出色，得益于django框架，dmoj对国际化、时区、本地化处理的非常好，还有其头像采用gravatar服务，也是加分项； 其WEB站点也是可定制的，通过管理后台可动态添加导航和简单页面，支持markdown页面。 安装问题总结 根据官方的安装手册，仍然会遇到不少问题，特将安装过程中遇到的问题记录下来，供其他人参考。我是在debian9虚拟机中安装的，安装分两部分，一部分是站点，一部分是judge，judge可以安装多个。 web站点安装 python版本 必须采用python3，代码中使用了仅python3才有的库。 pip install mysqlclient时报错 1apt install libmysqlclient-dev 修改为： 1apt install libmariadbclient-dev 内网屏蔽了git协议，pip install -r requirements.txt报错 先将https://github.com/DMOJ/dmoj-wpadmin取到本地，然后修改requirements.txt中-e git://github.com/DMOJ/dmoj-wpadmin.git#egg=dmoj-wpadmin为本地路径。 USE_TZ = True时在WEB站点上涉及时间字段的操作后报错:Database returned an invalid datetime value. Are time zone definitions for your database installed? 执行如下命令： 1mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -D mysql -u root -p 发送邮件时异常：Mail from must equal authorized user local_settings中加入DEFAULT_FROM_EMAIL，赋值为使用的邮件发送地址。 judge安装 python版本 python2或python3均可 编译时找不到seccomp相关头文件 执行： 1sudo apt install libseccomp-dev libseccomp2 seccomp 其他相关知识 vagrant安装增强功能 12345apt-get install build-essential linux-headers-$(uname -r)vagrant upload &quot;C:\Program Files\Oracle\VirtualBox\VBoxGuestAdditions.iso&quot; /home/vagrant/VBoxGuestAdditions.isosudo mkdir /media/VBoxGuestAdditionssudo mount -o loop,ro VBoxGuestAdditions.iso /media/VBoxGuestAdditionssudo sh /media/VBoxGuestAdditions/VBoxLinuxAdditions.run source: not found ls -l 'which sh' 提示/bin/sh -&gt; dash这说明是用dash来进行解析的。执行：sudo dpkg-reconfigure dash，在界面中选择no，再ls -l 'which sh' 提示/bin/sh -&gt; bash，修改成功，source可以用了。 安装JDK 1sudo apt-get install openjdk-8-jre openjdk-8-jdk mariadb修改认证方式和root密码 https://www.cnblogs.com/zhuxiaoxi/p/10843659.html 使用说明 关于题目 安装后是不包含题目的，要让OJ真正运转起来，必须包含题目，通过爬虫程序可以从dmoj官网提取题目，可参考我写的爬虫程序：https://github.com/zhongpan/dmoj_problems。 增加语言 以增加V8JS为例： 安装v8dmoj 参考https://github.com/DMOJ/v8dmoj中的安装步骤安装，会安装v8dmoj程序到/user/bin下 修改judge的runtime 运行dmoj-autoconf，会自动检测本机上可用的编程语言环境，根据输出修改judeg的配置文件中的runtime，安装v8dmoj后会增加如下行： 1v8dmoj: /user/bin/v8dmoj 重启judge，可以看到多了V8JS： 站点管理后台增加语言，注意标识符同上面裁判服务器的执行器标识符一致]]></content>
      <categories>
        <category>online judge</category>
      </categories>
      <tags>
        <tag>online judge</tag>
        <tag>dmoj</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次失败的Windows下使用Docker UCP搭建容器集群]]></title>
    <url>%2F2019%2F06%2F03%2F006-docker-ucp-failure-set-up%2F</url>
    <content type="text"><![CDATA[手头刚好有Windows容器的Docker UCP的License，所以想试着用Dockers UCP搭建一个Windows容器集群体验一下，没想到这是一个非常痛苦的经历，这也证明了玩容器主流还是Linux，折腾了很久终于成功了一次，但是当我在同样的环境想再重新来一次的时候，居然一直再也没有成功过💔，特将这次过程记录一下，哪位同学知道问题所在还望赐教🙏。 组网 我只有一台笔记本，只有充分利用现在有资源了，Windows容器集群的管理节点还是必须用Linux，因此采用虚拟机运行Linux管理节点，笔记本作为Windows工作节点，具体组网如下： 软件版本 Windows 10 Pro 1803 管理节点使用Docker ce 18.09.06，工作节点Docker ee 18.09.06 Docker UCP 2.2.18 Vagrant 2.2.4 安装 安装准备 开启Hyper-V和容器支持，在“控制面板”→“程序”→“启用或关闭Windows功能”中选中Hyper-V和Containers，点确定。如果已经开启则跳过此步。 准备管理节点 安装vagrant，下载地址https://releases.hashicorp.com/vagrant/2.2.4/vagrant_2.2.4_x86_64.msi 使用官方的centos/7虚拟机模板建立虚拟机，注意这里选择Hyper-V驱动，因为Windows容器必须使用hyper-v虚拟化，而VirtualBox的虚拟化技术依赖vt-x，Hyper-V和vt-x是冲突的，开启Hyper-V后会自动关闭vt-x，因此无法使用VirtualBox虚拟机。 12vagrant init centos/7vagrant up --provider=hyperv 运行vagrant up会先从官方下载虚拟机模板，此过程需要一定时间。使用Hyper-V后，vagrant也会有一些限制，例如无法通过Vagrantfile进行某些配置，包括网络，安装过程需要选择使用哪个虚拟交换机，选择nat，后续在Hyper-V管理其中还可以修改： default: to create a new virtual switch. default: default: 1) Default Switch default: 2) nat default: default: What switch would you like to use? 虚拟机配置成静态ip 基础虚拟机没有安装ifconfig，安装一下ifconfig： 1sudo yum install net-tools.x86_64 前面虚拟机选择了默认的内部交换机nat，其中内置的DHCH分配的ip是172.16.0.0/16，为避免ip地址变化，最好配置成静态ip，后续安装Docker UCP需要这个ip： 首先将虚拟交换机nat的ip改成静态192.168.10.1 然后修改虚拟机的ip为192.168.10.10，网关为192.168.10.1 安装docker ce 1234567$ sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2$ sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo$ sudo yum install docker-ce docker-ce-cli containerd.io 启动docker 12$ sudo systemctl start docker$ sudo docker run hello-world 准备工作节点 下载docker，地址https://download.docker.com/components/engine/windows-server/18.09/docker-18.09.6.zip 解压到D:\docker，安装成Windows服务123dockerd --register-serviceStart-Service dockerdocker container run hello-world:nanoserver 安装Dockers UCP 安装管理节点 123456vagrant sshdocker container run --rm -it --name ucp \ -v /var/run/docker.sock:/var/run/docker.sock \ docker/ucp:2.2.18 install \ --host-address 192.168.10.10 \ --interactive 上述过程会先拉取docker/ucp:2.2.18镜像，需要一定时间，然后根据交互提示一步步进行安装。 安装工作节点 设置防火墙策略等 123$script = [ScriptBlock]::Create((docker run --rm docker/ucp-agent-win:2.2.18 windows-script | Out-String))Invoke-Command $script 上述过程会先拉取docker/ucp-agent-win:2.2.18镜像，需要一定时间。 修改daemon.json 增加&quot;labels&quot;: [“os=windows”] 验证Docker UCP前端界面：https://192.168.10.10 工作节点接入集群 通过Dockers UCP界面上可以得到节点加入集群的命令，也可以登录到管理节点上通过命令行得到。在工作节点上运行上述命令，就加入到集群。再回到Dockers UCP界面上就可以看到工作节点加入进来。 问题 上述搭建过程还算顺利，问题出在最后一步，工作节点加入集群后，在节点列表中Windows工作节点状态总不正常，只成功过一次，工作节点CPU利用率等信息都显示出来了，后来再试就一直卡在如下错误： 可能的问题原因是工作节点容器网络中没有overlay网络，在成功的那一次中，在工作节点加入集群后通过docker network ls可以看到overlay网络，但是后来再试就没有同步到overlay网络。可能的原因是Default Switch导致，在成功的那一次我删除了Default Switch，但是后来再这样做，overlay网络仍然不存在。 工作节点加入集群还遇到如下一些问题： 工作节点虚拟内存设置太小 {“level”:“error”,“msg”:“Failed to start proxy. Run “docker logs ucp-proxy” for more details”,“time”:“2019-05-12T17:18:49+08:00”} {“level”:“fatal”,“msg”:&quot;unable to reconcile state of Docker Proxy component: Error response from daemon: CreateComputeSystem 1bbffaab2a7609e6b21cdeea0b3b0b11204d86e7629c2 077646119141fcd8682: The paging file is too small for this operation to completen(extra info: {“SystemType”:“Container”,“Name”:&quot;1bbffaab2a7609e6b21cdeea0b3b0b112 虚拟网卡设置为静态IP后虚拟，需要保留hyper-v网络内部ip，否则在节点列表中显示如下错误信息: This Windows node cannot connect to its local Docker daemon. Make sure the Docker daemon is set up correctly on that node. See https://www.docker.com/ddc-34 for more information. 应该如下设置： MTU问题，在如下case中提到MTU问题，不确定是否在我遇到的问题中起到作用 参考:https://success.docker.com/article/newly-added-windows-node-reports-awaiting-healthy-status-in-classic-node-inventory 经过多次折腾之后，windows启动dockerd报如下错误： Error starting daemon: Error initializing network controller: Error creating default network: hnsCall failed in Win32: A network with this name already exists. (0x803b0010) 解决方法是执行了，已经没有心思细究根源了： 1netcfg -d]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docer</tag>
        <tag>docker ucp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用github pages和hexo搭建个人博客]]></title>
    <url>%2F2019%2F06%2F01%2F005-build-blog-with-hexo%2F</url>
    <content type="text"><![CDATA[这又是一篇如何使用github pages和hexo搭建个人博客的文章，是的，网络上已经有无数这类文章，所以我不准备详细记录每一个步骤，对于网络上已经有的，我只列出参考的文章。本文着重选择建立一个个人博客最实用，最需要的配置修改，hexo的扩展性是非常出色的，完全可以定制出非常酷炫的各种效果，这类修改本文并不涉及。另外网络上的一些文章已经比较陈旧，完全参照可能会遇到一些问题，本文进行了一些补充。如果你正在准备搭建一个个人博客，本文应该可以帮助你，不需要在网络上各种文章中苦苦寻找验证。当然，主要还是要感谢网络上乐于分享的人们，正是在他们的成果之上，才能有本博客，也欢迎大家常来逛逛：http://zhongpan.tech。 使用github pages建立个人博客 github为开发者提供了基于git版本管理的代码和内容的托管服务，他同时提供了github pages功能，允许开发者建立个人网页，并提供了一系列建站模板，以及形如xxx.github.io的二级域名，其中xxx为github账号名，建立github pages的过程和建立仓库的过程类似，只是仓库名总是xxx.github.io，这样就启用github pages了。具体可参考[1]，关于git ssh key配置可参考[2]，关于如何绑定自己的域名参考[3]，其中提到主机记录@的记录值后要加一个.，经过验证其实是不需要的，我是使用的阿里云域名服务。 github pages也是有限制的，只能托管静态页面，无法运行后端服务，也就不能进行服务端渲染和路由。对于建立复杂的个人网站还是最好使用云主机，对于建立相对简单的个人博客，使用github pages还是不错的选择，对页面的管理和管理git代码类似，开发者会非常亲切。 使用hexo管理博客 前面已经提到github pages只能托管静态页面，一般的个人博客需要提供标签、分类等功能，通过手工去编辑静态页面是非常不方便的。因此就产生了hexo这样的项目，hexo是使用node.js编写的，用于将用户编写的页面转换成网站静态页面，不仅如此，它还提供了丰富的插件和主题，实现各种客服端网页功能，比如评论、分享等等。本博客使用了Hexo v3.9.0进行管理，具体安装步骤参考[1]。hexo初次使用可能会遇到如下问题： 通过git上传，除了修改站点的_config.yml的deploy属性，注意还需要安装hexo-deployer-git插件： 1npm i hexo-deployer-git --save 每次重新上传后会删掉github pages上的CNAME、README.md文件：可参考[4]解决。 如何使用图片：可参考[5]。 注意图片名中不能包含空格，否则图片无法显示 注意如果使用了hexo-image-assert插件，就不要用asset_img标签，总是用markdown语法，否则图片显示不了，转换生成的html中的image路径似乎进行了两次转换，导致路径不对 如何添加阅读全文：可参考[6]。 简单总结一下hexo的功能： 页面分post、page和draft三种，hexo中称为layout，post就是博客文章，位于source/_posts目录下，page是独立的页面，位于source目录下同名目录，draft是博客草稿，位于source/_drafts目录下，草稿完成后通过hexo publish命令自动发布到post中。hexo内置支持一些特殊用途页面，如标签、分类、存档，对post进行归类，如标签页面，在Front-matter中定义type属性为tags： 123456---title: tagsdate: 2019-06-21 10:15:51type: "tags"comments: false--- 然后在post中使用上述type，例如: 123456---title: 使用github page和hexo搭建个人博客问题总结date: 2019-06-01 11:03:06tags: - hexo--- 页面都是使用markdown编写，执行hexo g后会转换成html，放到public目录下，最后上传到github的就是public目录下的内容。 对于上述标签、分类等页面，hexo会自动生成静态页面，其中包含相应的博客文章链接，观察一下public下的目录结构就会发现这一点。这是通过hexo-generator-tag、hexo-generator-category等插件实现的。 网站的最终呈现效果都是通过主题实现的，hexo默认的主题是landscape，还有众多的第三方主题，主题主要使用模板技术生成最终的静态页面，如swig、ejs、jade等，相应的需要安装对应的插件，如hexo-renderer-jade，ejs是默认的。 配置Hexo-NexT主题 hexo默认的主题landscape难以完全满足个人博客需求，试用了几款主题之后，最终我选择了NexT.Mist v7.1.2，它的功能非常丰富，安装配置可参考[7]，参考上述文章，本博客主要做了如下修改，修改过程中发现上述文章有些内容已经过时： 修改底部标签样式：不需要修改Blog\themes\next\layout\_macro\post.swig了，修改主题配置文件_config.yml即可； 1tag_icon: true 启用canvas_nest动画：注意首先需要参照https://github.com/theme-next/theme-next-canvas-nest中说明安装canvas-nest库； 增加分类和标签页面 主页文章添加阴影效果 增加字数和阅读时长统计：注意先参照https://github.com/theme-next/hexo-symbols-count-time中说明安装symbols-count-time库，然后修改站点的_config.yml(注意不是主题的)，增加： 123456symbols_count_time: symbols: true time: true total_symbols: true total_time: true exclude_codeblock: false 增加访客数和访问量统计：next主题已经支持busuanzi，直接开启即可； 增加评论功能 增加分享功能：next已经不支持jiathis，本博客选择了needmoreshare2，直接开启即可； 增加站内搜索功能 在文章末尾添加“文章结束”标记：图标换了一下 侧边栏社交小图标设置 底部隐藏由Hexo强力驱动、主题–NexT.Mist 底部用户图标换成桃心 设置头像：没有设置动画效果 添加版权信息：next已经支持，在creative_commons这个部分，选择合适的license即可。 添加网页顶部进度加载条 另外还增加了： 开启阅读进度：参照https://github.com/theme-next/theme-next-reading-progress安装theme-next-reading-progress；修改主题的_config.yml，将reading_progress的enable置为true； 1234reading_progress: enable: true color: "#37c6c0" height: 2px 其效果是在文章顶部增加一个进度条显示阅读进度。 开启书签：参照https://github.com/theme-next/theme-next-bookmark安装theme-next-bookmark；修改主题的_config.yml，将bookmark的enable置为true； 12345bookmark: enable: true # If auto, save the reading position when closing the page or clicking the bookmark-icon. # If manual, only save it by clicking the bookmark-icon. save: auto 在使用hext主题时遇到一个问题： post中标题级别必须连续，否则post的toc出现奇怪的错乱问题： 例如post中第一层用h1，第二层用h3，toc就会这样： 改为第一层用h1，第二层用h2，保持连续，toc就正常了： markdown编辑工具 最后建议找一款合适的markdown编辑工具，我是使用的Typora，至此建立博客的各项准备完全就绪了，接下来就可以愉快的书写了，Typora和hexo配合使用会遇到如下问题： 图片在typora中显示问题 前面介绍过在hexo插入图片推荐使用hexo-asset-image插件的方法，这时图片是放在post同名的目录中，而hexo的markdown中引用图片并没有加上目录名，这样在typora中就显示不了图片。解决方法参考[8]，经过我的验证，可行方法如下，其中xxxxxx为post名： 在Front-matter中加入typora-copy-images-to或者在typora全局偏好设置中设置，其中xxxxxx为post名 1typora-copy-images-to: xxxxxx 或者如下图在typora设置： 在Front-matter中加入typora-root-url，否则markdown中会带有目录名xxxxxx，这时在typora中可以显示图片，但是hexo中不能 1typora-root-url: xxxxxx 进行上述修改后，在typora中复制粘贴或插入本地图片后，typora会自动放到xxxxxx目录下，并生成markdown如下： 1![name](/name.png) 这里还是有一点问题，就是路径前面有一个/，在typora中显示是正常的，但是在hexo中无法显示图片，需要删除/。 hexo转换后的显示效果并非和typora一样，目前碰到的一个问题是对Reference-style links转换后，引用的链接列表并没有显示，而在typora中是显示的。所以后来只有换成了Footnotes。typora中对引用的链接列表显示效果如下； 支持emoji表情 hexo默认的markdown渲染器是不支持emoji的，可以使用emoji相关插件或者更换markdown转换器，我是选择的更换markdown转换器，具体可参考[9]。除了emoji，我还用到了它的Footnote功能，所以我还安装了 footnote插件，如需其他插件，同样需要先安装： 1234npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --savenpm install markdown-it-emoji --savenpm install markdown-it-footnote --save 另外其配置文件，我将permalink置为false了： 1234567891011121314151617181920markdown: render: html: true xhtmlOut: false breaks: true linkify: true typographer: true quotes: '“”‘’' plugins: - markdown-it-footnote #- markdown-it-sup #- markdown-it-sub #- markdown-it-abbr - markdown-it-emoji anchors: level: 2 collisionSuffix: 'v' permalink: false permalinkClass: header-anchor permalinkSymbol: ¶ 参考文章 绝配：hexo+next主题及我走过的坑 ↩ ↩ git ssh key配置 ↩ 绑定域名到 GitHub Pages ↩ 避免read.me，CNAME文件的覆盖，手动改github page的域名 ↩ Hexo中添加本地图片 ↩ Hexo-设置阅读全文 ↩ Hexo-NexT配置超炫网页效果 ↩ hexo与typora结合 ↩ Hexo NexT 开启 emoji ↩]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>github pages</tag>
        <tag>hexo</tag>
        <tag>typora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解六边形架构]]></title>
    <url>%2F2017%2F09%2F28%2F004-understanding-of-hexagonal-architecture%2F</url>
    <content type="text"><![CDATA[六边形架构或六角架构是Alistair Cockburn在2005年提出，解决了传统的分层架构所带来的问题，实际上它也是一种分层架构，只不过不是上下或左右，而是变成了内部和外部。在领域驱动设计（DDD）和微服务架构中都出现了六边形架构的身影，在《实现领域驱动设计》一书中，作者将六边形架构应用到领域驱动设计的实现，六边形的内部代表了application和domain层，而在Chris Richardson对微服务架构模式的定义中，每个微服务使用六边形架构设计，足见六边形架构的重要性。那么让我们一探究竟，它为何如此受青睐。 问题 传统的分层架构具有广泛的应用，例如经典的三层架构，把系统分为表示层、业务逻辑层、数据访问层。在Martin Fowler的《企业应用架构模式》一书中做过深入阐述，本书04年出版，时至今日分层架构仍然是常用的设计方法，分层架构可以降低耦合、提高复用、分而治之，但同时也还是存在一些问题： 应用逻辑在不同层泄露，导致替换某一层变得困难、难以对核心逻辑完整测试：你是否有过困惑，代码到底应该放在哪个层，虽然定义了各层的职责，但是总有人不严格遵循层次的分界，对于三层架构，常常会出现业务逻辑写在了表示层，或者业务逻辑直接和数据访问绑定。 传统的分层架构是一维的结构，有时应用不光是上下的依赖，可能是多维的依赖，这时一维的结构就无法适应了。 架构说明 六边形架构又称为端口-适配器，这个名字更容器理解。六边形架构将系统分为内部（内部六边形）和外部，内部代表了应用的业务逻辑，外部代表应用的驱动逻辑、基础设施或其他应用。内部通过端口和外部系统通信，端口代表了一定协议，以API呈现。一个端口可能对应多个外部系统，不同的外部系统需要使用不同的适配器，适配器负责对协议进行转换。这样就使得应用程序能够以一致的方式被用户、程序、自动化测试、批处理脚本所驱动，并且，可以在与实际运行的设备和数据库相隔离的情况下开发和测试。 内涵 六边形架构的重点体现在以下几个方面： 关注点 对于分层架构中层次的界定，Martin Fowler给出了一个判定的方法，就是如果把表示层换成其他实现，如果和原来的表示层有重复实现的内容，那么这部分内容就应该放到业务逻辑层。那么如何让开发人员在系统设计过程中始终保持这种视角，传统的分层架构是难以做到的。六边形架构有一个明确的关注点，从一开始就强调把重心放在业务逻辑上，外部的驱动逻辑或被驱动逻辑存在可变性、可替换性，依赖具体技术细节。而业务逻辑相对更加稳定，体现应用的核心价值，需要被详尽的测试。 外部可替换 一个端口对应多个适配器，是对一类外部系统的归纳，它体现了对外部的抽象。应用通过端口为外界提供服务，这些端口需要被良好的设计和测试。内部不关心外部如何使用端口，从一开始就要假定外部使用者是可替换的。六边形的六并没有实质意义，只是为了留足够的空间放置端口和适配器，一般端口数不会超过4个。适配器可以分为2类，“主”、“从”适配器，也可称为“驱动者”和“被驱动者”。 自动测试 在六边形架构中，自动化测试和用户具有同等的地位，在实现用户界面的同时就需要考虑自动化测试。它们对应相同的端口。六边形架构不仅让自动化测试这件事情成为设计第一要素，同时自动化测试也保证应用逻辑不会泄露到用户界面，在技术上保证了层次的分界。 依赖倒置 六边形架构必须遵循如下规则：内部相关的代码不能泄露到外部。所谓的泄露是指不能出现内部依赖外部的情况，只能外部依赖内部，这样才能保证外部是可以替换的。对于驱动者适配器，就是外部依赖内部的。但是对于被驱动者适配器，实际是内部依赖外部，这时需要使用依赖倒置，由驱动者适配器将被驱动者适配器注入到应用内部，这时端口的定义在应用内部，但是实现是由适配器实现。 代码示例 https://github.com/zhongpan/hexagonal-architecture-sample app层 app.h 1234567891011121314151617181920#pragma oncenamespace app &#123; class RateRepository &#123; public: virtual double getRate(double amount) = 0; &#125;; class Discounter &#123; public: Discounter(RateRepository* rateRepository) : _rateRepository(rateRepository) &#123;&#125; double discount(double amount); private: RateRepository* _rateRepository; &#125;; &#125; app.cpp 12345678910#include &quot;app.h&quot;namespace app &#123; double Discounter::discount(double amount) &#123; if (amount &lt;= 0) return 0; double rate = _rateRepository-&gt;getRate(amount); return amount * rate; &#125;&#125; 命令行adapter cmd.cpp 123456789101112131415#include &quot;app.h&quot;#include &quot;repo.h&quot;#include &lt;iostream&gt;using namespace app;using namespace repo;int main(int argc, char* argv[]) &#123; double amount = 0.0; std::cout &lt;&lt; &quot;please enter amount:&quot;; std::cin &gt;&gt; amount; MockRateRepository repo; Discounter app(&amp;repo); std::cout &lt;&lt; &quot;discount is:&quot; &lt;&lt; app.discount(amount) &lt;&lt; std::endl;&#125; 单元测试adapter test.cpp 1234567891011121314151617181920212223242526272829#define BOOST_TEST_MAIN#include &quot;app.h&quot;#include &lt;boost/test/unit_test.hpp&gt;#include &lt;boost/smart_ptr.hpp&gt;using namespace boost;class MockConstRateRepository : public app::RateRepository &#123;public: MockConstRateRepository(double rate) : _rate(rate) &#123;&#125; double getRate(double amount) &#123; return _rate; &#125;private: double _rate;&#125;;BOOST_AUTO_TEST_SUITE(s_discount)BOOST_AUTO_TEST_CASE(t_discount)&#123; double RATE_0point01 = 0.01; MockConstRateRepository repo(RATE_0point01); app::Discounter app(&amp;repo); BOOST_CHECK_EQUAL(app.discount(100), 100*RATE_0point01); BOOST_CHECK_EQUAL(app.discount(0), 0); BOOST_CHECK_EQUAL(app.discount(-100), 0);&#125;BOOST_AUTO_TEST_SUITE_END() 持久化adapter repo.h 123456789#pragma once#include &quot;app.h&quot;namespace repo &#123; class MockRateRepository : public app::RateRepository &#123; public: double getRate(double amount); &#125;;&#125; repo.cpp 123456789#include &quot;repo.h&quot;namespace repo &#123; double MockRateRepository::getRate(double amount) &#123; if (amount &lt;= 100) return 0.01; if (amount &lt;= 1000) return 0.02; return 0.05; &#125;&#125; 备注： 本文从https://www.cnblogs.com/zhongpan/p/7606430.html迁移而来。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[boost::lockfree使用介绍]]></title>
    <url>%2F2017%2F09%2F15%2F003-how-to-use-boost-lockfree%2F</url>
    <content type="text"><![CDATA[boost::lockfree是boost1.53引入的无锁数据结构，包括boost::lockfree::stack、boost::lockfree::queue和boost::lockfree::spsc_queue三种，前两种用于多生产者/多消费者场景，第三个用于单生产者/单消费者场景，下面对它们的使用进行详细介绍，以boost::lockfree::stack为例，其他类似。 构造 boost::lockfree::stack源代码如下（boost 1.65）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#ifdef BOOST_NO_CXX11_VARIADIC_TEMPLATEStemplate &lt;typename T, class A0, class A1, class A2&gt;#elsetemplate &lt;typename T, typename ...Options&gt;#endifclass stack&#123;private:#ifndef BOOST_DOXYGEN_INVOKED BOOST_STATIC_ASSERT(boost::is_copy_constructible&lt;T&gt;::value);#ifdef BOOST_NO_CXX11_VARIADIC_TEMPLATES typedef typename detail::stack_signature::bind&lt;A0, A1, A2&gt;::type bound_args;#else typedef typename detail::stack_signature::bind&lt;Options...&gt;::type bound_args;#endif static const bool has_capacity = detail::extract_capacity&lt;bound_args&gt;::has_capacity; static const size_t capacity = detail::extract_capacity&lt;bound_args&gt;::capacity; static const bool fixed_sized = detail::extract_fixed_sized&lt;bound_args&gt;::value; static const bool node_based = !(has_capacity || fixed_sized); static const bool compile_time_sized = has_capacity;/* 省略 */public: typedef T value_type; typedef typename implementation_defined::allocator allocator; typedef typename implementation_defined::size_type size_type; //! Construct stack // @&#123; stack(void): pool(node_allocator(), capacity) &#123; BOOST_ASSERT(has_capacity); initialize(); &#125; template &lt;typename U&gt; explicit stack(typename node_allocator::template rebind&lt;U&gt;::other const &amp; alloc): pool(alloc, capacity) &#123; BOOST_STATIC_ASSERT(has_capacity); initialize(); &#125; explicit stack(allocator const &amp; alloc): pool(alloc, capacity) &#123; BOOST_ASSERT(has_capacity); initialize(); &#125; // @&#125; //! Construct stack, allocate n nodes for the freelist. // @&#123; explicit stack(size_type n): pool(node_allocator(), n) &#123; BOOST_ASSERT(!has_capacity); initialize(); &#125; template &lt;typename U&gt; stack(size_type n, typename node_allocator::template rebind&lt;U&gt;::other const &amp; alloc): pool(alloc, n) &#123; BOOST_STATIC_ASSERT(!has_capacity); initialize(); &#125; boost::lockfree::stack的第一个模板参数是元素类型，后面3个参数是用来配置stack的，没有顺序要求： boost::lockfree::fixed_sized：是否固定大小，默认为boost::lockfree::fixed_sized&lt;false&gt;，如果为true，则内部使用数组保存元素，大小不能动态增长； boost::lockfree::capacity：编译时设置内部数组大小，设置了capacity意味着一定是boost::lockfree::fixed_sized&lt;true&gt;，和运行时指定大小是互斥的，见下面的例子； boost::lockfree::allocator：设置分配器，默认boost::lockfree::allocator&lt;std::allocator&lt;void&gt;&gt;。 例如： 1234567891011//表示动态大小，初始大小为4，用完了再动态增长；此时必须在构造函数指定初始大小，否则断言失败；boost::lockfree::stack&lt;int&gt; s(4);//表示大小固定，运行时指定初始大小为4，用完后再push就会失败；此时必须在构造函数指定初始大小，否则断言失败；boost::lockfree::stack&lt;int, boost::lockfree::fixed_sized&lt;true&gt;&gt; s1(4);//表示大小固定，编译时指定初始大小为4，用完后再push就会失败；此时不能在构造函数指定初始大小，否则断言失败；boost::lockfree::stack&lt;int, boost::lockfree::capacity&lt;4&gt;&gt; s2;//和上面一样，设置了capacity，fixed_size就总是trueboost::lockfree::stack&lt;int, boost::lockfree::fixed_size&lt;false&gt;, boost::lockfree::capacity&lt;4&gt;&gt; s3; 成员方法 push：压入一个元素到容器，除了unsynchronized_，都是线程安全的。所有都是非阻塞的。 123456789101112bool push(T const &amp; v)bool bounded_push(T const &amp; v)template &lt;typename ConstIterator&gt;ConstIterator push(ConstIterator begin, ConstIterator end)template &lt;typename ConstIterator&gt;ConstIterator bounded_push(ConstIterator begin, ConstIterator end)bool unsynchronized_push(T const &amp; v)ConstIterator unsynchronized_push(ConstIterator begin, ConstIterator end) bounded_表示不动态增长，当初始大小用完后再push就会失败； unsynchronized_表示非线程安全； pop：从容器中弹出一个元素，除了unsynchronized_，都是线程安全的。所有都是非阻塞的。 123456789bool pop(T &amp; ret)template &lt;typename U&gt;bool pop(U &amp; ret)bool unsynchronized_pop(T &amp; ret)template &lt;typename U&gt;bool unsynchronized_pop(U &amp; ret) unsynchronized_表示非线程安全； consume_：从容器弹出1个或全部元素，并应用某个函数对象。线程安全或阻塞与否取决于函数对象。 1234567891011121314151617181920212223template &lt;typename Functor&gt;bool consume_one(Functor &amp; f)template &lt;typename Functor&gt;bool consume_one(Functor const &amp; f)template &lt;typename Functor&gt;size_t consume_all(Functor &amp; f)template &lt;typename Functor&gt;size_t consume_all(Functor const &amp; f)template &lt;typename Functor&gt;size_t consume_all_atomic(Functor &amp; f)template &lt;typename Functor&gt;size_t consume_all_atomic(Functor const &amp; f)template &lt;typename Functor&gt;size_t consume_all_atomic_reversed(Functor &amp; f)template &lt;typename Functor&gt;size_t consume_all_atomic_reversed(Functor const &amp; f) _one表示只消费1个元素； _all表示消费所有元素； _atomic表示消费过程是原子的，其间其他操作对其是不可见的。 _reversed表示倒序消费。 其他 123456//预分配空闲节点数，和编译时设置capacity互斥；线程安全，可能阻塞void reserve(size_type n)//非线程安全void reserve_unsafe(size_type n)//判断是否为空bool empty(void) const 简单示例 12345678910111213141516#include &lt;boost/lockfree/stack.hpp&gt;int main(int argc, char *argv[])&#123; boost::lockfree::stack&lt;int&gt; s(64); //producer for (int i = 0; i &lt; 1000; i++) &#123; s.push(i); &#125; //consumer s.consume_all([](int i) &#123; std::cout &lt;&lt; i &lt;&lt; std::endl; &#125;); return 0;&#125;]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>boost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式监控系统zipkin介绍]]></title>
    <url>%2F2017%2F09%2F11%2F002-introduction-to-zipkin%2F</url>
    <content type="text"><![CDATA[zipkin是Twitter基于google的分布式监控系统Dapper（论文）的开发源实现，zipkin用于跟踪分布式服务之间的应用数据链路，分析处理延时，帮助我们改进系统的性能和定位故障。 zipkin架构 Instrumented client和Instrumented server是分布式系统中的服务，通过装备库采集跟踪信息，装备库再调用Transport，把跟踪信息发送给zipkin。 装备库 针对不同语言，不同RPC框架，有不同的装备库实现，目前已有实现列表见此，其中Brave是zipkin官方提供的Java的装备库。 一个装备库的实现需要考虑如下情况： 实现语言，和需要装备的服务的语言一致 zipkin需要的核心数据结构信息记录，包括tracerid,spanid的生成，延迟时间的计算，事件记录，tag记录等 服务之间跟踪信息的传递，称为植入，不同RPC接口植入的方式不一样，例如HTTP接口采用B3协议植入 植入的信息包括：Trace Id、Span Id、Parent Id、Sampled、Flags 采样，减少跟踪导致的系统负荷 报告给zipkin，调用Transport将跟踪信息传给zipkin Transport Transport是zipkin对外提供的接口，目前有HTTP、Kafka、Scribe三种。 HTTP：采用json格式，接口定义见https://github.com/openzipkin/zipkin-api Kafka：分布式发布订阅消息系统 Scribe：Facebook的日志收集系统https://github.com/facebook/scribe 核心数据结构 v2版本： traceId：64位或128位，全局唯一， parentId：父spanid，64位，根span的parentId为空 id：spanid，64位，tranceId内唯一 name：方法名 serviceName：服务名 timestamp：自1970-1-1 00:00:00 UTC的微秒 duration：开始span到结束span的时间，单位微秒 annotations：记录事件，value有一些预定义的值，例如客户端发送(cs)，客户端接收(cr)，服务端接收(sr)，服务端发送(ss)等 tags：记录附加数据 一个Span就是记录[remoteEndpoint.serviceName]服务的[Span.name]方法的执行过程，其中的annotation记录了中间的一些事件发生时间，通过这些时间可以得到[Span.name]方法的网络传输时间，服务端执行时间，客户端响应时间等信息，从而对其进行诊断优化。多个Span通过parentId构成一个树形结构，根Span的parentId为空，描述了一次trace（tranceId标识）中多个服务之间的调用过程。 示例说明 https://github.com/zhongpan/zipkin-ice 假设service1.fun1中调用service2.fun2和service3.fun3，service2.fun2中调用service4.fun4。本次跟踪各个服务中会创建如上的span1~span7，span1为根span。span2和span4为一次RPC的客户端和服务端行为，可以共享spanid，span4不用新生成spanid，span2和span4在zipkin中会合并为1个span，span3/span5以及span6/span7类似。最终，在zipkin界面展示的树形结构为：]]></content>
      <categories>
        <category>分布式跟踪</category>
      </categories>
      <tags>
        <tag>zipkin</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解std::chrono的时钟Clock]]></title>
    <url>%2F2017%2F09%2F07%2F001-understand-chrono%2F</url>
    <content type="text"><![CDATA[std::chrono是C++11引入的日期时间处理库，其中包含3种时钟：system_clock，steady_clock，high_resolution_clock。近来需要使用高精度时间，很自然想到使用high_resolution_clock，然而使用后发现并非预期的得到自1970/1/1零点之后的计数，而是一个小得多的数字。那么这三种时钟有什么区别，用在什么情况下，我们来一探究竟。 问题 12auto tp = std::chrono::high_resolution_clock::now();std::cout &lt;&lt; tp.time_since_epoch().count() &lt;&lt; std::endl; 上述代码输出一个比较小的数字，high_resolution_clock的精度是纳秒，不可能是这么小的数字。 三种时钟的区别 所谓时钟，是指从一个时点开始，按照某个刻度的一个计数。如下代码摘自VC2017。 system_clock 1234567struct system_clock&#123; // wraps GetSystemTimePreciseAsFileTime/GetSystemTimeAsFileTimetypedef long long rep;typedef ratio_multiply&lt;ratio&lt;_XTIME_NSECS_PER_TICK, 1&gt;, nano&gt; period;typedef chrono::duration&lt;rep, period&gt; duration;typedef chrono::time_point&lt;system_clock&gt; time_point;static constexpr bool is_steady = false; 对于system_clock，其起点是epoch，即1970-01-01 00:00:00 UTC，其刻度是1个tick，也就是_XTIME_NSECS_PER_TICK纳秒。 high_resolution_clock 1typedef steady_clock high_resolution_clock; high_resolution_clock实际上和steady_clock一样。 steady_clock 1234567struct steady_clock&#123; // wraps QueryPerformanceCountertypedef long long rep;typedef nano period;typedef nanoseconds duration;typedef chrono::time_point&lt;steady_clock&gt; time_point;static constexpr bool is_steady = true; steady_clock的刻度是1纳秒，起点并非1970-01-01 00:00:00 UTC，一般是系统启动时间，这就是问题的关键。steady_clock的作用是为了得到不随系统时间修改而变化的时间间隔，所以凡是想得到绝对时点的用法都是错误的。steady_clock是没有to_time_t()的实现的，而system_clock是有的。 三种时钟用在什么时候 system_clock：用在需要得到绝对时点的场景 123auto tp = std::chrono::system_clock::now();std::time_t tt = std::chrono::system_clock::to_time_t(tp);std::cout &lt;&lt; tt &lt;&lt; &quot;seconds from 1970-01-01 00:00:00 UTC&quot; &lt;&lt; std::endl; steady_clock：用在需要得到时间间隔，并且这个时间间隔不会因为修改系统时间而受影响的场景 1234auto tp1 = std::chrono::steady_clock::now();//do somethingauto tp2 = std::chrono::steady_clock::now();std::cout &lt;&lt; std::chrono::duration_cast&lt;std::chrono::microseconds&gt;(tp2 - tp1).count() &lt;&lt; &quot;microseconds&quot; &lt;&lt; std::endl; high_resolution_clock：high_resolution_clock是system_clock或steady_clock之一，根据情况使用 常见的错误用法 std::this_thread::sleep_until传入的是steady_clock::time_point 根据steady_clock::time_point得到time_t]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[icebox服务启动失败问题定位记录]]></title>
    <url>%2F2016%2F01%2F11%2F024-icebox-service-started-failed%2F</url>
    <content type="text"><![CDATA[通过IceGridGUI启动某个icebox服务时，弹出如下图的错误对话框，启动服务失败。提示找不到icebox.exe程序，但是icebox.exe是存在的，并且在path中，问题非常诡异。下面详细记录问题定位的过程。 背景知识 IceGrid的架构中包含icegridregistry.exe和icegridnode.exe两个进程，icegridnode.exe负责启动具体的服务，当从IceGridGUI中start某个服务时， 实际上是调用icegridnode.exe接口，由icegridnode.exe负责启动服务，如果服务是icebox服务，就会根据application.xml中配置的icebox路径运行 icebox，由icebox加载具体服务dll。 application.xml中配置的icebox路径，一般配置的icebox程序名，不包含完整路径，icebox程序名根据不同的操作系统和运行环境，程序名有所不同。 初步判断 问题的现象是找不到icebox，因为application.xml中只配置icebox程序名，没有完整路径， 所以推测icegridnode.exe应该是通过系统path寻找icebox的，那么首先自然会想到可能是系统path路径中不包含icebox的路径。 第一次尝试 首先查看系统环境变量是否包含icebox的路径， 经过查看此路径是存在的； 然后查看进程的控制块是否包含系统Path，进程启动时会复制一份系统环境变量到进程控制块，也就是说每个进程都有自己的一份系统环境变量， 因此修改系统Path对已经启动的进程是无效的。那么怎么查看进程的环境变量呢，可以通过procexp.exe工具，经过查看icegridnode.exe进程中系统Path是正确的。至此第一次尝试以失败告终。 继续分析 经过初步诊断排除了外部环境的问题，接下来只能从程序内部来找原因，需要找到出问题的代码位置，根据服务启动的原理，问题应该出在icegridnode.exe中， 关键就是怎么在Ice源代码中快速找到，而目前唯一的线索就是弹出的错误对话框，这应该是突破口； 根据对话框中的错误消息，用关键字&quot;Couldn’t find&quot;在Ice源代码中搜索，这里推荐FindStr（Windows下也有findstr命令）这个工具，经过搜索果然找到了 出问题的代码位置，如下： 源代码的位置找到了，就要开始上终极神器了，那就是windbg，接下来先做一些准备工作，看一下出问题的代码。 icegridnode.exe启动icebox的源代码 查看代码关键的地方是SearchPathW的调用，这是一个Windows API。 123456789101112131415161718192021#ifdef _WIN32 if(!IceUtilInternal::isAbsolutePath(path)) &#123; if(path.find('/') == string::npos) &#123; // // Get the absolute pathname of the executable. // wchar_t absbuf[_MAX_PATH]; wchar_t* fPart; wstring ext = path.size() &lt;= 4 || path[path.size() - 4] != '.' ? L".exe" : L""; if(SearchPathW(NULL, IceUtil::stringToWstring(path).c_str(), ext.c_str(), _MAX_PATH, absbuf, &amp;fPart) == 0) &#123; if(_traceLevels-&gt;activator &gt; 0) &#123; Trace out(_traceLevels-&gt;logger, _traceLevels-&gt;activatorCat); out &lt;&lt; "couldn't find `" &lt;&lt; path &lt;&lt; "' executable."; &#125; throw string("Couldn't find `" + path + "' executable."); &#125; path = IceUtil::wstringToString(absbuf); SearchPath函数说明 SearchPath定义： 12345678DWORD WINAPI SearchPath( _In_opt_ LPCTSTR lpPath, _In_ LPCTSTR lpFileName, _In_opt_ LPCTSTR lpExtension, _In_ DWORD nBufferLength, _Out_ LPTSTR lpBuffer, _Out_opt_ LPTSTR *lpFilePart); Parameters lpPath [in, optional] The path to be searched for the file. If this parameter is NULL, the function searches for a matching file using a registry-dependent system search path. For more information, see the Remarks section. Return value If the function succeeds, the value returned is the length, in TCHARs, of the string that is copied to the buffer, not including the terminating null character. If the return value is greater than nBufferLength, the value returned is the size of the buffer that is required to hold the path, including the terminating null character. If the function fails, the return value is zero. To get extended error information, call GetLastError. 开始第二次尝试，上windbg 先对icegridnode.exe做了一个userdump保留好环境以防万一， 然后设置好windbg的符号和源代码路径； 用windbg attach到icegridnode.exe进程开始调试，打开Activator.cpp文件，在370行设置好断点，F5(Go)； 在IceGridGUI工具中start某个服务，windbg中可以看到断点命中。很显然出现本问题一定是SearchPathW返回的0， 那么为什么SearchPathW返回的0，现在就可以一探究竟了，查看SearchPath输入的参数，发现输入的参数都是正确的， Step Into到IceUtil::stringToWstring，其转换结果也是正确的，Step Out继续往下调确认SearchPathW 确实返回的0； 根据SearchPath函数的说明，返回0表示失败，通过GetLastError可以查询错误码，在windbg中通过!gle命令查询错误码为0，也就是没有错误， 这就奇怪了，输入参数都是正确的，错误码也显示没有错误，为什么找不到icebox呢？ 通过windbg的!peb命令可以查看进程控制块，确认了进程的环境变量系统Path确实包含正确路径，自此陷入僵局，一切看起来都是正常的， 但是SearchPath的返回结果就是不对。 windbg 小知识 !stl命令打印stl字符串 !gle获取GetLastError错误码 !peb命令查看进程控制块 僵局打破 真是山穷水复疑无路，柳暗花明又一村，正当一筹莫展之时，一个误操作反而打破僵局，让新的线索浮出水面， 本来是想detach的，由于windbg卡住没反应，就接着点了关闭windbg，弹出对话框提示等待还是取消，就点了取消， 结果windbg是退出了，但是icegridnode.exe也退出了，幸好之前userdump了，不过毕竟丢失了第一手环境，还是 有些可惜，难道这个问题就此又成了无头冤案，不能啊。 继续，这么奇怪的问题不找出真相让人不爽，既然icegridnode.exe已经退出，那重启一下，看能否重现问题吧， 进入services.msc，启动icegridnode服务，居然报错了，服务无法启动，顿时眼前一亮，马上想到肯定跟用户权限有关系， 查看了icegridnode服务的用户，使用的Local Service用户，那么会不会是Local Service用户权限问题导致的呢？ 什么是Local Service用户 参考1，参考2 LocalSystem 账户 LocalSystem是预设的拥有本机所有权限的本地账户，这个账户跟通常的用户账户没有任何关联，也没有用户名和密码之类的凭证。这个服务账户可以打开注册表的HKEY_LOCAL_MACHINE\Security键，当LocalSystem访问网络资源时，它是作为计算机的域账户使用的。 举例来说，以LocalSystem账户运行的服务主要有：WindowsUpdate Client、 Clipbook、Com+、DHCP Client、Messenger Service、Task Scheduler、Server Service、Workstation Service，还有Windows Installer。 Network Service 账户 Network Service账户是预设的拥有本机部分权限的本地账户，它能够以计算机的名义访问网络资源。但是他没有Local System 那么多的权限，以这个账户运行的服务会根据实际环境把访问凭据提交给远程的计算机。Network Service账户通常可以访问Network Service、Everyone组，还有认证用户有权限访问的资源。 举例来说，以Network Service账户运行的服务主要有：Distributed Transaction Coordinator、DNS Client、Performance Logs and Alerts，还有RPC Locator。 Local Service 账户 Local Service账户是预设的拥有最小权限的本地账户，并在网络凭证中具有匿名的身份。Local Service账户通常可以访问Local Service、Everyone组还有认证用户有权限访问的资源。 举例来说，以Local Service账户运行的服务主要有：Alerter、Remote Registry、Smart Card、SSDP，还有WebClient。 第三次尝试，验证猜想 感觉真相应该快出来了，接下来验证一下自己的猜想，很简单只要用Local Service用户运行cmd，在cmd中运行where命令就可以验证了； 首先想到runas命令，尝试了一下，对内置用户无法使用； 开始google如何使用Local Service用户运行某个程序，还真找到了方法，通过psexec工具， 这是一个非常强大的工具，点击此处下载，其实里面还有一系列其他工具； 执行psexec.exe -i -u “nt authority\localservice” cmd命令，弹出新的cmd窗口，执行whoami命令，确认确实使用localservice用户运行的， 注意这里localservice用户的全名是nt authority\localservice，然后运行where icebox命令，确实找不到，同时以administrator运行cmd， 确认可以找到，至此可以完全确定问题是由localservice用户权限导致。 进一步找到根源 要修复问题还需要找到到底是什么影响了localservice用户的权限，可以通过对比正常的机器的设置找到差异， 首先想到的是组策略中是否有对localservice用户权限的设置，运行gpedit.msc打开组策略，对比正常机器和问题机器， 确实发现有一项两边是不一样的，但是修改之后并没有效果，下图为正常机器的截图； 然后想到的可能是目录的权限设置问题，对比正常机器和问题机器，发现问题机器少了Users用户组的权限，下图为正常机器截图： 然后在问题机器上参照正常机器添加Users用户组的“Read &amp; execute,List folder contents,Read,Special permissions (Create files/write data,Create folders/append data)”权限，激动人心的时刻出现了，where命令可以正确找到icebox了， icegridnode服务也可以正常启动了，验证了一下在IceGridGUI中start服务也是OK的，然后删除以上权限，再验证是否问题能复现，果然问题复现了， 问题的根源终于找到，问题也得以圆满解决，Happy！！！从正常机器来看，icebox所在目录的权限其实是从D:\继承的，所以也可以通过继承父的权限来修改。 什么是Users用户组 参考 普通用户组，这个组的用户无法进行有意或无意的改动。因此，用户可以运行经过验证的应用程序，但不可以运行大多数旧版应用程序。Users 组是最安全的组，因为分配给该组的默认权限不允许成员修改操作系统的设置或用户资料。Users 组提供了一个最安全的程序运行环境。在经过 NTFS 格式化的卷上，默认安全设置旨在禁止该组的成员危及操作系统和已安装程序的完整性。用户不能修改系统注册表设置、操作系统文件或程序文件。Users 可以关闭工作站，但不能关闭服务器。Users 可以创建本地组，但只能修改自己创建的本地组。 它受administrator、admin 等管理员的管制和安排。比如：你家或你寝室里只有你自己1台电脑，你可以设置两个用户，一个管理员，另一个为普通用户。这样，在管理员身份下，设置文件的“只读”属性，禁止“添加或删除程序”……你可以放心他人不会修改或删除你的资料，另外，也可以防止他人用你的机器来下载东西。 总结 调试的过程是一个抽丝拨茧的过程，层层深入，需要足够的耐心和细心，每走一步前先仔细分析一下，然后验证自己的猜想，任何问题一定有其根本原因， 只要不轻易放弃，通过一定技巧及努力，一定是可以找到问题原因的； 调试的过程可以学到很多新的知识，这个问题中有很多知识其实我也不知道，比如Windows的用户权限知识，但是通过问题线索，顺藤摸瓜，在Google的帮助 下，既学到了新的知识，问题也得以解决，个人能力就是这样提升上来的，但是如果解决问题半途而废，那么隐藏其中的很多原理和相关知识，你就无缘知晓； Windows平台下的windbg绝对是神器，大部分问题，如果windbg调不出问题，那基本上可以断定应该不是程序本身的问题，这么好的东西，还不赶快学； 调试之前一定注意先收集各种线索以及保护好环境，退出调试时一定要detach，以免环境被破坏，这样问题可能就永远无法得见天日； 浏览了一下ice的手册，在Widnows Services一章，提到安装icegridnode为Windows服务时建议使用LocalService用户，并且发现了icacls命令， 可以用于查看和修改文件的权限，基于这些内容对本问题其实也可以做一些推断； 最后其实还有一个疑问，既然没有权限，为什么SearchPath的GetLastError错误码是0，如果这个时候从错误码就能判断错误，那么问题就更容易解决了。当然 也许SearchPath内部还调用了其他API函数，导致GetLastError为0，不管怎么样如果SearchPath的接口设计的好一点，查问题就更容易了。]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
      <tags>
        <tag>ice</tag>
        <tag>icegrid</tag>
        <tag>windbg</tag>
      </tags>
  </entry>
</search>
